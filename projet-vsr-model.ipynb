{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11751936,"sourceType":"datasetVersion","datasetId":7377800},{"sourceId":11771664,"sourceType":"datasetVersion","datasetId":7390455},{"sourceId":11775042,"sourceType":"datasetVersion","datasetId":7392733},{"sourceId":386075,"sourceType":"modelInstanceVersion","modelInstanceId":318430,"modelId":338998},{"sourceId":387636,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":319534,"modelId":340100},{"sourceId":395209,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":324733,"modelId":345552},{"sourceId":395276,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":324768,"modelId":345552},{"sourceId":397597,"sourceType":"modelInstanceVersion","modelInstanceId":325952,"modelId":346831}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"97ff2930-4795-4d9f-9c1f-e57be862fcbb","cell_type":"code","source":"# Cell 1: CharMap Class\nprint(\"--- Defining CharMap Class ---\")\nimport torch\nimport numpy as np\n\nclass CharMap:\n    \"\"\"\n    Classe pour mapper les caractères à des indices entiers et vice-versa.\n    Gère également le jeton BLANK nécessaire pour la perte CTC.\n    \"\"\"\n    def __init__(self, alphabet_string):\n        self.SOS_token = None\n        self.EOS_token = None\n        self.BLANK_token_idx = 0 # L'index du jeton BLANK (généralement 0 pour CTCLoss)\n\n        self.char_to_int = {char: i + 1 for i, char in enumerate(alphabet_string)}\n        self.int_to_char = {i + 1: char for i, char in enumerate(alphabet_string)}\n\n        self.int_to_char[self.BLANK_token_idx] = \"<BLANK>\"\n        self.char_to_int[\"<BLANK>\"] = self.BLANK_token_idx\n\n        self.vocab_size = len(self.char_to_int)\n        print(f\"CharMap initialisé. Taille du vocabulaire (avec BLANK) : {self.vocab_size}\")\n\n    def get_char_to_int_map(self):\n        return self.char_to_int\n\n    def get_int_to_char_map(self):\n        return self.int_to_char\n\n    def get_vocab_size(self):\n        return self.vocab_size\n\n    def get_blank_token_idx(self):\n        return self.BLANK_token_idx\n\n    def text_to_indices(self, text):\n        indices = [self.char_to_int.get(char) for char in text if char in self.char_to_int]\n        return [idx for idx in indices if idx is not None]\n\n    def indices_to_text(self, indices, remove_blanks=True, remove_duplicates=True):\n        text = []\n        last_char_idx = -1\n        for idx in indices:\n            idx_val = idx.item() if isinstance(idx, torch.Tensor) else idx\n\n            if idx_val == self.BLANK_token_idx and remove_blanks:\n                last_char_idx = -1\n                continue\n\n            if remove_duplicates and idx_val == last_char_idx:\n                continue\n\n            char = self.int_to_char.get(idx_val)\n            if char:\n                 text.append(char)\n\n            if idx_val != self.BLANK_token_idx:\n                 last_char_idx = idx_val\n            elif not remove_blanks:\n                 last_char_idx = idx_val\n        return \"\".join(text)\n\nprint(\"--- CharMap Class Defined ---\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:59:48.537383Z","iopub.execute_input":"2025-05-16T19:59:48.537930Z","iopub.status.idle":"2025-05-16T19:59:48.546139Z","shell.execute_reply.started":"2025-05-16T19:59:48.537904Z","shell.execute_reply":"2025-05-16T19:59:48.545399Z"}},"outputs":[{"name":"stdout","text":"--- Defining CharMap Class ---\n--- CharMap Class Defined ---\n\n","output_type":"stream"}],"execution_count":4},{"id":"94d4d6f4-fd3c-43f0-af0e-c880e8453dbb","cell_type":"code","source":"# Cell 2: VSRDataset Class and collate_fn\nprint(\"--- Defining VSRDataset Class and collate_fn ---\")\nimport os\nimport glob\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport traceback\n\n# Assurez-vous que CharMap est défini dans la cellule précédente\n\nclass VSRDataset(Dataset):\n    def __init__(self, data_root_dir, char_map_instance, img_size=(96, 96), is_train=True, augmentation_prob=0.5, video_metadata_suffix=\"_metadata.csv\", segment_path_col='archive_path', text_col='text'):\n        super().__init__()\n        print(f\"  [VSRDataset __init__] Initializing with data_root_dir: {data_root_dir}\")\n        self.data_root_dir = data_root_dir\n        self.char_map = char_map_instance\n        self.img_size = img_size\n        self.is_train = is_train\n        self.augmentation_prob = augmentation_prob\n        self.metadata_list = []\n\n        if not os.path.isdir(self.data_root_dir):\n            print(f\"  [VSRDataset __init__] ERREUR: Répertoire de base des données introuvable : {self.data_root_dir}\")\n            return\n\n        print(f\"  [VSRDataset __init__] Scanning for video metadata in: {self.data_root_dir} using suffix '{video_metadata_suffix}'\")\n        print(f\"  [VSRDataset __init__] Expecting segment path column: '{segment_path_col}', text column: '{text_col}'\")\n\n        processed_video_dirs = 0\n        for video_id_dir_name in sorted(os.listdir(self.data_root_dir)):\n            video_dir_path = os.path.join(self.data_root_dir, video_id_dir_name)\n            if os.path.isdir(video_dir_path):\n                processed_video_dirs += 1\n                metadata_csv_name = f\"{video_id_dir_name}{video_metadata_suffix}\"\n                video_metadata_csv_path = os.path.join(video_dir_path, metadata_csv_name)\n\n                if os.path.exists(video_metadata_csv_path):\n                    try:\n                        video_df = pd.read_csv(video_metadata_csv_path)\n                        # print(f\"    [VSRDataset __init__] Processing metadata: {video_metadata_csv_path} ({len(video_df)} segments found).\") # Less verbose\n                        missing_col_warning_shown = False\n                        for idx, row in video_df.iterrows():\n                            path_from_csv = row.get(segment_path_col)\n                            text_label = row.get(text_col)\n                            path_missing = pd.isna(path_from_csv) or not path_from_csv\n                            text_missing = pd.isna(text_label) or not text_label\n\n                            if not path_missing and not text_missing:\n                                relative_segment_dir_path = str(path_from_csv).replace('.zip', '')\n                                full_segment_dir_path = os.path.join(self.data_root_dir, relative_segment_dir_path)\n                                if os.path.isdir(full_segment_dir_path):\n                                    self.metadata_list.append({\n                                        'segment_dir_path': full_segment_dir_path,\n                                        'text': str(text_label)\n                                    })\n                                # else: # Reduced warning verbosity\n                                    # print(f\"      [VSRDataset __init__] Avertissement: Segment DIRECTORY non trouvé: {full_segment_dir_path}\")\n                            # else: # Reduced warning verbosity\n                                # if not missing_col_warning_shown:\n                                     # print(f\"      [VSRDataset __init__] Avertissement: Colonnes manquantes dans {video_metadata_csv_path}.\")\n                                     # missing_col_warning_shown = True\n                    except pd.errors.EmptyDataError: print(f\"    [VSRDataset __init__] Avertissement: Metadata file {video_metadata_csv_path} is empty.\")\n                    except Exception as e: print(f\"    [VSRDataset __init__] ERREUR lors de la lecture de {video_metadata_csv_path}: {e}\"); traceback.print_exc()\n                # else: # Reduced warning verbosity\n                    # print(f\"    [VSRDataset __init__] Avertissement: Fichier metadata {video_metadata_csv_path} non trouvé.\")\n\n        print(f\"  [VSRDataset __init__] Scanned {processed_video_dirs} potential video directories.\")\n        if not self.metadata_list:\n            print(f\"  [VSRDataset __init__] ERREUR: Aucune métadonnée de segment valide trouvée.\")\n        else:\n            print(f\"  [VSRDataset __init__] Total de {len(self.metadata_list)} échantillons de segments chargés.\")\n\n    def __len__(self):\n        return len(self.metadata_list)\n\n    def __getitem__(self, idx):\n        if idx >= len(self.metadata_list):\n            raise IndexError(\"Index hors limites\")\n\n        sample_info = self.metadata_list[idx]\n        segment_dir_path = sample_info['segment_dir_path']\n        text_label = str(sample_info['text'])\n        frames = []\n        try:\n            frame_files = sorted(glob.glob(os.path.join(segment_dir_path, '*.png')))\n            frame_files += sorted(glob.glob(os.path.join(segment_dir_path, '*.jpg')))\n            frame_files += sorted(glob.glob(os.path.join(segment_dir_path, '*.jpeg')))\n\n            if not frame_files: return None\n            for frame_filepath in frame_files:\n                img_bgr = cv2.imread(frame_filepath, cv2.IMREAD_COLOR)\n                if img_bgr is None: continue\n                img_resized = cv2.resize(img_bgr, self.img_size, interpolation=cv2.INTER_LINEAR)\n                img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n                if self.is_train and random.random() < self.augmentation_prob:\n                    img_gray = cv2.flip(img_gray, 1)\n                img_normalized = img_gray.astype(np.float32) / 255.0\n                frames.append(img_normalized)\n        except Exception as e:\n            # print(f\"  [VSRDataset __getitem__] ERREUR: Index {idx} ({segment_dir_path}): {e}\") # Less verbose\n            return None\n\n        if not frames: return None\n        frames_tensor = torch.tensor(np.array(frames), dtype=torch.float32).unsqueeze(1)\n        text_indices = self.char_map.text_to_indices(text_label)\n        if not text_indices: return None\n        text_tensor = torch.tensor(text_indices, dtype=torch.long)\n        return frames_tensor, text_tensor\n\ndef collate_fn(batch, char_map_instance_for_collate):\n    batch = [item for item in batch if item is not None]\n    if not batch:\n        # print(\"  [collate_fn] Warning: Received an entirely empty batch after filtering Nones.\") # Less verbose\n        return None\n\n    sequences, targets = zip(*batch)\n    seq_lengths = torch.tensor([s.size(0) for s in sequences], dtype=torch.long)\n    target_lengths = torch.tensor([t.size(0) for t in targets], dtype=torch.long)\n\n    padded_sequences = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0.0)\n    padded_targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=char_map_instance_for_collate.get_blank_token_idx())\n    return padded_sequences, padded_targets, seq_lengths, target_lengths\n\nprint(\"--- VSRDataset Class (Directory Mode) and collate_fn Defined ---\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:59:51.887581Z","iopub.execute_input":"2025-05-16T19:59:51.888270Z","iopub.status.idle":"2025-05-16T19:59:51.905587Z","shell.execute_reply.started":"2025-05-16T19:59:51.888245Z","shell.execute_reply":"2025-05-16T19:59:51.904759Z"}},"outputs":[{"name":"stdout","text":"--- Defining VSRDataset Class and collate_fn ---\n--- VSRDataset Class (Directory Mode) and collate_fn Defined ---\n\n","output_type":"stream"}],"execution_count":5},{"id":"b2440e87-a15b-48c8-a0a6-4634e7c4d8fb","cell_type":"code","source":"# Cell 3: New Model Definition - NewVSRModel\nprint(\"--- Defining NewVSRModel Class ---\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass NewVSRModel(nn.Module):\n    def __init__(self, num_classes, input_channel=1, rnn_hidden_size=256, rnn_num_layers=2, bottleneck_dim=256):\n        super(NewVSRModel, self).__init__()\n        # print(f\"  [NewVSRModel __init__] Initializing model with num_classes={num_classes}, RNN hidden_size={rnn_hidden_size}, Bottleneck={bottleneck_dim}\") # Less verbose\n\n        self.frontend3D = nn.Sequential(\n            nn.Conv3d(input_channel, 64, kernel_size=(3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2), bias=False),\n            nn.BatchNorm3d(64), nn.ReLU(inplace=True),\n            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),\n            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False),\n            nn.BatchNorm3d(128), nn.ReLU(inplace=True),\n            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=(0, 0, 0))\n        )\n        self.frontend3D_output_channels = 128\n\n        self.backend2D = nn.Sequential(\n            nn.Conv2d(self.frontend3D_output_channels, 256, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(512), nn.ReLU(inplace=True)\n        )\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((1,1))\n        self.backend2D_output_features = 512\n\n        self.bottleneck_linear = nn.Linear(self.backend2D_output_features, bottleneck_dim)\n        self.bottleneck_relu = nn.ReLU(inplace=True)\n\n        self.blstm1 = nn.LSTM(bottleneck_dim, rnn_hidden_size, num_layers=rnn_num_layers,\n                              bidirectional=True, batch_first=True, dropout=0.2 if rnn_num_layers > 1 else 0)\n        self.blstm2 = nn.LSTM(rnn_hidden_size * 2, rnn_hidden_size, num_layers=rnn_num_layers,\n                              bidirectional=True, batch_first=True, dropout=0.2 if rnn_num_layers > 1 else 0)\n        self.output_linear = nn.Linear(rnn_hidden_size * 2, num_classes)\n        # print(\"  [NewVSRModel __init__] Model Initialization Complete.\") # Less verbose\n\n    def forward(self, x):\n        B, T, C, H, W = x.shape\n        x = x.permute(0, 2, 1, 3, 4)\n        x = self.frontend3D(x)\n        x = x.permute(0, 2, 1, 3, 4)\n        C_3D_out = x.size(2)\n        x = x.contiguous().view(B * T, C_3D_out, x.size(3), x.size(4))\n        x = self.backend2D(x)\n        x = self.adaptive_pool(x)\n        x = x.view(B * T, -1)\n        x = x.view(B, T, self.backend2D_output_features)\n        x = self.bottleneck_linear(x)\n        x = self.bottleneck_relu(x)\n        self.blstm1.flatten_parameters()\n        x, _ = self.blstm1(x)\n        self.blstm2.flatten_parameters()\n        x, _ = self.blstm2(x)\n        x = self.output_linear(x)\n        output_log_probs = F.log_softmax(x, dim=2)\n        return output_log_probs\n\nprint(\"--- NewVSRModel Class Defined ---\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T19:59:55.703827Z","iopub.execute_input":"2025-05-16T19:59:55.704103Z","iopub.status.idle":"2025-05-16T19:59:55.716023Z","shell.execute_reply.started":"2025-05-16T19:59:55.704081Z","shell.execute_reply":"2025-05-16T19:59:55.715193Z"}},"outputs":[{"name":"stdout","text":"--- Defining NewVSRModel Class ---\n--- NewVSRModel Class Defined ---\n\n","output_type":"stream"}],"execution_count":6},{"id":"e1c057f1-fd99-47be-b746-1cfa24bb86b4","cell_type":"code","source":"# Cell 4: Configuration\nprint(\"--- Configuring Training Parameters ---\")\nimport torch\nimport torch.nn as nn # For CTCLoss definition later\nimport torch.optim as optim # For Optimizer definition later\nfrom functools import partial\nimport os\n\n# --- Path Configurations ---\nKAGGLE_DATASET_ROOT_DIR = \"/kaggle/input/30fpsdata/DATA\" # Adjust if your data path changes\n\n# Path for LOADING checkpoint to RESUME training\nPRETRAINED_WEIGHTS_FILENAME = \"vsr_checkpoint_16_05_01am.pth\" # FILENAME OF THE CHECKPOINT TO LOAD\nPRETRAINED_WEIGHTS_PATH = f\"/kaggle/input/trained-on-30fps-data/pytorch/default/1/{PRETRAINED_WEIGHTS_FILENAME}\"\n\n# Path for SAVING NEW checkpoints during this training session\nNEW_CHECKPOINT_SAVE_FILENAME = \"vsr_checkpoint_16_05_09pm.pth\" # New name for checkpoints saved during this run\nNEW_CHECKPOINT_SAVE_PATH = f\"/kaggle/working/{NEW_CHECKPOINT_SAVE_FILENAME}\"\n# Path for saving the final model after this training session\nFINAL_MODEL_SAVE_FILENAME = \"model_fully_trained_16_05_09pm.pth\"\nFINAL_MODEL_SAVE_PATH = f\"/kaggle/working/{FINAL_MODEL_SAVE_FILENAME}\"\n\nprint(f\"Dataset Root Directory: {KAGGLE_DATASET_ROOT_DIR}\")\nprint(f\"Loading checkpoint from: {PRETRAINED_WEIGHTS_PATH}\")\nprint(f\"Saving new checkpoints to: {NEW_CHECKPOINT_SAVE_PATH}\")\nprint(f\"Saving final model to: {FINAL_MODEL_SAVE_PATH}\")\n\n# --- Image Parameters ---\nIMG_HEIGHT = 96\nIMG_WIDTH = 96\nprint(f\"Image Size set to: ({IMG_HEIGHT}, {IMG_WIDTH})\")\n\n# --- Model Hyperparameters (ensure these match the NewVSRModel used for the checkpoint) ---\nGRU_HIDDEN_DIM = 256\nGRU_NUM_LAYERS = 2 # As used in your NewVSRModel instantiation\nBOTTLENECK_DIM = 256 # As used in your NewVSRModel instantiation\n\n# --- Training Hyperparameters ---\nNUM_EPOCHS = 200 # Total epochs desired (e.g., if checkpoint is at epoch 10 and you want 10 more, set to 20)\nBATCH_SIZE = 4\nLEARNING_RATE_OTHERS = 1e-4 # Initial LR for optimizer (will be loaded from checkpoint if available)\n# Note: LEARNING_RATE_RESNET was for a different model structure. For NewVSRModel, optimizer groups might be different or a single LR used.\n# The optimizer state from the checkpoint will manage LRs if loaded correctly.\n\nNUM_WORKERS = 2\nstart_epoch = 0 # Will be updated if loaded from checkpoint\n\nprint(f\"Target Total Epochs: {NUM_EPOCHS}, Batch Size: {BATCH_SIZE}, Initial LR: {LEARNING_RATE_OTHERS}\")\n\n# --- Alphabet and CharMap ---\narabic_alphabet_str = \" اأبتثجحخدذرزسشصضطظعغفقكلمنهويىءؤئآة\"\ntry:\n    char_map_instance = CharMap(arabic_alphabet_str)\n    NUM_CLASSES = char_map_instance.vocab_size\n    print(f\"Alphabet defined. Number of classes (incl. blank): {NUM_CLASSES}\")\nexcept NameError:\n     print(\"ERREUR: CharMap class not defined. Please run Cell 1 first.\")\n     char_map_instance = None\n     NUM_CLASSES = -1 # Should not happen\n\n# --- Device Setup ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif device.type == 'cuda':\n    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n\n# --- Collate Function ---\ncollate_function_with_charmap = None\nif 'collate_fn' in globals() and char_map_instance:\n    collate_function_with_charmap = partial(collate_fn, char_map_instance_for_collate=char_map_instance)\n    print(\"Partial collate function created.\")\nelse:\n    print(\"ERREUR: collate_fn or char_map_instance not defined for partial collate_fn.\")\n\nprint(\"--- Configuration Done ---\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:02:22.531870Z","iopub.execute_input":"2025-05-16T20:02:22.532278Z","iopub.status.idle":"2025-05-16T20:02:22.541404Z","shell.execute_reply.started":"2025-05-16T20:02:22.532256Z","shell.execute_reply":"2025-05-16T20:02:22.540590Z"}},"outputs":[{"name":"stdout","text":"--- Configuring Training Parameters ---\nDataset Root Directory: /kaggle/input/30fpsdata/DATA\nLoading checkpoint from: /kaggle/input/trained-on-30fps-data/pytorch/default/1/vsr_checkpoint_16_05_01am.pth\nSaving new checkpoints to: /kaggle/working/vsr_checkpoint_16_05_09pm.pth\nSaving final model to: /kaggle/working/model_fully_trained_16_05_09pm.pth\nImage Size set to: (96, 96)\nTarget Total Epochs: 200, Batch Size: 4, Initial LR: 0.0001\nCharMap initialisé. Taille du vocabulaire (avec BLANK) : 37\nAlphabet defined. Number of classes (incl. blank): 37\nUsing device: cuda\nCUDA Device Name: Tesla P100-PCIE-16GB\nPartial collate function created.\n--- Configuration Done ---\n\n","output_type":"stream"}],"execution_count":10},{"id":"d5bc2650-ee7d-4f07-a19d-2efbe9debf30","cell_type":"code","source":"# Cell 5: Initialize Components & Load Checkpoint\nprint(\"--- Initializing Training Components & Loading Checkpoint ---\")\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn as nn\nimport traceback\nimport os\n\n# Initialize components to None\ntrain_dataset = None\ntrain_loader = None\nmodel = None\noptimizer = None\nctc_loss_fn = None\n# start_epoch is already initialized in Cell 4\n\n# --- 1. Instantiate Dataset and DataLoader ---\nif KAGGLE_DATASET_ROOT_DIR and char_map_instance and collate_function_with_charmap:\n    try:\n        print(\"  [Init Components] Instantiating VSRDataset...\")\n        train_dataset = VSRDataset(\n            data_root_dir=KAGGLE_DATASET_ROOT_DIR,\n            char_map_instance=char_map_instance,\n            img_size=(IMG_HEIGHT, IMG_WIDTH),\n            is_train=True, # Set to True for training\n            augmentation_prob=0.5, # Or your desired value\n            video_metadata_suffix=\"_metadata.csv\",\n            segment_path_col='archive_path',\n            text_col='text'\n        )\n        if len(train_dataset) == 0:\n            print(\"  [Init Components] ERREUR: train_dataset is empty.\")\n            train_dataset = None # Ensure it's None if empty\n        else:\n            print(f\"  [Init Components] VSRDataset created with {len(train_dataset)} samples.\")\n            print(\"  [Init Components] Creating DataLoader...\")\n            train_loader = DataLoader(\n                dataset=train_dataset,\n                batch_size=BATCH_SIZE,\n                shuffle=True,\n                collate_fn=collate_function_with_charmap,\n                num_workers=NUM_WORKERS,\n                pin_memory=True if device.type == \"cuda\" else False,\n                persistent_workers=True if NUM_WORKERS > 0 else False\n            )\n            print(f\"  [Init Components] DataLoader created with {len(train_loader)} batches.\")\n    except Exception as e:\n        print(f\"  [Init Components] ERREUR during Dataset/DataLoader creation: {e}\"); traceback.print_exc()\n        train_loader = None\n        train_dataset = None\nelse:\n    print(\"  [Init Components] Skipping Dataset/DataLoader: Missing config (e.g., data_root_dir, char_map, collate_fn).\")\n\n\n# --- 2. Instantiate Model, Optimizer, and Load Checkpoint (if loader is ready) ---\nif train_loader:\n    try:\n        print(\"  [Init Components] Instantiating NewVSRModel...\")\n        model = NewVSRModel(\n            num_classes=NUM_CLASSES,\n            input_channel=1, # Assuming grayscale\n            rnn_hidden_size=GRU_HIDDEN_DIM,\n            rnn_num_layers=GRU_NUM_LAYERS,\n            bottleneck_dim=BOTTLENECK_DIM\n        ).to(device)\n        print(f\"  [Init Components] NewVSRModel instantiated on {device}.\")\n\n        print(\"  [Init Components] Defining Optimizer...\")\n        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE_OTHERS)\n        print(\"  [Init Components] AdamW Optimizer defined.\")\n\n        # --- Load Checkpoint ---\n        if os.path.exists(PRETRAINED_WEIGHTS_PATH):\n            print(f\"  [Init Components] Checkpoint found at {PRETRAINED_WEIGHTS_PATH}. Loading...\")\n            checkpoint = torch.load(PRETRAINED_WEIGHTS_PATH, map_location=device)\n\n            if 'model_state_dict' in checkpoint:\n                model.load_state_dict(checkpoint['model_state_dict'])\n                print(\"  [Init Components] Loaded model_state_dict from checkpoint.\")\n            else:\n                print(\"  [Init Components] Warning: model_state_dict not found in checkpoint.\")\n\n            if 'optimizer_state_dict' in checkpoint and optimizer is not None:\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                print(\"  [Init Components] Loaded optimizer_state_dict from checkpoint.\")\n            else:\n                print(\"  [Init Components] Warning: optimizer_state_dict not found or optimizer is None.\")\n\n            if 'epoch' in checkpoint:\n                start_epoch = checkpoint['epoch'] + 1 # Resume from NEXT epoch\n                print(f\"  [Init Components] Resuming training from epoch {start_epoch}.\")\n            else:\n                print(\"  [Init Components] Warning: Epoch number not found in checkpoint. Will use default start_epoch.\")\n            print(\"  [Init Components] Checkpoint loading process finished.\")\n        else:\n            print(f\"  [Init Components] No checkpoint file found at {PRETRAINED_WEIGHTS_PATH}. Starting fresh or with random weights.\")\n\n        # --- 3. Define Loss Function ---\n        print(\"  [Init Components] Defining CTCLoss...\")\n        ctc_loss_fn = nn.CTCLoss(blank=char_map_instance.get_blank_token_idx(),\n                                 reduction='mean',\n                                 zero_infinity=True).to(device)\n        print(\"  [Init Components] CTCLoss defined.\")\n\n    except Exception as e:\n        print(f\"  [Init Components] ERREUR during Model/Optimizer/Loss setup: {e}\")\n        traceback.print_exc()\n        model = None # Ensure components are None if setup fails\n        optimizer = None\n        ctc_loss_fn = None\nelse:\n    print(\"  [Init Components] Skipping Model/Optimizer/Loss setup because DataLoader was not created.\")\n\nprint(f\"--- Initialization of Training Components Done. Training will attempt to resume/start from epoch: {start_epoch} ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:02:25.818199Z","iopub.execute_input":"2025-05-16T20:02:25.819032Z","iopub.status.idle":"2025-05-16T20:02:29.190373Z","shell.execute_reply.started":"2025-05-16T20:02:25.819002Z","shell.execute_reply":"2025-05-16T20:02:29.189598Z"}},"outputs":[{"name":"stdout","text":"--- Initializing Training Components & Loading Checkpoint ---\n  [Init Components] Instantiating VSRDataset...\n  [VSRDataset __init__] Initializing with data_root_dir: /kaggle/input/30fpsdata/DATA\n  [VSRDataset __init__] Scanning for video metadata in: /kaggle/input/30fpsdata/DATA using suffix '_metadata.csv'\n  [VSRDataset __init__] Expecting segment path column: 'archive_path', text column: 'text'\n  [VSRDataset __init__] Scanned 8 potential video directories.\n  [VSRDataset __init__] Total de 1295 échantillons de segments chargés.\n  [Init Components] VSRDataset created with 1295 samples.\n  [Init Components] Creating DataLoader...\n  [Init Components] DataLoader created with 324 batches.\n  [Init Components] Instantiating NewVSRModel...\n  [Init Components] NewVSRModel instantiated on cuda.\n  [Init Components] Defining Optimizer...\n  [Init Components] AdamW Optimizer defined.\n  [Init Components] Checkpoint found at /kaggle/input/trained-on-30fps-data/pytorch/default/1/vsr_checkpoint_16_05_01am.pth. Loading...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3961821092.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(PRETRAINED_WEIGHTS_PATH, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"  [Init Components] Loaded model_state_dict from checkpoint.\n  [Init Components] Loaded optimizer_state_dict from checkpoint.\n  [Init Components] Resuming training from epoch 110.\n  [Init Components] Checkpoint loading process finished.\n  [Init Components] Defining CTCLoss...\n  [Init Components] CTCLoss defined.\n--- Initialization of Training Components Done. Training will attempt to resume/start from epoch: 110 ---\n","output_type":"stream"}],"execution_count":11},{"id":"ff0168c8-678e-4e88-bb1f-a5f2cca923df","cell_type":"code","source":"# Cell 6: Training Loop\nprint(\"--- Starting Training Loop ---\")\nimport torch # Ensure torch is in scope\nimport traceback\n\n# Check if all necessary components are ready\nif train_loader and model and ctc_loss_fn and optimizer and char_map_instance:\n    print(f\"Training will run from epoch {start_epoch} up to {NUM_EPOCHS-1} (target total epochs: {NUM_EPOCHS}).\")\n    print(f\"Device: {device}\")\n\n    if len(train_loader) == 0:\n        print(\"  [Training Loop] ERREUR: train_loader is empty. Cannot start training.\")\n    else:\n        total_batches_per_epoch = len(train_loader)\n        print(f\"Total batches per epoch: {total_batches_per_epoch}\")\n\n        model.train() # Ensure model is in training mode initially\n\n        for epoch in range(start_epoch, NUM_EPOCHS):\n            print(f\"\\n--- Starting Epoch {epoch+1}/{NUM_EPOCHS} (current loop index: {epoch}) ---\")\n            model.train() # Set model to training mode at the start of each epoch\n            epoch_loss = 0.0\n            processed_batches_in_epoch = 0\n\n            for batch_idx, data_batch in enumerate(train_loader):\n                if data_batch is None:\n                    print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{total_batches_per_epoch}: Skipper (None batch).\")\n                    continue\n                try:\n                    padded_sequences_batch_first, padded_targets, seq_lengths, target_lengths = data_batch\n\n                    if torch.any(target_lengths <= 0) or torch.any(seq_lengths <= 0):\n                        # print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}: Invalid lengths, skipping.\") # Less verbose\n                        continue\n\n                    padded_sequences_batch_first = padded_sequences_batch_first.to(device)\n                    padded_targets_cpu = padded_targets.cpu() # Targets for CTCLoss on CPU\n                    seq_lengths_cpu = seq_lengths.cpu()\n                    target_lengths_cpu = target_lengths.cpu()\n\n                    optimizer.zero_grad()\n                    output_log_probs = model(padded_sequences_batch_first) # (B, T, NumClasses)\n                    output_log_probs_permuted = output_log_probs.permute(1, 0, 2) # (T, B, NumClasses) for CTCLoss\n\n                    model_output_temporal_len = output_log_probs_permuted.size(0)\n                    # Clamp input_lengths for CTCLoss to be <= model's output T\n                    ctc_input_lengths = torch.clamp(seq_lengths_cpu, max=model_output_temporal_len)\n\n                    if torch.any(ctc_input_lengths <= 0):\n                        # print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}: CTC input length <=0, skipping.\") # Less verbose\n                        continue\n                    \n                    # CTCLoss expects targets on CPU\n                    loss = ctc_loss_fn(output_log_probs_permuted, padded_targets_cpu, ctc_input_lengths, target_lengths_cpu)\n\n                    if torch.isnan(loss) or torch.isinf(loss):\n                        print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}: NaN/Inf loss ({loss.item()}), skipping backward.\")\n                        continue\n\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0) # Optional gradient clipping\n                    optimizer.step()\n\n                    current_loss = loss.item()\n                    epoch_loss += current_loss\n                    processed_batches_in_epoch += 1\n\n                    if (batch_idx + 1) % 20 == 0 or (batch_idx + 1) == total_batches_per_epoch: # Log less frequently\n                        print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{total_batches_per_epoch}, Loss: {current_loss:.6f}\")\n\n                except RuntimeError as e_runtime:\n                    print(f\"  RUNTIME ERROR (Batch {batch_idx+1}): {e_runtime}\")\n                    # print(f\"    CTC Inputs - In: {ctc_input_lengths.tolist()}, Tgt: {target_lengths_cpu.tolist()}, Model T: {model_output_temporal_len}\") # Verbose debug\n                    # traceback.print_exc() # Very verbose\n                    print(\"    Skipping batch due to CTCLoss error (likely target_length > input_length).\")\n                    continue\n                except Exception as e_batch:\n                    print(f\"  UNEXPECTED BATCH ERROR (Batch {batch_idx+1}): {e_batch}\")\n                    traceback.print_exc() # Keep for unexpected errors\n\n            avg_epoch_loss = 0.0\n            if processed_batches_in_epoch > 0:\n                avg_epoch_loss = epoch_loss / processed_batches_in_epoch\n                print(f\"--- Finished Epoch {epoch+1}/{NUM_EPOCHS}, Avg Training Loss: {avg_epoch_loss:.6f} ---\")\n\n                # Save checkpoint\n                print(f\"    Saving checkpoint for epoch {epoch} to {NEW_CHECKPOINT_SAVE_PATH}...\")\n                torch.save({\n                    'epoch': epoch, # Save the epoch that was just completed\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': avg_epoch_loss,\n                }, NEW_CHECKPOINT_SAVE_PATH)\n                print(f\"    Checkpoint saved successfully for epoch {epoch}.\")\n            else:\n                print(f\"--- Finished Epoch {epoch+1}/{NUM_EPOCHS}, No batches processed successfully. ---\")\n\n        print(\"\\n--- Training Loop Finished ---\")\n\n        # Save the final model state dict\n        try:\n            torch.save(model.state_dict(), FINAL_MODEL_SAVE_PATH)\n            print(f\"--- Final trained model state_dict saved to {FINAL_MODEL_SAVE_PATH} ---\")\n        except Exception as e_save:\n            print(f\"--- Error saving final model: {e_save} ---\")\nelse:\n    print(\"--- Training Skipped: Not all components initialized correctly or DataLoader is empty. ---\")\n\nprint(\"--- End of Training Cell ---\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:02:36.379163Z","iopub.execute_input":"2025-05-16T20:02:36.379722Z","iopub.status.idle":"2025-05-16T22:21:11.645321Z","shell.execute_reply.started":"2025-05-16T20:02:36.379698Z","shell.execute_reply":"2025-05-16T22:21:11.644296Z"}},"outputs":[{"name":"stdout","text":"--- Starting Training Loop ---\nTraining will run from epoch 110 up to 199 (target total epochs: 200).\nDevice: cuda\nTotal batches per epoch: 324\n\n--- Starting Epoch 111/200 (current loop index: 110) ---\n  Epoch 111, Batch 20/324, Loss: 1.419597\n  Epoch 111, Batch 40/324, Loss: 0.777421\n  Epoch 111, Batch 60/324, Loss: 0.343232\n  Epoch 111, Batch 80/324, Loss: 0.787901\n  Epoch 111, Batch 100/324, Loss: 0.433051\n  Epoch 111, Batch 120/324, Loss: 0.971184\n  Epoch 111, Batch 140/324, Loss: 0.890646\n  Epoch 111, Batch 160/324, Loss: 0.674916\n  Epoch 111, Batch 180/324, Loss: 0.694932\n  Epoch 111, Batch 200/324, Loss: 1.055144\n  Epoch 111, Batch 220/324, Loss: 1.148416\n  Epoch 111, Batch 240/324, Loss: 0.488117\n  Epoch 111, Batch 260/324, Loss: 0.761283\n  Epoch 111, Batch 280/324, Loss: 0.804959\n  Epoch 111, Batch 300/324, Loss: 0.310797\n  Epoch 111, Batch 320/324, Loss: 0.791819\n  Epoch 111, Batch 324/324, Loss: 1.520169\n--- Finished Epoch 111/200, Avg Training Loss: 0.707434 ---\n    Saving checkpoint for epoch 110 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 110.\n\n--- Starting Epoch 112/200 (current loop index: 111) ---\n  Epoch 112, Batch 20/324, Loss: 0.138514\n  Epoch 112, Batch 40/324, Loss: 0.781516\n  Epoch 112, Batch 60/324, Loss: 1.222682\n  Epoch 112, Batch 80/324, Loss: 0.841711\n  Epoch 112, Batch 100/324, Loss: 0.788522\n  Epoch 112, Batch 120/324, Loss: 1.004439\n  Epoch 112, Batch 140/324, Loss: 0.461942\n  Epoch 112, Batch 160/324, Loss: 1.085326\n  Epoch 112, Batch 180/324, Loss: 0.366768\n  Epoch 112, Batch 200/324, Loss: 0.738267\n  Epoch 112, Batch 220/324, Loss: 0.829770\n  Epoch 112, Batch 240/324, Loss: 0.312676\n  Epoch 112, Batch 260/324, Loss: 0.784683\n  Epoch 112, Batch 280/324, Loss: 1.032048\n  Epoch 112, Batch 300/324, Loss: 0.892671\n  Epoch 112, Batch 320/324, Loss: 0.626787\n  Epoch 112, Batch 324/324, Loss: 0.884284\n--- Finished Epoch 112/200, Avg Training Loss: 0.689896 ---\n    Saving checkpoint for epoch 111 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 111.\n\n--- Starting Epoch 113/200 (current loop index: 112) ---\n  Epoch 113, Batch 20/324, Loss: 0.618929\n  Epoch 113, Batch 40/324, Loss: 0.450887\n  Epoch 113, Batch 60/324, Loss: 0.613414\n  Epoch 113, Batch 80/324, Loss: 1.052069\n  Epoch 113, Batch 100/324, Loss: 0.411379\n  Epoch 113, Batch 120/324, Loss: 0.321674\n  Epoch 113, Batch 140/324, Loss: 1.210293\n  Epoch 113, Batch 160/324, Loss: 1.270636\n  Epoch 113, Batch 180/324, Loss: 0.600588\n  Epoch 113, Batch 200/324, Loss: 0.792847\n  Epoch 113, Batch 220/324, Loss: 0.698218\n  Epoch 113, Batch 240/324, Loss: 0.202822\n  Epoch 113, Batch 260/324, Loss: 0.304781\n  Epoch 113, Batch 280/324, Loss: 0.523032\n  Epoch 113, Batch 300/324, Loss: 0.891109\n  Epoch 113, Batch 320/324, Loss: 0.000000\n  Epoch 113, Batch 324/324, Loss: 0.658961\n--- Finished Epoch 113/200, Avg Training Loss: 0.680307 ---\n    Saving checkpoint for epoch 112 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 112.\n\n--- Starting Epoch 114/200 (current loop index: 113) ---\n  Epoch 114, Batch 20/324, Loss: 0.000000\n  Epoch 114, Batch 40/324, Loss: 0.594253\n  Epoch 114, Batch 60/324, Loss: 0.355690\n  Epoch 114, Batch 80/324, Loss: 0.812079\n  Epoch 114, Batch 100/324, Loss: 1.303257\n  Epoch 114, Batch 120/324, Loss: 0.223579\n  Epoch 114, Batch 140/324, Loss: 1.040810\n  Epoch 114, Batch 160/324, Loss: 0.850892\n  Epoch 114, Batch 180/324, Loss: 1.216332\n  Epoch 114, Batch 200/324, Loss: 0.892673\n  Epoch 114, Batch 220/324, Loss: 0.416345\n  Epoch 114, Batch 240/324, Loss: 0.461366\n  Epoch 114, Batch 260/324, Loss: 0.297971\n  Epoch 114, Batch 280/324, Loss: 0.594693\n  Epoch 114, Batch 300/324, Loss: 0.258575\n  Epoch 114, Batch 320/324, Loss: 1.028858\n  Epoch 114, Batch 324/324, Loss: 0.707955\n--- Finished Epoch 114/200, Avg Training Loss: 0.675637 ---\n    Saving checkpoint for epoch 113 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 113.\n\n--- Starting Epoch 115/200 (current loop index: 114) ---\n  Epoch 115, Batch 20/324, Loss: 0.355537\n  Epoch 115, Batch 40/324, Loss: 0.815273\n  Epoch 115, Batch 60/324, Loss: 0.912431\n  Epoch 115, Batch 80/324, Loss: 0.636926\n  Epoch 115, Batch 100/324, Loss: 0.989341\n  Epoch 115, Batch 120/324, Loss: 0.696551\n  Epoch 115, Batch 140/324, Loss: 0.303449\n  Epoch 115, Batch 160/324, Loss: 0.991028\n  Epoch 115, Batch 180/324, Loss: 1.018600\n  Epoch 115, Batch 200/324, Loss: 0.543900\n  Epoch 115, Batch 220/324, Loss: 1.014641\n  Epoch 115, Batch 240/324, Loss: 0.615114\n  Epoch 115, Batch 260/324, Loss: 0.432183\n  Epoch 115, Batch 280/324, Loss: 0.446102\n  Epoch 115, Batch 300/324, Loss: 0.824424\n  Epoch 115, Batch 320/324, Loss: 0.962191\n  Epoch 115, Batch 324/324, Loss: 0.467361\n--- Finished Epoch 115/200, Avg Training Loss: 0.658621 ---\n    Saving checkpoint for epoch 114 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 114.\n\n--- Starting Epoch 116/200 (current loop index: 115) ---\n  Epoch 116, Batch 20/324, Loss: 0.680723\n  Epoch 116, Batch 40/324, Loss: 0.447353\n  Epoch 116, Batch 60/324, Loss: 0.459654\n  Epoch 116, Batch 80/324, Loss: 0.685237\n  Epoch 116, Batch 100/324, Loss: 0.594530\n  Epoch 116, Batch 120/324, Loss: 0.338719\n  Epoch 116, Batch 140/324, Loss: 0.326150\n  Epoch 116, Batch 160/324, Loss: 0.849782\n  Epoch 116, Batch 180/324, Loss: 0.847124\n  Epoch 116, Batch 200/324, Loss: 0.533017\n  Epoch 116, Batch 220/324, Loss: 0.403597\n  Epoch 116, Batch 240/324, Loss: 0.435464\n  Epoch 116, Batch 260/324, Loss: 1.377288\n  Epoch 116, Batch 280/324, Loss: 0.220787\n  Epoch 116, Batch 300/324, Loss: 0.899929\n  Epoch 116, Batch 320/324, Loss: 0.582463\n  Epoch 116, Batch 324/324, Loss: 0.272074\n--- Finished Epoch 116/200, Avg Training Loss: 0.650431 ---\n    Saving checkpoint for epoch 115 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 115.\n\n--- Starting Epoch 117/200 (current loop index: 116) ---\n  Epoch 117, Batch 20/324, Loss: 0.193462\n  Epoch 117, Batch 40/324, Loss: 0.823596\n  Epoch 117, Batch 60/324, Loss: 0.382474\n  Epoch 117, Batch 80/324, Loss: 0.641682\n  Epoch 117, Batch 100/324, Loss: 0.924351\n  Epoch 117, Batch 120/324, Loss: 0.673030\n  Epoch 117, Batch 140/324, Loss: 0.850486\n  Epoch 117, Batch 160/324, Loss: 0.496357\n  Epoch 117, Batch 180/324, Loss: 0.939243\n  Epoch 117, Batch 200/324, Loss: 0.762635\n  Epoch 117, Batch 220/324, Loss: 0.630002\n  Epoch 117, Batch 240/324, Loss: 1.106773\n  Epoch 117, Batch 260/324, Loss: 1.171987\n  Epoch 117, Batch 280/324, Loss: 0.521008\n  Epoch 117, Batch 300/324, Loss: 0.616916\n  Epoch 117, Batch 320/324, Loss: 0.751500\n  Epoch 117, Batch 324/324, Loss: 0.446076\n--- Finished Epoch 117/200, Avg Training Loss: 0.636432 ---\n    Saving checkpoint for epoch 116 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 116.\n\n--- Starting Epoch 118/200 (current loop index: 117) ---\n  Epoch 118, Batch 20/324, Loss: 0.562717\n  Epoch 118, Batch 40/324, Loss: 0.463831\n  Epoch 118, Batch 60/324, Loss: 1.070364\n  Epoch 118, Batch 80/324, Loss: 0.372050\n  Epoch 118, Batch 100/324, Loss: 0.293521\n  Epoch 118, Batch 120/324, Loss: 0.786451\n  Epoch 118, Batch 140/324, Loss: 0.423849\n  Epoch 118, Batch 160/324, Loss: 0.755764\n  Epoch 118, Batch 180/324, Loss: 0.405089\n  Epoch 118, Batch 200/324, Loss: 0.443758\n  Epoch 118, Batch 220/324, Loss: 0.780978\n  Epoch 118, Batch 240/324, Loss: 0.652305\n  Epoch 118, Batch 260/324, Loss: 0.661339\n  Epoch 118, Batch 280/324, Loss: 0.554989\n  Epoch 118, Batch 300/324, Loss: 0.910533\n  Epoch 118, Batch 320/324, Loss: 0.865746\n  Epoch 118, Batch 324/324, Loss: 0.922552\n--- Finished Epoch 118/200, Avg Training Loss: 0.628850 ---\n    Saving checkpoint for epoch 117 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 117.\n\n--- Starting Epoch 119/200 (current loop index: 118) ---\n  Epoch 119, Batch 20/324, Loss: 0.999253\n  Epoch 119, Batch 40/324, Loss: 0.737957\n  Epoch 119, Batch 60/324, Loss: 0.321227\n  Epoch 119, Batch 80/324, Loss: 0.377335\n  Epoch 119, Batch 100/324, Loss: 0.226516\n  Epoch 119, Batch 120/324, Loss: 0.890350\n  Epoch 119, Batch 140/324, Loss: 0.637140\n  Epoch 119, Batch 160/324, Loss: 1.215486\n  Epoch 119, Batch 180/324, Loss: 0.783082\n  Epoch 119, Batch 200/324, Loss: 0.887031\n  Epoch 119, Batch 220/324, Loss: 0.586972\n  Epoch 119, Batch 240/324, Loss: 0.743686\n  Epoch 119, Batch 260/324, Loss: 1.186039\n  Epoch 119, Batch 280/324, Loss: 0.653897\n  Epoch 119, Batch 300/324, Loss: 0.603986\n  Epoch 119, Batch 320/324, Loss: 0.918346\n  Epoch 119, Batch 324/324, Loss: 0.520023\n--- Finished Epoch 119/200, Avg Training Loss: 0.616225 ---\n    Saving checkpoint for epoch 118 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 118.\n\n--- Starting Epoch 120/200 (current loop index: 119) ---\n  Epoch 120, Batch 20/324, Loss: 1.199463\n  Epoch 120, Batch 40/324, Loss: 0.705051\n  Epoch 120, Batch 60/324, Loss: 0.780584\n  Epoch 120, Batch 80/324, Loss: 0.655903\n  Epoch 120, Batch 100/324, Loss: 0.482284\n  Epoch 120, Batch 120/324, Loss: 0.434045\n  Epoch 120, Batch 140/324, Loss: 0.911905\n  Epoch 120, Batch 160/324, Loss: 0.954053\n  Epoch 120, Batch 180/324, Loss: 0.724584\n  Epoch 120, Batch 200/324, Loss: 0.416798\n  Epoch 120, Batch 220/324, Loss: 0.931572\n  Epoch 120, Batch 240/324, Loss: 0.624793\n  Epoch 120, Batch 260/324, Loss: 0.680051\n  Epoch 120, Batch 280/324, Loss: 0.189333\n  Epoch 120, Batch 300/324, Loss: 0.909149\n  Epoch 120, Batch 320/324, Loss: 0.078951\n  Epoch 120, Batch 324/324, Loss: 1.014061\n--- Finished Epoch 120/200, Avg Training Loss: 0.607522 ---\n    Saving checkpoint for epoch 119 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 119.\n\n--- Starting Epoch 121/200 (current loop index: 120) ---\n  Epoch 121, Batch 20/324, Loss: 0.601203\n  Epoch 121, Batch 40/324, Loss: 0.572522\n  Epoch 121, Batch 60/324, Loss: 0.628157\n  Epoch 121, Batch 80/324, Loss: 0.654942\n  Epoch 121, Batch 100/324, Loss: 0.398766\n  Epoch 121, Batch 120/324, Loss: 0.345809\n  Epoch 121, Batch 140/324, Loss: 0.156390\n  Epoch 121, Batch 160/324, Loss: 0.671303\n  Epoch 121, Batch 180/324, Loss: 1.102255\n  Epoch 121, Batch 200/324, Loss: 0.370539\n  Epoch 121, Batch 220/324, Loss: 0.522501\n  Epoch 121, Batch 240/324, Loss: 0.722319\n  Epoch 121, Batch 260/324, Loss: 0.368728\n  Epoch 121, Batch 280/324, Loss: 1.112764\n  Epoch 121, Batch 300/324, Loss: 0.398270\n  Epoch 121, Batch 320/324, Loss: 0.341626\n  Epoch 121, Batch 324/324, Loss: 0.838451\n--- Finished Epoch 121/200, Avg Training Loss: 0.594193 ---\n    Saving checkpoint for epoch 120 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 120.\n\n--- Starting Epoch 122/200 (current loop index: 121) ---\n  Epoch 122, Batch 20/324, Loss: 0.378618\n  Epoch 122, Batch 40/324, Loss: 0.532206\n  Epoch 122, Batch 60/324, Loss: 0.793701\n  Epoch 122, Batch 80/324, Loss: 0.402058\n  Epoch 122, Batch 100/324, Loss: 0.306337\n  Epoch 122, Batch 120/324, Loss: 0.588837\n  Epoch 122, Batch 140/324, Loss: 0.248496\n  Epoch 122, Batch 160/324, Loss: 0.510941\n  Epoch 122, Batch 180/324, Loss: 0.454519\n  Epoch 122, Batch 200/324, Loss: 0.804031\n  Epoch 122, Batch 220/324, Loss: 0.566953\n  Epoch 122, Batch 240/324, Loss: 0.970915\n  Epoch 122, Batch 260/324, Loss: 1.333611\n  Epoch 122, Batch 280/324, Loss: 0.916344\n  Epoch 122, Batch 300/324, Loss: 0.804724\n  Epoch 122, Batch 320/324, Loss: 0.472275\n  Epoch 122, Batch 324/324, Loss: 0.614013\n--- Finished Epoch 122/200, Avg Training Loss: 0.584563 ---\n    Saving checkpoint for epoch 121 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 121.\n\n--- Starting Epoch 123/200 (current loop index: 122) ---\n  Epoch 123, Batch 20/324, Loss: 0.308655\n  Epoch 123, Batch 40/324, Loss: 0.473501\n  Epoch 123, Batch 60/324, Loss: 0.476492\n  Epoch 123, Batch 80/324, Loss: 0.774800\n  Epoch 123, Batch 100/324, Loss: 0.341372\n  Epoch 123, Batch 120/324, Loss: 0.039302\n  Epoch 123, Batch 140/324, Loss: 0.645582\n  Epoch 123, Batch 160/324, Loss: 0.278776\n  Epoch 123, Batch 180/324, Loss: 0.921311\n  Epoch 123, Batch 200/324, Loss: 0.569934\n  Epoch 123, Batch 220/324, Loss: 0.653399\n  Epoch 123, Batch 240/324, Loss: 0.353993\n  Epoch 123, Batch 260/324, Loss: 0.852421\n  Epoch 123, Batch 280/324, Loss: 0.465202\n  Epoch 123, Batch 300/324, Loss: 1.103600\n  Epoch 123, Batch 320/324, Loss: 0.555212\n  Epoch 123, Batch 324/324, Loss: 0.529167\n--- Finished Epoch 123/200, Avg Training Loss: 0.583565 ---\n    Saving checkpoint for epoch 122 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 122.\n\n--- Starting Epoch 124/200 (current loop index: 123) ---\n  Epoch 124, Batch 20/324, Loss: 0.090685\n  Epoch 124, Batch 40/324, Loss: 0.517245\n  Epoch 124, Batch 60/324, Loss: 0.110840\n  Epoch 124, Batch 80/324, Loss: 0.457427\n  Epoch 124, Batch 100/324, Loss: 0.794324\n  Epoch 124, Batch 120/324, Loss: 0.578020\n  Epoch 124, Batch 140/324, Loss: 0.947809\n  Epoch 124, Batch 160/324, Loss: 0.818134\n  Epoch 124, Batch 180/324, Loss: 0.306690\n  Epoch 124, Batch 200/324, Loss: 0.399157\n  Epoch 124, Batch 220/324, Loss: 0.606090\n  Epoch 124, Batch 240/324, Loss: 0.823676\n  Epoch 124, Batch 260/324, Loss: 0.848815\n  Epoch 124, Batch 280/324, Loss: 0.159559\n  Epoch 124, Batch 300/324, Loss: 0.375619\n  Epoch 124, Batch 320/324, Loss: 0.696938\n  Epoch 124, Batch 324/324, Loss: 0.440229\n--- Finished Epoch 124/200, Avg Training Loss: 0.566304 ---\n    Saving checkpoint for epoch 123 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 123.\n\n--- Starting Epoch 125/200 (current loop index: 124) ---\n  Epoch 125, Batch 20/324, Loss: 0.510278\n  Epoch 125, Batch 40/324, Loss: 0.499858\n  Epoch 125, Batch 60/324, Loss: 0.423547\n  Epoch 125, Batch 80/324, Loss: 0.706677\n  Epoch 125, Batch 100/324, Loss: 0.711937\n  Epoch 125, Batch 120/324, Loss: 0.774454\n  Epoch 125, Batch 140/324, Loss: 0.759375\n  Epoch 125, Batch 160/324, Loss: 0.229043\n  Epoch 125, Batch 180/324, Loss: 0.524698\n  Epoch 125, Batch 200/324, Loss: 0.670595\n  Epoch 125, Batch 220/324, Loss: 0.645075\n  Epoch 125, Batch 240/324, Loss: 0.478592\n  Epoch 125, Batch 260/324, Loss: 0.986413\n  Epoch 125, Batch 280/324, Loss: 0.189196\n  Epoch 125, Batch 300/324, Loss: 0.214128\n  Epoch 125, Batch 320/324, Loss: 0.688080\n  Epoch 125, Batch 324/324, Loss: 0.000000\n--- Finished Epoch 125/200, Avg Training Loss: 0.552876 ---\n    Saving checkpoint for epoch 124 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 124.\n\n--- Starting Epoch 126/200 (current loop index: 125) ---\n  Epoch 126, Batch 20/324, Loss: 0.713146\n  Epoch 126, Batch 40/324, Loss: 0.767281\n  Epoch 126, Batch 60/324, Loss: 0.385368\n  Epoch 126, Batch 80/324, Loss: 0.330641\n  Epoch 126, Batch 100/324, Loss: 0.723164\n  Epoch 126, Batch 120/324, Loss: 0.501823\n  Epoch 126, Batch 140/324, Loss: 0.298704\n  Epoch 126, Batch 160/324, Loss: 0.814340\n  Epoch 126, Batch 180/324, Loss: 0.615299\n  Epoch 126, Batch 200/324, Loss: 0.301572\n  Epoch 126, Batch 220/324, Loss: 0.176774\n  Epoch 126, Batch 240/324, Loss: 0.538744\n  Epoch 126, Batch 260/324, Loss: 0.127647\n  Epoch 126, Batch 280/324, Loss: 1.271068\n  Epoch 126, Batch 300/324, Loss: 0.635327\n  Epoch 126, Batch 320/324, Loss: 0.146078\n  Epoch 126, Batch 324/324, Loss: 0.000000\n--- Finished Epoch 126/200, Avg Training Loss: 0.552153 ---\n    Saving checkpoint for epoch 125 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 125.\n\n--- Starting Epoch 127/200 (current loop index: 126) ---\n  Epoch 127, Batch 20/324, Loss: 0.473740\n  Epoch 127, Batch 40/324, Loss: 0.122646\n  Epoch 127, Batch 60/324, Loss: 0.847312\n  Epoch 127, Batch 80/324, Loss: 0.640240\n  Epoch 127, Batch 100/324, Loss: 0.381015\n  Epoch 127, Batch 120/324, Loss: 0.496805\n  Epoch 127, Batch 140/324, Loss: 0.930004\n  Epoch 127, Batch 160/324, Loss: 0.724462\n  Epoch 127, Batch 180/324, Loss: 0.793373\n  Epoch 127, Batch 200/324, Loss: 0.302205\n  Epoch 127, Batch 220/324, Loss: 0.620926\n  Epoch 127, Batch 240/324, Loss: 0.700889\n  Epoch 127, Batch 260/324, Loss: 0.301043\n  Epoch 127, Batch 280/324, Loss: 0.383903\n  Epoch 127, Batch 300/324, Loss: 0.830485\n  Epoch 127, Batch 320/324, Loss: 0.490739\n  Epoch 127, Batch 324/324, Loss: 0.326626\n--- Finished Epoch 127/200, Avg Training Loss: 0.540203 ---\n    Saving checkpoint for epoch 126 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 126.\n\n--- Starting Epoch 128/200 (current loop index: 127) ---\n  Epoch 128, Batch 20/324, Loss: 0.561623\n  Epoch 128, Batch 40/324, Loss: 0.473776\n  Epoch 128, Batch 60/324, Loss: 0.365712\n  Epoch 128, Batch 80/324, Loss: 0.611438\n  Epoch 128, Batch 100/324, Loss: 0.296780\n  Epoch 128, Batch 120/324, Loss: 0.680998\n  Epoch 128, Batch 140/324, Loss: 0.686889\n  Epoch 128, Batch 160/324, Loss: 0.757881\n  Epoch 128, Batch 180/324, Loss: 0.497035\n  Epoch 128, Batch 200/324, Loss: 0.532674\n  Epoch 128, Batch 220/324, Loss: 0.788289\n  Epoch 128, Batch 240/324, Loss: 0.142730\n  Epoch 128, Batch 260/324, Loss: 0.443295\n  Epoch 128, Batch 280/324, Loss: 0.518557\n  Epoch 128, Batch 300/324, Loss: 0.389370\n  Epoch 128, Batch 320/324, Loss: 0.595227\n  Epoch 128, Batch 324/324, Loss: 0.732837\n--- Finished Epoch 128/200, Avg Training Loss: 0.532873 ---\n    Saving checkpoint for epoch 127 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 127.\n\n--- Starting Epoch 129/200 (current loop index: 128) ---\n  Epoch 129, Batch 20/324, Loss: 0.219222\n  Epoch 129, Batch 40/324, Loss: 0.700757\n  Epoch 129, Batch 60/324, Loss: 0.602424\n  Epoch 129, Batch 80/324, Loss: 0.478217\n  Epoch 129, Batch 100/324, Loss: 0.183883\n  Epoch 129, Batch 120/324, Loss: 0.178627\n  Epoch 129, Batch 140/324, Loss: 0.417020\n  Epoch 129, Batch 160/324, Loss: 0.793964\n  Epoch 129, Batch 180/324, Loss: 0.602440\n  Epoch 129, Batch 200/324, Loss: 0.730340\n  Epoch 129, Batch 220/324, Loss: 0.479595\n  Epoch 129, Batch 240/324, Loss: 0.664317\n  Epoch 129, Batch 260/324, Loss: 0.652897\n  Epoch 129, Batch 280/324, Loss: 0.284486\n  Epoch 129, Batch 300/324, Loss: 0.453707\n  Epoch 129, Batch 320/324, Loss: 0.459486\n  Epoch 129, Batch 324/324, Loss: 0.712197\n--- Finished Epoch 129/200, Avg Training Loss: 0.527747 ---\n    Saving checkpoint for epoch 128 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 128.\n\n--- Starting Epoch 130/200 (current loop index: 129) ---\n  Epoch 130, Batch 20/324, Loss: 0.472051\n  Epoch 130, Batch 40/324, Loss: 0.751442\n  Epoch 130, Batch 60/324, Loss: 0.335203\n  Epoch 130, Batch 80/324, Loss: 0.553536\n  Epoch 130, Batch 100/324, Loss: 0.055340\n  Epoch 130, Batch 120/324, Loss: 0.527061\n  Epoch 130, Batch 140/324, Loss: 0.568743\n  Epoch 130, Batch 160/324, Loss: 0.755239\n  Epoch 130, Batch 180/324, Loss: 0.239483\n  Epoch 130, Batch 200/324, Loss: 0.781359\n  Epoch 130, Batch 220/324, Loss: 0.684229\n  Epoch 130, Batch 240/324, Loss: 0.200485\n  Epoch 130, Batch 260/324, Loss: 0.397020\n  Epoch 130, Batch 280/324, Loss: 0.771343\n  Epoch 130, Batch 300/324, Loss: 0.572644\n  Epoch 130, Batch 320/324, Loss: 0.619562\n  Epoch 130, Batch 324/324, Loss: 0.439815\n--- Finished Epoch 130/200, Avg Training Loss: 0.519476 ---\n    Saving checkpoint for epoch 129 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 129.\n\n--- Starting Epoch 131/200 (current loop index: 130) ---\n  Epoch 131, Batch 20/324, Loss: 0.259940\n  Epoch 131, Batch 40/324, Loss: 0.404308\n  Epoch 131, Batch 60/324, Loss: 0.387105\n  Epoch 131, Batch 80/324, Loss: 0.482767\n  Epoch 131, Batch 100/324, Loss: 0.334029\n  Epoch 131, Batch 120/324, Loss: 0.827197\n  Epoch 131, Batch 140/324, Loss: 0.181454\n  Epoch 131, Batch 160/324, Loss: 0.241800\n  Epoch 131, Batch 180/324, Loss: 0.580531\n  Epoch 131, Batch 200/324, Loss: 1.316921\n  Epoch 131, Batch 220/324, Loss: 0.657686\n  Epoch 131, Batch 240/324, Loss: 0.347716\n  Epoch 131, Batch 260/324, Loss: 0.521318\n  Epoch 131, Batch 280/324, Loss: 0.292633\n  Epoch 131, Batch 300/324, Loss: 0.555808\n  Epoch 131, Batch 320/324, Loss: 0.380263\n  Epoch 131, Batch 324/324, Loss: 0.536783\n--- Finished Epoch 131/200, Avg Training Loss: 0.508895 ---\n    Saving checkpoint for epoch 130 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 130.\n\n--- Starting Epoch 132/200 (current loop index: 131) ---\n  Epoch 132, Batch 20/324, Loss: 0.507450\n  Epoch 132, Batch 40/324, Loss: 0.485347\n  Epoch 132, Batch 60/324, Loss: 0.464069\n  Epoch 132, Batch 80/324, Loss: 0.328829\n  Epoch 132, Batch 100/324, Loss: 0.238703\n  Epoch 132, Batch 120/324, Loss: 0.243242\n  Epoch 132, Batch 140/324, Loss: 0.698929\n  Epoch 132, Batch 160/324, Loss: 0.561997\n  Epoch 132, Batch 180/324, Loss: 0.496976\n  Epoch 132, Batch 200/324, Loss: 0.470751\n  Epoch 132, Batch 220/324, Loss: 0.866302\n  Epoch 132, Batch 240/324, Loss: 0.756915\n  Epoch 132, Batch 260/324, Loss: 0.751608\n  Epoch 132, Batch 280/324, Loss: 0.780147\n  Epoch 132, Batch 300/324, Loss: 0.665926\n  Epoch 132, Batch 320/324, Loss: 0.687666\n  Epoch 132, Batch 324/324, Loss: 0.523371\n--- Finished Epoch 132/200, Avg Training Loss: 0.501147 ---\n    Saving checkpoint for epoch 131 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 131.\n\n--- Starting Epoch 133/200 (current loop index: 132) ---\n  Epoch 133, Batch 20/324, Loss: 0.772634\n  Epoch 133, Batch 40/324, Loss: 0.509724\n  Epoch 133, Batch 60/324, Loss: 0.791266\n  Epoch 133, Batch 80/324, Loss: 0.357028\n  Epoch 133, Batch 100/324, Loss: 0.575719\n  Epoch 133, Batch 120/324, Loss: 0.341491\n  Epoch 133, Batch 140/324, Loss: 0.296095\n  Epoch 133, Batch 160/324, Loss: 0.525792\n  Epoch 133, Batch 180/324, Loss: 0.377747\n  Epoch 133, Batch 200/324, Loss: 0.667073\n  Epoch 133, Batch 220/324, Loss: 0.292284\n  Epoch 133, Batch 240/324, Loss: 0.278885\n  Epoch 133, Batch 260/324, Loss: 0.365463\n  Epoch 133, Batch 280/324, Loss: 0.325357\n  Epoch 133, Batch 300/324, Loss: 0.481632\n  Epoch 133, Batch 320/324, Loss: 0.480694\n  Epoch 133, Batch 324/324, Loss: 0.414032\n--- Finished Epoch 133/200, Avg Training Loss: 0.498789 ---\n    Saving checkpoint for epoch 132 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 132.\n\n--- Starting Epoch 134/200 (current loop index: 133) ---\n  Epoch 134, Batch 20/324, Loss: 0.296367\n  Epoch 134, Batch 40/324, Loss: 0.470167\n  Epoch 134, Batch 60/324, Loss: 0.304211\n  Epoch 134, Batch 80/324, Loss: 0.455576\n  Epoch 134, Batch 100/324, Loss: 0.338808\n  Epoch 134, Batch 120/324, Loss: 0.518393\n  Epoch 134, Batch 140/324, Loss: 0.652845\n  Epoch 134, Batch 160/324, Loss: 0.556979\n  Epoch 134, Batch 180/324, Loss: 0.557921\n  Epoch 134, Batch 200/324, Loss: 0.835748\n  Epoch 134, Batch 220/324, Loss: 0.452380\n  Epoch 134, Batch 240/324, Loss: 0.589949\n  Epoch 134, Batch 260/324, Loss: 0.359908\n  Epoch 134, Batch 280/324, Loss: 0.684832\n  Epoch 134, Batch 300/324, Loss: 0.348955\n  Epoch 134, Batch 320/324, Loss: 0.657162\n  Epoch 134, Batch 324/324, Loss: 0.527388\n--- Finished Epoch 134/200, Avg Training Loss: 0.489055 ---\n    Saving checkpoint for epoch 133 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 133.\n\n--- Starting Epoch 135/200 (current loop index: 134) ---\n  Epoch 135, Batch 20/324, Loss: 0.339342\n  Epoch 135, Batch 40/324, Loss: 0.577204\n  Epoch 135, Batch 60/324, Loss: 0.577505\n  Epoch 135, Batch 80/324, Loss: 0.396569\n  Epoch 135, Batch 100/324, Loss: 0.556806\n  Epoch 135, Batch 120/324, Loss: 0.676909\n  Epoch 135, Batch 140/324, Loss: 0.265283\n  Epoch 135, Batch 160/324, Loss: 0.319066\n  Epoch 135, Batch 180/324, Loss: 0.594171\n  Epoch 135, Batch 200/324, Loss: 0.091992\n  Epoch 135, Batch 220/324, Loss: 0.924200\n  Epoch 135, Batch 240/324, Loss: 0.647731\n  Epoch 135, Batch 260/324, Loss: 0.469810\n  Epoch 135, Batch 280/324, Loss: 0.222550\n  Epoch 135, Batch 300/324, Loss: 0.383044\n  Epoch 135, Batch 320/324, Loss: 0.893857\n  Epoch 135, Batch 324/324, Loss: 0.687475\n--- Finished Epoch 135/200, Avg Training Loss: 0.479116 ---\n    Saving checkpoint for epoch 134 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 134.\n\n--- Starting Epoch 136/200 (current loop index: 135) ---\n  Epoch 136, Batch 20/324, Loss: 0.976016\n  Epoch 136, Batch 40/324, Loss: 0.444680\n  Epoch 136, Batch 60/324, Loss: 0.749260\n  Epoch 136, Batch 80/324, Loss: 0.453658\n  Epoch 136, Batch 100/324, Loss: 1.019475\n  Epoch 136, Batch 120/324, Loss: 0.587050\n  Epoch 136, Batch 140/324, Loss: 0.580438\n  Epoch 136, Batch 160/324, Loss: 0.183371\n  Epoch 136, Batch 180/324, Loss: 0.251325\n  Epoch 136, Batch 200/324, Loss: 0.438060\n  Epoch 136, Batch 220/324, Loss: 0.419565\n  Epoch 136, Batch 240/324, Loss: 1.067806\n  Epoch 136, Batch 260/324, Loss: 0.490319\n  Epoch 136, Batch 280/324, Loss: 0.804129\n  Epoch 136, Batch 300/324, Loss: 0.613557\n  Epoch 136, Batch 320/324, Loss: 0.736319\n  Epoch 136, Batch 324/324, Loss: 0.544334\n--- Finished Epoch 136/200, Avg Training Loss: 0.478126 ---\n    Saving checkpoint for epoch 135 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 135.\n\n--- Starting Epoch 137/200 (current loop index: 136) ---\n  Epoch 137, Batch 20/324, Loss: 0.661308\n  Epoch 137, Batch 40/324, Loss: 0.256118\n  Epoch 137, Batch 60/324, Loss: 0.319849\n  Epoch 137, Batch 80/324, Loss: 0.716380\n  Epoch 137, Batch 100/324, Loss: 0.404008\n  Epoch 137, Batch 120/324, Loss: 0.240491\n  Epoch 137, Batch 140/324, Loss: 0.660855\n  Epoch 137, Batch 160/324, Loss: 0.510949\n  Epoch 137, Batch 180/324, Loss: 0.383860\n  Epoch 137, Batch 200/324, Loss: 0.280123\n  Epoch 137, Batch 220/324, Loss: 0.415202\n  Epoch 137, Batch 240/324, Loss: 0.513435\n  Epoch 137, Batch 260/324, Loss: 0.211966\n  Epoch 137, Batch 280/324, Loss: 0.528698\n  Epoch 137, Batch 300/324, Loss: 0.441534\n  Epoch 137, Batch 320/324, Loss: 0.320733\n  Epoch 137, Batch 324/324, Loss: 1.055462\n--- Finished Epoch 137/200, Avg Training Loss: 0.463889 ---\n    Saving checkpoint for epoch 136 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 136.\n\n--- Starting Epoch 138/200 (current loop index: 137) ---\n  Epoch 138, Batch 20/324, Loss: 0.595814\n  Epoch 138, Batch 40/324, Loss: 0.141254\n  Epoch 138, Batch 60/324, Loss: 0.762345\n  Epoch 138, Batch 80/324, Loss: 0.504914\n  Epoch 138, Batch 100/324, Loss: 0.493669\n  Epoch 138, Batch 120/324, Loss: 0.270296\n  Epoch 138, Batch 140/324, Loss: 0.383705\n  Epoch 138, Batch 160/324, Loss: 0.565742\n  Epoch 138, Batch 180/324, Loss: 0.829944\n  Epoch 138, Batch 200/324, Loss: 0.254271\n  Epoch 138, Batch 220/324, Loss: 0.193710\n  Epoch 138, Batch 240/324, Loss: 0.576919\n  Epoch 138, Batch 260/324, Loss: 0.561491\n  Epoch 138, Batch 280/324, Loss: 0.587954\n  Epoch 138, Batch 300/324, Loss: 0.329281\n  Epoch 138, Batch 320/324, Loss: 0.692472\n  Epoch 138, Batch 324/324, Loss: 0.542859\n--- Finished Epoch 138/200, Avg Training Loss: 0.456265 ---\n    Saving checkpoint for epoch 137 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 137.\n\n--- Starting Epoch 139/200 (current loop index: 138) ---\n  Epoch 139, Batch 20/324, Loss: 0.114025\n  Epoch 139, Batch 40/324, Loss: 0.192058\n  Epoch 139, Batch 60/324, Loss: 0.346858\n  Epoch 139, Batch 80/324, Loss: 0.108554\n  Epoch 139, Batch 100/324, Loss: 0.159152\n  Epoch 139, Batch 120/324, Loss: 0.484436\n  Epoch 139, Batch 140/324, Loss: 0.228623\n  Epoch 139, Batch 160/324, Loss: 0.240666\n  Epoch 139, Batch 180/324, Loss: 0.485013\n  Epoch 139, Batch 200/324, Loss: 0.807925\n  Epoch 139, Batch 220/324, Loss: 0.505764\n  Epoch 139, Batch 240/324, Loss: 0.422480\n  Epoch 139, Batch 260/324, Loss: 0.419670\n  Epoch 139, Batch 280/324, Loss: 0.459924\n  Epoch 139, Batch 300/324, Loss: 0.462826\n  Epoch 139, Batch 320/324, Loss: 0.324260\n  Epoch 139, Batch 324/324, Loss: 0.606418\n--- Finished Epoch 139/200, Avg Training Loss: 0.453144 ---\n    Saving checkpoint for epoch 138 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 138.\n\n--- Starting Epoch 140/200 (current loop index: 139) ---\n  Epoch 140, Batch 20/324, Loss: 0.295987\n  Epoch 140, Batch 40/324, Loss: 0.395517\n  Epoch 140, Batch 60/324, Loss: 0.507738\n  Epoch 140, Batch 80/324, Loss: 0.391270\n  Epoch 140, Batch 100/324, Loss: 0.318591\n  Epoch 140, Batch 120/324, Loss: 0.859112\n  Epoch 140, Batch 140/324, Loss: 0.620123\n  Epoch 140, Batch 160/324, Loss: 0.435747\n  Epoch 140, Batch 180/324, Loss: 0.277014\n  Epoch 140, Batch 200/324, Loss: 0.624881\n  Epoch 140, Batch 220/324, Loss: 0.621042\n  Epoch 140, Batch 240/324, Loss: 0.524927\n  Epoch 140, Batch 260/324, Loss: 0.445390\n  Epoch 140, Batch 280/324, Loss: 0.797006\n  Epoch 140, Batch 300/324, Loss: 0.344293\n  Epoch 140, Batch 320/324, Loss: 0.293203\n  Epoch 140, Batch 324/324, Loss: 0.657420\n--- Finished Epoch 140/200, Avg Training Loss: 0.444920 ---\n    Saving checkpoint for epoch 139 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 139.\n\n--- Starting Epoch 141/200 (current loop index: 140) ---\n  Epoch 141, Batch 20/324, Loss: 0.386064\n  Epoch 141, Batch 40/324, Loss: 0.359000\n  Epoch 141, Batch 60/324, Loss: 0.290071\n  Epoch 141, Batch 80/324, Loss: 0.126979\n  Epoch 141, Batch 100/324, Loss: 0.573073\n  Epoch 141, Batch 120/324, Loss: 0.691817\n  Epoch 141, Batch 140/324, Loss: 0.375451\n  Epoch 141, Batch 160/324, Loss: 0.551109\n  Epoch 141, Batch 180/324, Loss: 0.520435\n  Epoch 141, Batch 200/324, Loss: 0.142474\n  Epoch 141, Batch 220/324, Loss: 0.732211\n  Epoch 141, Batch 240/324, Loss: 0.201010\n  Epoch 141, Batch 260/324, Loss: 0.382620\n  Epoch 141, Batch 280/324, Loss: 0.261323\n  Epoch 141, Batch 300/324, Loss: 0.295880\n  Epoch 141, Batch 320/324, Loss: 0.830647\n  Epoch 141, Batch 324/324, Loss: 0.380525\n--- Finished Epoch 141/200, Avg Training Loss: 0.435453 ---\n    Saving checkpoint for epoch 140 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 140.\n\n--- Starting Epoch 142/200 (current loop index: 141) ---\n  Epoch 142, Batch 20/324, Loss: 0.348338\n  Epoch 142, Batch 40/324, Loss: 0.182201\n  Epoch 142, Batch 60/324, Loss: 0.354196\n  Epoch 142, Batch 80/324, Loss: 0.421321\n  Epoch 142, Batch 100/324, Loss: 0.549765\n  Epoch 142, Batch 120/324, Loss: 0.392604\n  Epoch 142, Batch 140/324, Loss: 0.446374\n  Epoch 142, Batch 160/324, Loss: 0.507905\n  Epoch 142, Batch 180/324, Loss: 0.636745\n  Epoch 142, Batch 200/324, Loss: 0.412689\n  Epoch 142, Batch 220/324, Loss: 0.525158\n  Epoch 142, Batch 240/324, Loss: 0.144079\n  Epoch 142, Batch 260/324, Loss: 0.421277\n  Epoch 142, Batch 280/324, Loss: 0.480861\n  Epoch 142, Batch 300/324, Loss: 0.268827\n  Epoch 142, Batch 320/324, Loss: 0.383005\n  Epoch 142, Batch 324/324, Loss: 0.323465\n--- Finished Epoch 142/200, Avg Training Loss: 0.427804 ---\n    Saving checkpoint for epoch 141 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 141.\n\n--- Starting Epoch 143/200 (current loop index: 142) ---\n  Epoch 143, Batch 20/324, Loss: 0.386880\n  Epoch 143, Batch 40/324, Loss: 0.394591\n  Epoch 143, Batch 60/324, Loss: 0.297363\n  Epoch 143, Batch 80/324, Loss: 0.811443\n  Epoch 143, Batch 100/324, Loss: 1.032288\n  Epoch 143, Batch 120/324, Loss: 0.393610\n  Epoch 143, Batch 140/324, Loss: 0.365502\n  Epoch 143, Batch 160/324, Loss: 0.311955\n  Epoch 143, Batch 180/324, Loss: 0.516285\n  Epoch 143, Batch 200/324, Loss: 0.458590\n  Epoch 143, Batch 220/324, Loss: 0.473959\n  Epoch 143, Batch 240/324, Loss: 0.392416\n  Epoch 143, Batch 260/324, Loss: 0.393120\n  Epoch 143, Batch 280/324, Loss: 0.291406\n  Epoch 143, Batch 300/324, Loss: 0.357829\n  Epoch 143, Batch 320/324, Loss: 0.141288\n  Epoch 143, Batch 324/324, Loss: 0.271030\n--- Finished Epoch 143/200, Avg Training Loss: 0.426144 ---\n    Saving checkpoint for epoch 142 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 142.\n\n--- Starting Epoch 144/200 (current loop index: 143) ---\n  Epoch 144, Batch 20/324, Loss: 0.307331\n  Epoch 144, Batch 40/324, Loss: 0.797664\n  Epoch 144, Batch 60/324, Loss: 0.359534\n  Epoch 144, Batch 80/324, Loss: 0.378672\n  Epoch 144, Batch 100/324, Loss: 0.169778\n  Epoch 144, Batch 120/324, Loss: 0.460052\n  Epoch 144, Batch 140/324, Loss: 0.765054\n  Epoch 144, Batch 160/324, Loss: 0.104640\n  Epoch 144, Batch 180/324, Loss: 0.337349\n  Epoch 144, Batch 200/324, Loss: 0.384316\n  Epoch 144, Batch 220/324, Loss: 0.228397\n  Epoch 144, Batch 240/324, Loss: 0.356058\n  Epoch 144, Batch 260/324, Loss: 0.304808\n  Epoch 144, Batch 280/324, Loss: 0.321523\n  Epoch 144, Batch 300/324, Loss: 0.335745\n  Epoch 144, Batch 320/324, Loss: 0.537410\n  Epoch 144, Batch 324/324, Loss: 0.754775\n--- Finished Epoch 144/200, Avg Training Loss: 0.417868 ---\n    Saving checkpoint for epoch 143 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 143.\n\n--- Starting Epoch 145/200 (current loop index: 144) ---\n  Epoch 145, Batch 20/324, Loss: 0.408276\n  Epoch 145, Batch 40/324, Loss: 0.658172\n  Epoch 145, Batch 60/324, Loss: 0.483239\n  Epoch 145, Batch 80/324, Loss: 0.263702\n  Epoch 145, Batch 100/324, Loss: 0.378976\n  Epoch 145, Batch 120/324, Loss: 0.483438\n  Epoch 145, Batch 140/324, Loss: 0.275790\n  Epoch 145, Batch 160/324, Loss: 0.291750\n  Epoch 145, Batch 180/324, Loss: 0.204258\n  Epoch 145, Batch 200/324, Loss: 0.701738\n  Epoch 145, Batch 220/324, Loss: 0.481479\n  Epoch 145, Batch 240/324, Loss: 0.320074\n  Epoch 145, Batch 260/324, Loss: 0.422842\n  Epoch 145, Batch 280/324, Loss: 0.295676\n  Epoch 145, Batch 300/324, Loss: 0.668796\n  Epoch 145, Batch 320/324, Loss: 0.458681\n  Epoch 145, Batch 324/324, Loss: 0.000000\n--- Finished Epoch 145/200, Avg Training Loss: 0.410104 ---\n    Saving checkpoint for epoch 144 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 144.\n\n--- Starting Epoch 146/200 (current loop index: 145) ---\n  Epoch 146, Batch 20/324, Loss: 0.685269\n  Epoch 146, Batch 40/324, Loss: 0.495514\n  Epoch 146, Batch 60/324, Loss: 0.655590\n  Epoch 146, Batch 80/324, Loss: 0.361629\n  Epoch 146, Batch 100/324, Loss: 0.612938\n  Epoch 146, Batch 120/324, Loss: 0.513009\n  Epoch 146, Batch 140/324, Loss: 0.392345\n  Epoch 146, Batch 160/324, Loss: 0.530257\n  Epoch 146, Batch 180/324, Loss: 0.587251\n  Epoch 146, Batch 200/324, Loss: 0.252027\n  Epoch 146, Batch 220/324, Loss: 0.100298\n  Epoch 146, Batch 240/324, Loss: 0.454562\n  Epoch 146, Batch 260/324, Loss: 0.323840\n  Epoch 146, Batch 280/324, Loss: 0.322771\n  Epoch 146, Batch 300/324, Loss: 0.395834\n  Epoch 146, Batch 320/324, Loss: 0.396533\n  Epoch 146, Batch 324/324, Loss: 0.731682\n--- Finished Epoch 146/200, Avg Training Loss: 0.409693 ---\n    Saving checkpoint for epoch 145 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 145.\n\n--- Starting Epoch 147/200 (current loop index: 146) ---\n  Epoch 147, Batch 20/324, Loss: 0.670401\n  Epoch 147, Batch 40/324, Loss: 0.354826\n  Epoch 147, Batch 60/324, Loss: 0.382387\n  Epoch 147, Batch 80/324, Loss: 0.468486\n  Epoch 147, Batch 100/324, Loss: 0.193738\n  Epoch 147, Batch 120/324, Loss: 0.617828\n  Epoch 147, Batch 140/324, Loss: 0.580336\n  Epoch 147, Batch 160/324, Loss: 0.389382\n  Epoch 147, Batch 180/324, Loss: 0.308452\n  Epoch 147, Batch 200/324, Loss: 0.649980\n  Epoch 147, Batch 220/324, Loss: 0.371329\n  Epoch 147, Batch 240/324, Loss: 0.218980\n  Epoch 147, Batch 260/324, Loss: 0.332733\n  Epoch 147, Batch 280/324, Loss: 0.295200\n  Epoch 147, Batch 300/324, Loss: 0.483354\n  Epoch 147, Batch 320/324, Loss: 0.478838\n  Epoch 147, Batch 324/324, Loss: 0.543751\n--- Finished Epoch 147/200, Avg Training Loss: 0.402813 ---\n    Saving checkpoint for epoch 146 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 146.\n\n--- Starting Epoch 148/200 (current loop index: 147) ---\n  Epoch 148, Batch 20/324, Loss: 0.382118\n  Epoch 148, Batch 40/324, Loss: 0.580027\n  Epoch 148, Batch 60/324, Loss: 0.332664\n  Epoch 148, Batch 80/324, Loss: 0.396834\n  Epoch 148, Batch 100/324, Loss: 0.378335\n  Epoch 148, Batch 120/324, Loss: 0.660568\n  Epoch 148, Batch 140/324, Loss: 0.492443\n  Epoch 148, Batch 160/324, Loss: 0.592739\n  Epoch 148, Batch 180/324, Loss: 0.526039\n  Epoch 148, Batch 200/324, Loss: 0.244627\n  Epoch 148, Batch 220/324, Loss: 0.521928\n  Epoch 148, Batch 240/324, Loss: 0.279973\n  Epoch 148, Batch 260/324, Loss: 0.194266\n  Epoch 148, Batch 280/324, Loss: 0.568902\n  Epoch 148, Batch 300/324, Loss: 0.282347\n  Epoch 148, Batch 320/324, Loss: 0.083352\n  Epoch 148, Batch 324/324, Loss: 0.174551\n--- Finished Epoch 148/200, Avg Training Loss: 0.389592 ---\n    Saving checkpoint for epoch 147 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 147.\n\n--- Starting Epoch 149/200 (current loop index: 148) ---\n  Epoch 149, Batch 20/324, Loss: 0.459616\n  Epoch 149, Batch 40/324, Loss: 0.149467\n  Epoch 149, Batch 60/324, Loss: 0.600511\n  Epoch 149, Batch 80/324, Loss: 0.460309\n  Epoch 149, Batch 100/324, Loss: 0.208530\n  Epoch 149, Batch 120/324, Loss: 0.458575\n  Epoch 149, Batch 140/324, Loss: 0.073999\n  Epoch 149, Batch 160/324, Loss: 0.133370\n  Epoch 149, Batch 180/324, Loss: 0.427677\n  Epoch 149, Batch 200/324, Loss: 0.421019\n  Epoch 149, Batch 220/324, Loss: 0.729457\n  Epoch 149, Batch 240/324, Loss: 0.650525\n  Epoch 149, Batch 260/324, Loss: 0.449049\n  Epoch 149, Batch 280/324, Loss: 0.110405\n  Epoch 149, Batch 300/324, Loss: 0.143946\n  Epoch 149, Batch 320/324, Loss: 0.134286\n  Epoch 149, Batch 324/324, Loss: 0.436973\n--- Finished Epoch 149/200, Avg Training Loss: 0.391048 ---\n    Saving checkpoint for epoch 148 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 148.\n\n--- Starting Epoch 150/200 (current loop index: 149) ---\n  Epoch 150, Batch 20/324, Loss: 0.679106\n  Epoch 150, Batch 40/324, Loss: 0.300686\n  Epoch 150, Batch 60/324, Loss: 0.384210\n  Epoch 150, Batch 80/324, Loss: 0.070632\n  Epoch 150, Batch 100/324, Loss: 0.475554\n  Epoch 150, Batch 120/324, Loss: 0.419892\n  Epoch 150, Batch 140/324, Loss: 0.442980\n  Epoch 150, Batch 160/324, Loss: 0.145904\n  Epoch 150, Batch 180/324, Loss: 0.157495\n  Epoch 150, Batch 200/324, Loss: 0.165551\n  Epoch 150, Batch 220/324, Loss: 0.302854\n  Epoch 150, Batch 240/324, Loss: 0.334514\n  Epoch 150, Batch 260/324, Loss: 0.230916\n  Epoch 150, Batch 280/324, Loss: 0.434691\n  Epoch 150, Batch 300/324, Loss: 0.206130\n  Epoch 150, Batch 320/324, Loss: 0.496233\n  Epoch 150, Batch 324/324, Loss: 0.510572\n--- Finished Epoch 150/200, Avg Training Loss: 0.385307 ---\n    Saving checkpoint for epoch 149 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 149.\n\n--- Starting Epoch 151/200 (current loop index: 150) ---\n  Epoch 151, Batch 20/324, Loss: 0.338625\n  Epoch 151, Batch 40/324, Loss: 0.521906\n  Epoch 151, Batch 60/324, Loss: 0.339735\n  Epoch 151, Batch 80/324, Loss: 0.423350\n  Epoch 151, Batch 100/324, Loss: 0.430433\n  Epoch 151, Batch 120/324, Loss: 0.500249\n  Epoch 151, Batch 140/324, Loss: 0.363382\n  Epoch 151, Batch 160/324, Loss: 0.268542\n  Epoch 151, Batch 180/324, Loss: 0.521793\n  Epoch 151, Batch 200/324, Loss: 0.432957\n  Epoch 151, Batch 220/324, Loss: 0.223589\n  Epoch 151, Batch 240/324, Loss: 0.262399\n  Epoch 151, Batch 260/324, Loss: 0.167101\n  Epoch 151, Batch 280/324, Loss: 0.450199\n  Epoch 151, Batch 300/324, Loss: 0.301697\n  Epoch 151, Batch 320/324, Loss: 0.585690\n  Epoch 151, Batch 324/324, Loss: 0.347683\n--- Finished Epoch 151/200, Avg Training Loss: 0.380010 ---\n    Saving checkpoint for epoch 150 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 150.\n\n--- Starting Epoch 152/200 (current loop index: 151) ---\n  Epoch 152, Batch 20/324, Loss: 0.494461\n  Epoch 152, Batch 40/324, Loss: 0.682248\n  Epoch 152, Batch 60/324, Loss: 0.316911\n  Epoch 152, Batch 80/324, Loss: 0.398562\n  Epoch 152, Batch 100/324, Loss: 0.353890\n  Epoch 152, Batch 120/324, Loss: 0.402067\n  Epoch 152, Batch 140/324, Loss: 0.421294\n  Epoch 152, Batch 160/324, Loss: 0.212868\n  Epoch 152, Batch 180/324, Loss: 0.617994\n  Epoch 152, Batch 200/324, Loss: 0.449418\n  Epoch 152, Batch 220/324, Loss: 0.238864\n  Epoch 152, Batch 240/324, Loss: 0.437486\n  Epoch 152, Batch 260/324, Loss: 0.426510\n  Epoch 152, Batch 280/324, Loss: 0.447567\n  Epoch 152, Batch 300/324, Loss: 0.381744\n  Epoch 152, Batch 320/324, Loss: 0.547006\n  Epoch 152, Batch 324/324, Loss: 0.315518\n--- Finished Epoch 152/200, Avg Training Loss: 0.378300 ---\n    Saving checkpoint for epoch 151 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 151.\n\n--- Starting Epoch 153/200 (current loop index: 152) ---\n  Epoch 153, Batch 20/324, Loss: 0.373889\n  Epoch 153, Batch 40/324, Loss: 0.510810\n  Epoch 153, Batch 60/324, Loss: 0.489716\n  Epoch 153, Batch 80/324, Loss: 0.333175\n  Epoch 153, Batch 100/324, Loss: 0.203427\n  Epoch 153, Batch 120/324, Loss: 0.261375\n  Epoch 153, Batch 140/324, Loss: 0.336304\n  Epoch 153, Batch 160/324, Loss: 0.391127\n  Epoch 153, Batch 180/324, Loss: 0.269248\n  Epoch 153, Batch 200/324, Loss: 0.590824\n  Epoch 153, Batch 220/324, Loss: 0.272685\n  Epoch 153, Batch 240/324, Loss: 0.446716\n  Epoch 153, Batch 260/324, Loss: 0.338370\n  Epoch 153, Batch 280/324, Loss: 0.311667\n  Epoch 153, Batch 300/324, Loss: 0.299390\n  Epoch 153, Batch 320/324, Loss: 0.429736\n  Epoch 153, Batch 324/324, Loss: 0.000000\n--- Finished Epoch 153/200, Avg Training Loss: 0.361490 ---\n    Saving checkpoint for epoch 152 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 152.\n\n--- Starting Epoch 154/200 (current loop index: 153) ---\n  Epoch 154, Batch 20/324, Loss: 0.320566\n  Epoch 154, Batch 40/324, Loss: 0.177117\n  Epoch 154, Batch 60/324, Loss: 0.530602\n  Epoch 154, Batch 80/324, Loss: 0.154468\n  Epoch 154, Batch 100/324, Loss: 0.299251\n  Epoch 154, Batch 120/324, Loss: 0.229532\n  Epoch 154, Batch 140/324, Loss: 0.574587\n  Epoch 154, Batch 160/324, Loss: 0.213093\n  Epoch 154, Batch 180/324, Loss: 0.254280\n  Epoch 154, Batch 200/324, Loss: 0.483615\n  Epoch 154, Batch 220/324, Loss: 0.269193\n  Epoch 154, Batch 240/324, Loss: 0.392024\n  Epoch 154, Batch 260/324, Loss: 0.331580\n  Epoch 154, Batch 280/324, Loss: 0.420564\n  Epoch 154, Batch 300/324, Loss: 0.585474\n  Epoch 154, Batch 320/324, Loss: 0.213041\n  Epoch 154, Batch 324/324, Loss: 0.345609\n--- Finished Epoch 154/200, Avg Training Loss: 0.361347 ---\n    Saving checkpoint for epoch 153 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 153.\n\n--- Starting Epoch 155/200 (current loop index: 154) ---\n  Epoch 155, Batch 20/324, Loss: 0.283497\n  Epoch 155, Batch 40/324, Loss: 0.533186\n  Epoch 155, Batch 60/324, Loss: 0.291252\n  Epoch 155, Batch 80/324, Loss: 0.181324\n  Epoch 155, Batch 100/324, Loss: 0.233195\n  Epoch 155, Batch 120/324, Loss: 0.203680\n  Epoch 155, Batch 140/324, Loss: 0.295732\n  Epoch 155, Batch 160/324, Loss: 0.460795\n  Epoch 155, Batch 180/324, Loss: 0.189305\n  Epoch 155, Batch 200/324, Loss: 0.304591\n  Epoch 155, Batch 220/324, Loss: 0.378774\n  Epoch 155, Batch 240/324, Loss: 0.366483\n  Epoch 155, Batch 260/324, Loss: 0.175395\n  Epoch 155, Batch 280/324, Loss: 0.102629\n  Epoch 155, Batch 300/324, Loss: 0.415358\n  Epoch 155, Batch 320/324, Loss: 0.311970\n  Epoch 155, Batch 324/324, Loss: 0.244454\n--- Finished Epoch 155/200, Avg Training Loss: 0.357502 ---\n    Saving checkpoint for epoch 154 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 154.\n\n--- Starting Epoch 156/200 (current loop index: 155) ---\n  Epoch 156, Batch 20/324, Loss: 0.185987\n  Epoch 156, Batch 40/324, Loss: 0.374885\n  Epoch 156, Batch 60/324, Loss: 0.436542\n  Epoch 156, Batch 80/324, Loss: 0.181666\n  Epoch 156, Batch 100/324, Loss: 0.382479\n  Epoch 156, Batch 120/324, Loss: 0.269800\n  Epoch 156, Batch 140/324, Loss: 0.162560\n  Epoch 156, Batch 160/324, Loss: 0.161771\n  Epoch 156, Batch 180/324, Loss: 0.295841\n  Epoch 156, Batch 200/324, Loss: 0.148366\n  Epoch 156, Batch 220/324, Loss: 0.617391\n  Epoch 156, Batch 240/324, Loss: 0.374167\n  Epoch 156, Batch 260/324, Loss: 0.437536\n  Epoch 156, Batch 280/324, Loss: 0.478412\n  Epoch 156, Batch 300/324, Loss: 0.525257\n  Epoch 156, Batch 320/324, Loss: 0.297639\n  Epoch 156, Batch 324/324, Loss: 0.188797\n--- Finished Epoch 156/200, Avg Training Loss: 0.352600 ---\n    Saving checkpoint for epoch 155 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 155.\n\n--- Starting Epoch 157/200 (current loop index: 156) ---\n  Epoch 157, Batch 20/324, Loss: 0.338375\n  Epoch 157, Batch 40/324, Loss: 0.257835\n  Epoch 157, Batch 60/324, Loss: 0.130065\n  Epoch 157, Batch 80/324, Loss: 0.450708\n  Epoch 157, Batch 100/324, Loss: 0.315773\n  Epoch 157, Batch 120/324, Loss: 0.420361\n  Epoch 157, Batch 140/324, Loss: 0.408202\n  Epoch 157, Batch 160/324, Loss: 0.263907\n  Epoch 157, Batch 180/324, Loss: 0.245642\n  Epoch 157, Batch 200/324, Loss: 0.480326\n  Epoch 157, Batch 220/324, Loss: 0.429630\n  Epoch 157, Batch 240/324, Loss: 0.178275\n  Epoch 157, Batch 260/324, Loss: 0.571673\n  Epoch 157, Batch 280/324, Loss: 0.220201\n  Epoch 157, Batch 300/324, Loss: 0.477655\n  Epoch 157, Batch 320/324, Loss: 0.218027\n  Epoch 157, Batch 324/324, Loss: 0.305819\n--- Finished Epoch 157/200, Avg Training Loss: 0.344527 ---\n    Saving checkpoint for epoch 156 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 156.\n\n--- Starting Epoch 158/200 (current loop index: 157) ---\n  Epoch 158, Batch 20/324, Loss: 0.462905\n  Epoch 158, Batch 40/324, Loss: 0.057877\n  Epoch 158, Batch 60/324, Loss: 0.260096\n  Epoch 158, Batch 80/324, Loss: 0.056474\n  Epoch 158, Batch 100/324, Loss: 0.399991\n  Epoch 158, Batch 120/324, Loss: 0.221066\n  Epoch 158, Batch 140/324, Loss: 0.289355\n  Epoch 158, Batch 160/324, Loss: 0.210130\n  Epoch 158, Batch 180/324, Loss: 0.445105\n  Epoch 158, Batch 200/324, Loss: 0.487562\n  Epoch 158, Batch 220/324, Loss: 0.358077\n  Epoch 158, Batch 240/324, Loss: 0.592344\n  Epoch 158, Batch 260/324, Loss: 0.275000\n  Epoch 158, Batch 280/324, Loss: 0.385212\n  Epoch 158, Batch 300/324, Loss: 0.700121\n  Epoch 158, Batch 320/324, Loss: 0.368045\n  Epoch 158, Batch 324/324, Loss: 0.211617\n--- Finished Epoch 158/200, Avg Training Loss: 0.338123 ---\n    Saving checkpoint for epoch 157 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 157.\n\n--- Starting Epoch 159/200 (current loop index: 158) ---\n  Epoch 159, Batch 20/324, Loss: 0.416385\n  Epoch 159, Batch 40/324, Loss: 0.000000\n  Epoch 159, Batch 60/324, Loss: 0.222395\n  Epoch 159, Batch 80/324, Loss: 0.470893\n  Epoch 159, Batch 100/324, Loss: 0.373664\n  Epoch 159, Batch 120/324, Loss: 0.386077\n  Epoch 159, Batch 140/324, Loss: 0.286185\n  Epoch 159, Batch 160/324, Loss: 0.279577\n  Epoch 159, Batch 180/324, Loss: 0.312502\n  Epoch 159, Batch 200/324, Loss: 0.406025\n  Epoch 159, Batch 220/324, Loss: 0.157088\n  Epoch 159, Batch 240/324, Loss: 0.341283\n  Epoch 159, Batch 260/324, Loss: 0.101133\n  Epoch 159, Batch 280/324, Loss: 0.577803\n  Epoch 159, Batch 300/324, Loss: 0.479934\n  Epoch 159, Batch 320/324, Loss: 0.225005\n  Epoch 159, Batch 324/324, Loss: 0.083728\n--- Finished Epoch 159/200, Avg Training Loss: 0.332603 ---\n    Saving checkpoint for epoch 158 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 158.\n\n--- Starting Epoch 160/200 (current loop index: 159) ---\n  Epoch 160, Batch 20/324, Loss: 0.127747\n  Epoch 160, Batch 40/324, Loss: 0.343842\n  Epoch 160, Batch 60/324, Loss: 0.370524\n  Epoch 160, Batch 80/324, Loss: 0.225131\n  Epoch 160, Batch 100/324, Loss: 0.291207\n  Epoch 160, Batch 120/324, Loss: 0.277188\n  Epoch 160, Batch 140/324, Loss: 0.321479\n  Epoch 160, Batch 160/324, Loss: 0.127592\n  Epoch 160, Batch 180/324, Loss: 0.268692\n  Epoch 160, Batch 200/324, Loss: 0.304603\n  Epoch 160, Batch 220/324, Loss: 0.448496\n  Epoch 160, Batch 240/324, Loss: 0.232069\n  Epoch 160, Batch 260/324, Loss: 0.530864\n  Epoch 160, Batch 280/324, Loss: 0.210106\n  Epoch 160, Batch 300/324, Loss: 0.363188\n  Epoch 160, Batch 320/324, Loss: 0.048307\n  Epoch 160, Batch 324/324, Loss: 0.349051\n--- Finished Epoch 160/200, Avg Training Loss: 0.336032 ---\n    Saving checkpoint for epoch 159 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 159.\n\n--- Starting Epoch 161/200 (current loop index: 160) ---\n  Epoch 161, Batch 20/324, Loss: 0.246952\n  Epoch 161, Batch 40/324, Loss: 0.622106\n  Epoch 161, Batch 60/324, Loss: 0.219277\n  Epoch 161, Batch 80/324, Loss: 0.264113\n  Epoch 161, Batch 100/324, Loss: 0.485247\n  Epoch 161, Batch 120/324, Loss: 0.236628\n  Epoch 161, Batch 140/324, Loss: 0.221652\n  Epoch 161, Batch 160/324, Loss: 0.435700\n  Epoch 161, Batch 180/324, Loss: 0.685890\n  Epoch 161, Batch 200/324, Loss: 0.285710\n  Epoch 161, Batch 220/324, Loss: 0.197122\n  Epoch 161, Batch 240/324, Loss: 0.159235\n  Epoch 161, Batch 260/324, Loss: 0.433206\n  Epoch 161, Batch 280/324, Loss: 0.387862\n  Epoch 161, Batch 300/324, Loss: 0.092987\n  Epoch 161, Batch 320/324, Loss: 0.371113\n  Epoch 161, Batch 324/324, Loss: 0.326533\n--- Finished Epoch 161/200, Avg Training Loss: 0.329241 ---\n    Saving checkpoint for epoch 160 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 160.\n\n--- Starting Epoch 162/200 (current loop index: 161) ---\n  Epoch 162, Batch 20/324, Loss: 0.298592\n  Epoch 162, Batch 40/324, Loss: 0.168056\n  Epoch 162, Batch 60/324, Loss: 0.375944\n  Epoch 162, Batch 80/324, Loss: 0.208582\n  Epoch 162, Batch 100/324, Loss: 0.245628\n  Epoch 162, Batch 120/324, Loss: 0.469233\n  Epoch 162, Batch 140/324, Loss: 0.000000\n  Epoch 162, Batch 160/324, Loss: 0.097547\n  Epoch 162, Batch 180/324, Loss: 0.252472\n  Epoch 162, Batch 200/324, Loss: 0.483628\n  Epoch 162, Batch 220/324, Loss: 0.320803\n  Epoch 162, Batch 240/324, Loss: 0.320188\n  Epoch 162, Batch 260/324, Loss: 0.153160\n  Epoch 162, Batch 280/324, Loss: 0.453872\n  Epoch 162, Batch 300/324, Loss: 0.087512\n  Epoch 162, Batch 320/324, Loss: 0.469395\n  Epoch 162, Batch 324/324, Loss: 0.000000\n--- Finished Epoch 162/200, Avg Training Loss: 0.322155 ---\n    Saving checkpoint for epoch 161 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 161.\n\n--- Starting Epoch 163/200 (current loop index: 162) ---\n  Epoch 163, Batch 20/324, Loss: 0.366205\n  Epoch 163, Batch 40/324, Loss: 0.217796\n  Epoch 163, Batch 60/324, Loss: 0.539034\n  Epoch 163, Batch 80/324, Loss: 0.094285\n  Epoch 163, Batch 100/324, Loss: 0.487505\n  Epoch 163, Batch 120/324, Loss: 0.336314\n  Epoch 163, Batch 140/324, Loss: 0.326227\n  Epoch 163, Batch 160/324, Loss: 0.412300\n  Epoch 163, Batch 180/324, Loss: 0.418941\n  Epoch 163, Batch 200/324, Loss: 0.224827\n  Epoch 163, Batch 220/324, Loss: 0.238414\n  Epoch 163, Batch 240/324, Loss: 0.291417\n  Epoch 163, Batch 260/324, Loss: 0.300813\n  Epoch 163, Batch 280/324, Loss: 0.365410\n  Epoch 163, Batch 300/324, Loss: 0.383685\n  Epoch 163, Batch 320/324, Loss: 0.329818\n  Epoch 163, Batch 324/324, Loss: 0.329363\n--- Finished Epoch 163/200, Avg Training Loss: 0.318046 ---\n    Saving checkpoint for epoch 162 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 162.\n\n--- Starting Epoch 164/200 (current loop index: 163) ---\n  Epoch 164, Batch 20/324, Loss: 0.205494\n  Epoch 164, Batch 40/324, Loss: 0.351536\n  Epoch 164, Batch 60/324, Loss: 0.449522\n  Epoch 164, Batch 80/324, Loss: 0.093972\n  Epoch 164, Batch 100/324, Loss: 0.241589\n  Epoch 164, Batch 120/324, Loss: 0.434351\n  Epoch 164, Batch 140/324, Loss: 0.541112\n  Epoch 164, Batch 160/324, Loss: 0.291244\n  Epoch 164, Batch 180/324, Loss: 0.085731\n  Epoch 164, Batch 200/324, Loss: 0.189924\n  Epoch 164, Batch 220/324, Loss: 0.239068\n  Epoch 164, Batch 240/324, Loss: 0.510331\n  Epoch 164, Batch 260/324, Loss: 0.186192\n  Epoch 164, Batch 280/324, Loss: 0.241382\n  Epoch 164, Batch 300/324, Loss: 0.204329\n  Epoch 164, Batch 320/324, Loss: 0.276491\n  Epoch 164, Batch 324/324, Loss: 0.135570\n--- Finished Epoch 164/200, Avg Training Loss: 0.312468 ---\n    Saving checkpoint for epoch 163 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 163.\n\n--- Starting Epoch 165/200 (current loop index: 164) ---\n  Epoch 165, Batch 20/324, Loss: 0.213678\n  Epoch 165, Batch 40/324, Loss: 0.392664\n  Epoch 165, Batch 60/324, Loss: 0.202791\n  Epoch 165, Batch 80/324, Loss: 0.312034\n  Epoch 165, Batch 100/324, Loss: 0.312136\n  Epoch 165, Batch 120/324, Loss: 0.565205\n  Epoch 165, Batch 140/324, Loss: 0.370074\n  Epoch 165, Batch 160/324, Loss: 0.208592\n  Epoch 165, Batch 180/324, Loss: 0.525518\n  Epoch 165, Batch 200/324, Loss: 0.236494\n  Epoch 165, Batch 220/324, Loss: 0.245030\n  Epoch 165, Batch 240/324, Loss: 0.266494\n  Epoch 165, Batch 260/324, Loss: 0.152653\n  Epoch 165, Batch 280/324, Loss: 0.588765\n  Epoch 165, Batch 300/324, Loss: 0.426695\n  Epoch 165, Batch 320/324, Loss: 0.271822\n  Epoch 165, Batch 324/324, Loss: 0.453815\n--- Finished Epoch 165/200, Avg Training Loss: 0.309855 ---\n    Saving checkpoint for epoch 164 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 164.\n\n--- Starting Epoch 166/200 (current loop index: 165) ---\n  Epoch 166, Batch 20/324, Loss: 0.203938\n  Epoch 166, Batch 40/324, Loss: 0.392790\n  Epoch 166, Batch 60/324, Loss: 0.279737\n  Epoch 166, Batch 80/324, Loss: 0.400945\n  Epoch 166, Batch 100/324, Loss: 0.289197\n  Epoch 166, Batch 120/324, Loss: 0.324570\n  Epoch 166, Batch 140/324, Loss: 0.242178\n  Epoch 166, Batch 160/324, Loss: 0.199300\n  Epoch 166, Batch 180/324, Loss: 0.457360\n  Epoch 166, Batch 200/324, Loss: 0.372450\n  Epoch 166, Batch 220/324, Loss: 0.294756\n  Epoch 166, Batch 240/324, Loss: 0.204286\n  Epoch 166, Batch 260/324, Loss: 0.337310\n  Epoch 166, Batch 280/324, Loss: 0.535659\n  Epoch 166, Batch 300/324, Loss: 0.264795\n  Epoch 166, Batch 320/324, Loss: 0.195018\n  Epoch 166, Batch 324/324, Loss: 0.231629\n--- Finished Epoch 166/200, Avg Training Loss: 0.309590 ---\n    Saving checkpoint for epoch 165 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 165.\n\n--- Starting Epoch 167/200 (current loop index: 166) ---\n  Epoch 167, Batch 20/324, Loss: 0.135279\n  Epoch 167, Batch 40/324, Loss: 0.315757\n  Epoch 167, Batch 60/324, Loss: 0.304412\n  Epoch 167, Batch 80/324, Loss: 0.535758\n  Epoch 167, Batch 100/324, Loss: 0.278121\n  Epoch 167, Batch 120/324, Loss: 0.173396\n  Epoch 167, Batch 140/324, Loss: 0.085147\n  Epoch 167, Batch 160/324, Loss: 0.272956\n  Epoch 167, Batch 180/324, Loss: 0.355580\n  Epoch 167, Batch 200/324, Loss: 0.174580\n  Epoch 167, Batch 220/324, Loss: 0.611389\n  Epoch 167, Batch 240/324, Loss: 0.568669\n  Epoch 167, Batch 260/324, Loss: 0.560258\n  Epoch 167, Batch 280/324, Loss: 0.444890\n  Epoch 167, Batch 300/324, Loss: 0.199558\n  Epoch 167, Batch 320/324, Loss: 0.590777\n  Epoch 167, Batch 324/324, Loss: 0.250357\n--- Finished Epoch 167/200, Avg Training Loss: 0.301258 ---\n    Saving checkpoint for epoch 166 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 166.\n\n--- Starting Epoch 168/200 (current loop index: 167) ---\n  Epoch 168, Batch 20/324, Loss: 0.342836\n  Epoch 168, Batch 40/324, Loss: 0.240345\n  Epoch 168, Batch 60/324, Loss: 0.145189\n  Epoch 168, Batch 80/324, Loss: 0.475091\n  Epoch 168, Batch 100/324, Loss: 0.219584\n  Epoch 168, Batch 120/324, Loss: 0.172338\n  Epoch 168, Batch 140/324, Loss: 0.399940\n  Epoch 168, Batch 160/324, Loss: 0.511295\n  Epoch 168, Batch 180/324, Loss: 0.117301\n  Epoch 168, Batch 200/324, Loss: 0.188721\n  Epoch 168, Batch 220/324, Loss: 0.290333\n  Epoch 168, Batch 240/324, Loss: 0.000000\n  Epoch 168, Batch 260/324, Loss: 0.339816\n  Epoch 168, Batch 280/324, Loss: 0.264893\n  Epoch 168, Batch 300/324, Loss: 0.454513\n  Epoch 168, Batch 320/324, Loss: 0.437108\n  Epoch 168, Batch 324/324, Loss: 0.431424\n--- Finished Epoch 168/200, Avg Training Loss: 0.301227 ---\n    Saving checkpoint for epoch 167 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 167.\n\n--- Starting Epoch 169/200 (current loop index: 168) ---\n  Epoch 169, Batch 20/324, Loss: 0.212144\n  Epoch 169, Batch 40/324, Loss: 0.344103\n  Epoch 169, Batch 60/324, Loss: 0.273977\n  Epoch 169, Batch 80/324, Loss: 0.316309\n  Epoch 169, Batch 100/324, Loss: 0.194798\n  Epoch 169, Batch 120/324, Loss: 0.263270\n  Epoch 169, Batch 140/324, Loss: 0.505087\n  Epoch 169, Batch 160/324, Loss: 0.164586\n  Epoch 169, Batch 180/324, Loss: 0.261513\n  Epoch 169, Batch 200/324, Loss: 0.337623\n  Epoch 169, Batch 220/324, Loss: 0.284463\n  Epoch 169, Batch 240/324, Loss: 0.419073\n  Epoch 169, Batch 260/324, Loss: 0.245085\n  Epoch 169, Batch 280/324, Loss: 0.279069\n  Epoch 169, Batch 300/324, Loss: 0.188440\n  Epoch 169, Batch 320/324, Loss: 0.081131\n  Epoch 169, Batch 324/324, Loss: 0.528849\n--- Finished Epoch 169/200, Avg Training Loss: 0.292822 ---\n    Saving checkpoint for epoch 168 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 168.\n\n--- Starting Epoch 170/200 (current loop index: 169) ---\n  Epoch 170, Batch 20/324, Loss: 0.172643\n  Epoch 170, Batch 40/324, Loss: 0.110952\n  Epoch 170, Batch 60/324, Loss: 0.276348\n  Epoch 170, Batch 80/324, Loss: 0.271970\n  Epoch 170, Batch 100/324, Loss: 0.277058\n  Epoch 170, Batch 120/324, Loss: 0.259381\n  Epoch 170, Batch 140/324, Loss: 0.338835\n  Epoch 170, Batch 160/324, Loss: 0.258719\n  Epoch 170, Batch 180/324, Loss: 0.218422\n  Epoch 170, Batch 200/324, Loss: 0.316429\n  Epoch 170, Batch 220/324, Loss: 0.259991\n  Epoch 170, Batch 240/324, Loss: 0.122199\n  Epoch 170, Batch 260/324, Loss: 0.227846\n  Epoch 170, Batch 280/324, Loss: 0.245408\n  Epoch 170, Batch 300/324, Loss: 0.234589\n  Epoch 170, Batch 320/324, Loss: 0.496168\n  Epoch 170, Batch 324/324, Loss: 0.392177\n--- Finished Epoch 170/200, Avg Training Loss: 0.294687 ---\n    Saving checkpoint for epoch 169 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 169.\n\n--- Starting Epoch 171/200 (current loop index: 170) ---\n  Epoch 171, Batch 20/324, Loss: 0.391401\n  Epoch 171, Batch 40/324, Loss: 0.144280\n  Epoch 171, Batch 60/324, Loss: 0.393667\n  Epoch 171, Batch 80/324, Loss: 0.381612\n  Epoch 171, Batch 100/324, Loss: 0.207269\n  Epoch 171, Batch 120/324, Loss: 0.217314\n  Epoch 171, Batch 140/324, Loss: 0.305210\n  Epoch 171, Batch 160/324, Loss: 0.222891\n  Epoch 171, Batch 180/324, Loss: 0.230155\n  Epoch 171, Batch 200/324, Loss: 0.195511\n  Epoch 171, Batch 220/324, Loss: 0.315244\n  Epoch 171, Batch 240/324, Loss: 0.380775\n  Epoch 171, Batch 260/324, Loss: 0.430794\n  Epoch 171, Batch 280/324, Loss: 0.232905\n  Epoch 171, Batch 300/324, Loss: 0.369763\n  Epoch 171, Batch 320/324, Loss: 0.153012\n  Epoch 171, Batch 324/324, Loss: 0.179481\n--- Finished Epoch 171/200, Avg Training Loss: 0.285198 ---\n    Saving checkpoint for epoch 170 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 170.\n\n--- Starting Epoch 172/200 (current loop index: 171) ---\n  Epoch 172, Batch 20/324, Loss: 0.369491\n  Epoch 172, Batch 40/324, Loss: 0.209698\n  Epoch 172, Batch 60/324, Loss: 0.258400\n  Epoch 172, Batch 80/324, Loss: 0.263869\n  Epoch 172, Batch 100/324, Loss: 0.000000\n  Epoch 172, Batch 120/324, Loss: 0.392188\n  Epoch 172, Batch 140/324, Loss: 0.436275\n  Epoch 172, Batch 160/324, Loss: 0.183349\n  Epoch 172, Batch 180/324, Loss: 0.245465\n  Epoch 172, Batch 200/324, Loss: 0.335883\n  Epoch 172, Batch 220/324, Loss: 0.077146\n  Epoch 172, Batch 240/324, Loss: 0.351539\n  Epoch 172, Batch 260/324, Loss: 0.206771\n  Epoch 172, Batch 280/324, Loss: 0.311614\n  Epoch 172, Batch 300/324, Loss: 0.369862\n  Epoch 172, Batch 320/324, Loss: 0.294409\n  Epoch 172, Batch 324/324, Loss: 0.168335\n--- Finished Epoch 172/200, Avg Training Loss: 0.287498 ---\n    Saving checkpoint for epoch 171 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 171.\n\n--- Starting Epoch 173/200 (current loop index: 172) ---\n  Epoch 173, Batch 20/324, Loss: 0.213015\n  Epoch 173, Batch 40/324, Loss: 0.057600\n  Epoch 173, Batch 60/324, Loss: 0.515647\n  Epoch 173, Batch 80/324, Loss: 0.380561\n  Epoch 173, Batch 100/324, Loss: 0.440517\n  Epoch 173, Batch 120/324, Loss: 0.447857\n  Epoch 173, Batch 140/324, Loss: 0.280738\n  Epoch 173, Batch 160/324, Loss: 0.299076\n  Epoch 173, Batch 180/324, Loss: 0.314239\n  Epoch 173, Batch 200/324, Loss: 0.205297\n  Epoch 173, Batch 220/324, Loss: 0.269477\n  Epoch 173, Batch 240/324, Loss: 0.236352\n  Epoch 173, Batch 260/324, Loss: 0.259593\n  Epoch 173, Batch 280/324, Loss: 0.256851\n  Epoch 173, Batch 300/324, Loss: 0.506977\n  Epoch 173, Batch 320/324, Loss: 0.039606\n  Epoch 173, Batch 324/324, Loss: 0.150039\n--- Finished Epoch 173/200, Avg Training Loss: 0.283718 ---\n    Saving checkpoint for epoch 172 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 172.\n\n--- Starting Epoch 174/200 (current loop index: 173) ---\n  Epoch 174, Batch 20/324, Loss: 0.188673\n  Epoch 174, Batch 40/324, Loss: 0.348722\n  Epoch 174, Batch 60/324, Loss: 0.190712\n  Epoch 174, Batch 80/324, Loss: 0.320083\n  Epoch 174, Batch 100/324, Loss: 0.485547\n  Epoch 174, Batch 120/324, Loss: 0.109404\n  Epoch 174, Batch 140/324, Loss: 0.232599\n  Epoch 174, Batch 160/324, Loss: 0.302200\n  Epoch 174, Batch 180/324, Loss: 0.204798\n  Epoch 174, Batch 200/324, Loss: 0.200000\n  Epoch 174, Batch 220/324, Loss: 0.263756\n  Epoch 174, Batch 240/324, Loss: 0.161234\n  Epoch 174, Batch 260/324, Loss: 0.161500\n  Epoch 174, Batch 280/324, Loss: 0.283964\n  Epoch 174, Batch 300/324, Loss: 0.152899\n  Epoch 174, Batch 320/324, Loss: 0.239276\n  Epoch 174, Batch 324/324, Loss: 0.085305\n--- Finished Epoch 174/200, Avg Training Loss: 0.277503 ---\n    Saving checkpoint for epoch 173 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 173.\n\n--- Starting Epoch 175/200 (current loop index: 174) ---\n  Epoch 175, Batch 20/324, Loss: 0.353740\n  Epoch 175, Batch 40/324, Loss: 0.343586\n  Epoch 175, Batch 60/324, Loss: 0.217548\n  Epoch 175, Batch 80/324, Loss: 0.167097\n  Epoch 175, Batch 100/324, Loss: 0.229150\n  Epoch 175, Batch 120/324, Loss: 0.090429\n  Epoch 175, Batch 140/324, Loss: 0.466659\n  Epoch 175, Batch 160/324, Loss: 0.242522\n  Epoch 175, Batch 180/324, Loss: 0.329281\n  Epoch 175, Batch 200/324, Loss: 0.170460\n  Epoch 175, Batch 220/324, Loss: 0.253143\n  Epoch 175, Batch 240/324, Loss: 0.235697\n  Epoch 175, Batch 260/324, Loss: 0.269825\n  Epoch 175, Batch 280/324, Loss: 0.190407\n  Epoch 175, Batch 300/324, Loss: 0.398245\n  Epoch 175, Batch 320/324, Loss: 0.382326\n  Epoch 175, Batch 324/324, Loss: 0.224688\n--- Finished Epoch 175/200, Avg Training Loss: 0.273661 ---\n    Saving checkpoint for epoch 174 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 174.\n\n--- Starting Epoch 176/200 (current loop index: 175) ---\n  Epoch 176, Batch 20/324, Loss: 0.184976\n  Epoch 176, Batch 40/324, Loss: 0.202782\n  Epoch 176, Batch 60/324, Loss: 0.220062\n  Epoch 176, Batch 80/324, Loss: 0.282308\n  Epoch 176, Batch 100/324, Loss: 0.232273\n  Epoch 176, Batch 120/324, Loss: 0.045470\n  Epoch 176, Batch 140/324, Loss: 0.276922\n  Epoch 176, Batch 160/324, Loss: 0.357376\n  Epoch 176, Batch 180/324, Loss: 0.225994\n  Epoch 176, Batch 200/324, Loss: 0.188746\n  Epoch 176, Batch 220/324, Loss: 0.376030\n  Epoch 176, Batch 240/324, Loss: 0.347169\n  Epoch 176, Batch 260/324, Loss: 0.415594\n  Epoch 176, Batch 280/324, Loss: 0.210968\n  Epoch 176, Batch 300/324, Loss: 0.286404\n  Epoch 176, Batch 320/324, Loss: 0.211579\n  Epoch 176, Batch 324/324, Loss: 0.470705\n--- Finished Epoch 176/200, Avg Training Loss: 0.269781 ---\n    Saving checkpoint for epoch 175 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 175.\n\n--- Starting Epoch 177/200 (current loop index: 176) ---\n  Epoch 177, Batch 20/324, Loss: 0.224288\n  Epoch 177, Batch 40/324, Loss: 0.122200\n  Epoch 177, Batch 60/324, Loss: 0.328056\n  Epoch 177, Batch 80/324, Loss: 0.291281\n  Epoch 177, Batch 100/324, Loss: 0.318628\n  Epoch 177, Batch 120/324, Loss: 0.196981\n  Epoch 177, Batch 140/324, Loss: 0.235137\n  Epoch 177, Batch 160/324, Loss: 0.267950\n  Epoch 177, Batch 180/324, Loss: 0.283102\n  Epoch 177, Batch 200/324, Loss: 0.334310\n  Epoch 177, Batch 220/324, Loss: 0.334692\n  Epoch 177, Batch 240/324, Loss: 0.356573\n  Epoch 177, Batch 260/324, Loss: 0.359967\n  Epoch 177, Batch 280/324, Loss: 0.371684\n  Epoch 177, Batch 300/324, Loss: 0.343657\n  Epoch 177, Batch 320/324, Loss: 0.299593\n  Epoch 177, Batch 324/324, Loss: 0.233844\n--- Finished Epoch 177/200, Avg Training Loss: 0.268297 ---\n    Saving checkpoint for epoch 176 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 176.\n\n--- Starting Epoch 178/200 (current loop index: 177) ---\n  Epoch 178, Batch 20/324, Loss: 0.289489\n  Epoch 178, Batch 40/324, Loss: 0.316087\n  Epoch 178, Batch 60/324, Loss: 0.310833\n  Epoch 178, Batch 80/324, Loss: 0.317068\n  Epoch 178, Batch 100/324, Loss: 0.250083\n  Epoch 178, Batch 120/324, Loss: 0.344171\n  Epoch 178, Batch 140/324, Loss: 0.211398\n  Epoch 178, Batch 160/324, Loss: 0.304023\n  Epoch 178, Batch 180/324, Loss: 0.408660\n  Epoch 178, Batch 200/324, Loss: 0.260320\n  Epoch 178, Batch 220/324, Loss: 0.170447\n  Epoch 178, Batch 240/324, Loss: 0.305807\n  Epoch 178, Batch 260/324, Loss: 0.483183\n  Epoch 178, Batch 280/324, Loss: 0.211536\n  Epoch 178, Batch 300/324, Loss: 0.283038\n  Epoch 178, Batch 320/324, Loss: 0.376446\n  Epoch 178, Batch 324/324, Loss: 0.450512\n--- Finished Epoch 178/200, Avg Training Loss: 0.263445 ---\n    Saving checkpoint for epoch 177 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 177.\n\n--- Starting Epoch 179/200 (current loop index: 178) ---\n  Epoch 179, Batch 20/324, Loss: 0.500742\n  Epoch 179, Batch 40/324, Loss: 0.260883\n  Epoch 179, Batch 60/324, Loss: 0.467383\n  Epoch 179, Batch 80/324, Loss: 0.268608\n  Epoch 179, Batch 100/324, Loss: 0.372145\n  Epoch 179, Batch 120/324, Loss: 0.238069\n  Epoch 179, Batch 140/324, Loss: 0.284729\n  Epoch 179, Batch 160/324, Loss: 0.439893\n  Epoch 179, Batch 180/324, Loss: 0.417452\n  Epoch 179, Batch 200/324, Loss: 0.288163\n  Epoch 179, Batch 220/324, Loss: 0.410073\n  Epoch 179, Batch 240/324, Loss: 0.200416\n  Epoch 179, Batch 260/324, Loss: 0.265403\n  Epoch 179, Batch 280/324, Loss: 0.051927\n  Epoch 179, Batch 300/324, Loss: 0.125826\n  Epoch 179, Batch 320/324, Loss: 0.285598\n  Epoch 179, Batch 324/324, Loss: 0.180254\n--- Finished Epoch 179/200, Avg Training Loss: 0.257776 ---\n    Saving checkpoint for epoch 178 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 178.\n\n--- Starting Epoch 180/200 (current loop index: 179) ---\n  Epoch 180, Batch 20/324, Loss: 0.149406\n  Epoch 180, Batch 40/324, Loss: 0.131024\n  Epoch 180, Batch 60/324, Loss: 0.159504\n  Epoch 180, Batch 80/324, Loss: 0.224130\n  Epoch 180, Batch 100/324, Loss: 0.157494\n  Epoch 180, Batch 120/324, Loss: 0.254142\n  Epoch 180, Batch 140/324, Loss: 0.636151\n  Epoch 180, Batch 160/324, Loss: 0.081654\n  Epoch 180, Batch 180/324, Loss: 0.347548\n  Epoch 180, Batch 200/324, Loss: 0.300656\n  Epoch 180, Batch 220/324, Loss: 0.179876\n  Epoch 180, Batch 240/324, Loss: 0.278073\n  Epoch 180, Batch 260/324, Loss: 0.113601\n  Epoch 180, Batch 280/324, Loss: 0.429041\n  Epoch 180, Batch 300/324, Loss: 0.342999\n  Epoch 180, Batch 320/324, Loss: 0.343024\n  Epoch 180, Batch 324/324, Loss: 0.060818\n--- Finished Epoch 180/200, Avg Training Loss: 0.261426 ---\n    Saving checkpoint for epoch 179 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 179.\n\n--- Starting Epoch 181/200 (current loop index: 180) ---\n  Epoch 181, Batch 20/324, Loss: 0.218542\n  Epoch 181, Batch 40/324, Loss: 0.250505\n  Epoch 181, Batch 60/324, Loss: 0.177464\n  Epoch 181, Batch 80/324, Loss: 0.292891\n  Epoch 181, Batch 100/324, Loss: 0.345079\n  Epoch 181, Batch 120/324, Loss: 0.102024\n  Epoch 181, Batch 140/324, Loss: 0.176634\n  Epoch 181, Batch 160/324, Loss: 0.137852\n  Epoch 181, Batch 180/324, Loss: 0.109171\n  Epoch 181, Batch 200/324, Loss: 0.217335\n  Epoch 181, Batch 220/324, Loss: 0.204707\n  Epoch 181, Batch 240/324, Loss: 0.304919\n  Epoch 181, Batch 260/324, Loss: 0.174792\n  Epoch 181, Batch 280/324, Loss: 0.346657\n  Epoch 181, Batch 300/324, Loss: 0.085196\n  Epoch 181, Batch 320/324, Loss: 0.245034\n  Epoch 181, Batch 324/324, Loss: 0.255198\n--- Finished Epoch 181/200, Avg Training Loss: 0.252783 ---\n    Saving checkpoint for epoch 180 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 180.\n\n--- Starting Epoch 182/200 (current loop index: 181) ---\n  Epoch 182, Batch 20/324, Loss: 0.299282\n  Epoch 182, Batch 40/324, Loss: 0.327242\n  Epoch 182, Batch 60/324, Loss: 0.083703\n  Epoch 182, Batch 80/324, Loss: 0.444925\n  Epoch 182, Batch 100/324, Loss: 0.253960\n  Epoch 182, Batch 120/324, Loss: 0.291767\n  Epoch 182, Batch 140/324, Loss: 0.117932\n  Epoch 182, Batch 160/324, Loss: 0.027221\n  Epoch 182, Batch 180/324, Loss: 0.350008\n  Epoch 182, Batch 200/324, Loss: 0.104037\n  Epoch 182, Batch 220/324, Loss: 0.222333\n  Epoch 182, Batch 240/324, Loss: 0.214231\n  Epoch 182, Batch 260/324, Loss: 0.160669\n  Epoch 182, Batch 280/324, Loss: 0.178417\n  Epoch 182, Batch 300/324, Loss: 0.346379\n  Epoch 182, Batch 320/324, Loss: 0.236256\n  Epoch 182, Batch 324/324, Loss: 0.375745\n--- Finished Epoch 182/200, Avg Training Loss: 0.247629 ---\n    Saving checkpoint for epoch 181 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 181.\n\n--- Starting Epoch 183/200 (current loop index: 182) ---\n  Epoch 183, Batch 20/324, Loss: 0.142836\n  Epoch 183, Batch 40/324, Loss: 0.039163\n  Epoch 183, Batch 60/324, Loss: 0.164294\n  Epoch 183, Batch 80/324, Loss: 0.160610\n  Epoch 183, Batch 100/324, Loss: 0.238085\n  Epoch 183, Batch 120/324, Loss: 0.341539\n  Epoch 183, Batch 140/324, Loss: 0.289387\n  Epoch 183, Batch 160/324, Loss: 0.207275\n  Epoch 183, Batch 180/324, Loss: 0.144763\n  Epoch 183, Batch 200/324, Loss: 0.399725\n  Epoch 183, Batch 220/324, Loss: 0.287019\n  Epoch 183, Batch 240/324, Loss: 0.106270\n  Epoch 183, Batch 260/324, Loss: 0.176795\n  Epoch 183, Batch 280/324, Loss: 0.169293\n  Epoch 183, Batch 300/324, Loss: 0.198585\n  Epoch 183, Batch 320/324, Loss: 0.200568\n  Epoch 183, Batch 324/324, Loss: 0.219787\n--- Finished Epoch 183/200, Avg Training Loss: 0.243856 ---\n    Saving checkpoint for epoch 182 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 182.\n\n--- Starting Epoch 184/200 (current loop index: 183) ---\n  Epoch 184, Batch 20/324, Loss: 0.269890\n  Epoch 184, Batch 40/324, Loss: 0.060897\n  Epoch 184, Batch 60/324, Loss: 0.219689\n  Epoch 184, Batch 80/324, Loss: 0.106762\n  Epoch 184, Batch 100/324, Loss: 0.084825\n  Epoch 184, Batch 120/324, Loss: 0.117844\n  Epoch 184, Batch 140/324, Loss: 0.251034\n  Epoch 184, Batch 160/324, Loss: 0.324188\n  Epoch 184, Batch 180/324, Loss: 0.076311\n  Epoch 184, Batch 200/324, Loss: 0.117357\n  Epoch 184, Batch 220/324, Loss: 0.268552\n  Epoch 184, Batch 240/324, Loss: 0.148841\n  Epoch 184, Batch 260/324, Loss: 0.029785\n  Epoch 184, Batch 280/324, Loss: 0.338134\n  Epoch 184, Batch 300/324, Loss: 0.392415\n  Epoch 184, Batch 320/324, Loss: 0.284074\n  Epoch 184, Batch 324/324, Loss: 0.581448\n--- Finished Epoch 184/200, Avg Training Loss: 0.240975 ---\n    Saving checkpoint for epoch 183 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 183.\n\n--- Starting Epoch 185/200 (current loop index: 184) ---\n  Epoch 185, Batch 20/324, Loss: 0.106803\n  Epoch 185, Batch 40/324, Loss: 0.366607\n  Epoch 185, Batch 60/324, Loss: 0.185342\n  Epoch 185, Batch 80/324, Loss: 0.320612\n  Epoch 185, Batch 100/324, Loss: 0.142570\n  Epoch 185, Batch 120/324, Loss: 0.191177\n  Epoch 185, Batch 140/324, Loss: 0.173736\n  Epoch 185, Batch 160/324, Loss: 0.229965\n  Epoch 185, Batch 180/324, Loss: 0.218801\n  Epoch 185, Batch 200/324, Loss: 0.096643\n  Epoch 185, Batch 220/324, Loss: 0.289264\n  Epoch 185, Batch 240/324, Loss: 0.203694\n  Epoch 185, Batch 260/324, Loss: 0.395680\n  Epoch 185, Batch 280/324, Loss: 0.100584\n  Epoch 185, Batch 300/324, Loss: 0.146259\n  Epoch 185, Batch 320/324, Loss: 0.276084\n  Epoch 185, Batch 324/324, Loss: 0.231592\n--- Finished Epoch 185/200, Avg Training Loss: 0.243766 ---\n    Saving checkpoint for epoch 184 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 184.\n\n--- Starting Epoch 186/200 (current loop index: 185) ---\n  Epoch 186, Batch 20/324, Loss: 0.254301\n  Epoch 186, Batch 40/324, Loss: 0.452622\n  Epoch 186, Batch 60/324, Loss: 0.100054\n  Epoch 186, Batch 80/324, Loss: 0.463887\n  Epoch 186, Batch 100/324, Loss: 0.201479\n  Epoch 186, Batch 120/324, Loss: 0.154395\n  Epoch 186, Batch 140/324, Loss: 0.429499\n  Epoch 186, Batch 160/324, Loss: 0.062614\n  Epoch 186, Batch 180/324, Loss: 0.091365\n  Epoch 186, Batch 200/324, Loss: 0.157153\n  Epoch 186, Batch 220/324, Loss: 0.299692\n  Epoch 186, Batch 240/324, Loss: 0.068912\n  Epoch 186, Batch 260/324, Loss: 0.144481\n  Epoch 186, Batch 280/324, Loss: 0.071450\n  Epoch 186, Batch 300/324, Loss: 0.053997\n  Epoch 186, Batch 320/324, Loss: 0.126331\n  Epoch 186, Batch 324/324, Loss: 0.456828\n--- Finished Epoch 186/200, Avg Training Loss: 0.235851 ---\n    Saving checkpoint for epoch 185 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 185.\n\n--- Starting Epoch 187/200 (current loop index: 186) ---\n  Epoch 187, Batch 20/324, Loss: 0.434981\n  Epoch 187, Batch 40/324, Loss: 0.029522\n  Epoch 187, Batch 60/324, Loss: 0.127109\n  Epoch 187, Batch 80/324, Loss: 0.153691\n  Epoch 187, Batch 100/324, Loss: 0.274313\n  Epoch 187, Batch 120/324, Loss: 0.236174\n  Epoch 187, Batch 140/324, Loss: 0.318438\n  Epoch 187, Batch 160/324, Loss: 0.263042\n  Epoch 187, Batch 180/324, Loss: 0.370412\n  Epoch 187, Batch 200/324, Loss: 0.161438\n  Epoch 187, Batch 220/324, Loss: 0.295177\n  Epoch 187, Batch 240/324, Loss: 0.244858\n  Epoch 187, Batch 260/324, Loss: 0.111141\n  Epoch 187, Batch 280/324, Loss: 0.079477\n  Epoch 187, Batch 300/324, Loss: 0.482776\n  Epoch 187, Batch 320/324, Loss: 0.508622\n  Epoch 187, Batch 324/324, Loss: 0.000000\n--- Finished Epoch 187/200, Avg Training Loss: 0.232475 ---\n    Saving checkpoint for epoch 186 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 186.\n\n--- Starting Epoch 188/200 (current loop index: 187) ---\n  Epoch 188, Batch 20/324, Loss: 0.246401\n  Epoch 188, Batch 40/324, Loss: 0.191721\n  Epoch 188, Batch 60/324, Loss: 0.318416\n  Epoch 188, Batch 80/324, Loss: 0.188444\n  Epoch 188, Batch 100/324, Loss: 0.246011\n  Epoch 188, Batch 120/324, Loss: 0.241142\n  Epoch 188, Batch 140/324, Loss: 0.231512\n  Epoch 188, Batch 160/324, Loss: 0.194196\n  Epoch 188, Batch 180/324, Loss: 0.099909\n  Epoch 188, Batch 200/324, Loss: 0.316272\n  Epoch 188, Batch 220/324, Loss: 0.444323\n  Epoch 188, Batch 240/324, Loss: 0.206026\n  Epoch 188, Batch 260/324, Loss: 0.213240\n  Epoch 188, Batch 280/324, Loss: 0.233087\n  Epoch 188, Batch 300/324, Loss: 0.316423\n  Epoch 188, Batch 320/324, Loss: 0.287425\n  Epoch 188, Batch 324/324, Loss: 0.207467\n--- Finished Epoch 188/200, Avg Training Loss: 0.232453 ---\n    Saving checkpoint for epoch 187 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 187.\n\n--- Starting Epoch 189/200 (current loop index: 188) ---\n  Epoch 189, Batch 20/324, Loss: 0.129128\n  Epoch 189, Batch 40/324, Loss: 0.132795\n  Epoch 189, Batch 60/324, Loss: 0.309149\n  Epoch 189, Batch 80/324, Loss: 0.272488\n  Epoch 189, Batch 100/324, Loss: 0.096489\n  Epoch 189, Batch 120/324, Loss: 0.116285\n  Epoch 189, Batch 140/324, Loss: 0.328166\n  Epoch 189, Batch 160/324, Loss: 0.130711\n  Epoch 189, Batch 180/324, Loss: 0.259353\n  Epoch 189, Batch 200/324, Loss: 0.255393\n  Epoch 189, Batch 220/324, Loss: 0.126964\n  Epoch 189, Batch 240/324, Loss: 0.271632\n  Epoch 189, Batch 260/324, Loss: 0.233309\n  Epoch 189, Batch 280/324, Loss: 0.214709\n  Epoch 189, Batch 300/324, Loss: 0.369366\n  Epoch 189, Batch 320/324, Loss: 0.391539\n  Epoch 189, Batch 324/324, Loss: 0.198732\n--- Finished Epoch 189/200, Avg Training Loss: 0.227020 ---\n    Saving checkpoint for epoch 188 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 188.\n\n--- Starting Epoch 190/200 (current loop index: 189) ---\n  Epoch 190, Batch 20/324, Loss: 0.270806\n  Epoch 190, Batch 40/324, Loss: 0.211471\n  Epoch 190, Batch 60/324, Loss: 0.104091\n  Epoch 190, Batch 80/324, Loss: 0.114742\n  Epoch 190, Batch 100/324, Loss: 0.308399\n  Epoch 190, Batch 120/324, Loss: 0.082204\n  Epoch 190, Batch 140/324, Loss: 0.212205\n  Epoch 190, Batch 160/324, Loss: 0.207351\n  Epoch 190, Batch 180/324, Loss: 0.060848\n  Epoch 190, Batch 200/324, Loss: 0.343379\n  Epoch 190, Batch 220/324, Loss: 0.230036\n  Epoch 190, Batch 240/324, Loss: 0.350948\n  Epoch 190, Batch 260/324, Loss: 0.138938\n  Epoch 190, Batch 280/324, Loss: 0.363016\n  Epoch 190, Batch 300/324, Loss: 0.143757\n  Epoch 190, Batch 320/324, Loss: 0.238714\n  Epoch 190, Batch 324/324, Loss: 0.106611\n--- Finished Epoch 190/200, Avg Training Loss: 0.231095 ---\n    Saving checkpoint for epoch 189 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 189.\n\n--- Starting Epoch 191/200 (current loop index: 190) ---\n  Epoch 191, Batch 20/324, Loss: 0.115139\n  Epoch 191, Batch 40/324, Loss: 0.498051\n  Epoch 191, Batch 60/324, Loss: 0.225818\n  Epoch 191, Batch 80/324, Loss: 0.145235\n  Epoch 191, Batch 100/324, Loss: 0.384961\n  Epoch 191, Batch 120/324, Loss: 0.142081\n  Epoch 191, Batch 140/324, Loss: 0.349022\n  Epoch 191, Batch 160/324, Loss: 0.206902\n  Epoch 191, Batch 180/324, Loss: 0.187549\n  Epoch 191, Batch 200/324, Loss: 0.303514\n  Epoch 191, Batch 220/324, Loss: 0.181753\n  Epoch 191, Batch 240/324, Loss: 0.146904\n  Epoch 191, Batch 260/324, Loss: 0.177914\n  Epoch 191, Batch 280/324, Loss: 0.239902\n  Epoch 191, Batch 300/324, Loss: 0.259041\n  Epoch 191, Batch 320/324, Loss: 0.000000\n  Epoch 191, Batch 324/324, Loss: 0.136337\n--- Finished Epoch 191/200, Avg Training Loss: 0.226829 ---\n    Saving checkpoint for epoch 190 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 190.\n\n--- Starting Epoch 192/200 (current loop index: 191) ---\n  Epoch 192, Batch 20/324, Loss: 0.189209\n  Epoch 192, Batch 40/324, Loss: 0.224536\n  Epoch 192, Batch 60/324, Loss: 0.160567\n  Epoch 192, Batch 80/324, Loss: 0.227048\n  Epoch 192, Batch 100/324, Loss: 0.178400\n  Epoch 192, Batch 120/324, Loss: 0.114792\n  Epoch 192, Batch 140/324, Loss: 0.254393\n  Epoch 192, Batch 160/324, Loss: 0.061198\n  Epoch 192, Batch 180/324, Loss: 0.158051\n  Epoch 192, Batch 200/324, Loss: 0.132709\n  Epoch 192, Batch 220/324, Loss: 0.188130\n  Epoch 192, Batch 240/324, Loss: 0.166986\n  Epoch 192, Batch 260/324, Loss: 0.257810\n  Epoch 192, Batch 280/324, Loss: 0.320948\n  Epoch 192, Batch 300/324, Loss: 0.068102\n  Epoch 192, Batch 320/324, Loss: 0.271208\n  Epoch 192, Batch 324/324, Loss: 0.201410\n--- Finished Epoch 192/200, Avg Training Loss: 0.221232 ---\n    Saving checkpoint for epoch 191 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 191.\n\n--- Starting Epoch 193/200 (current loop index: 192) ---\n  Epoch 193, Batch 20/324, Loss: 0.189220\n  Epoch 193, Batch 40/324, Loss: 0.199012\n  Epoch 193, Batch 60/324, Loss: 0.229450\n  Epoch 193, Batch 80/324, Loss: 0.375889\n  Epoch 193, Batch 100/324, Loss: 0.155797\n  Epoch 193, Batch 120/324, Loss: 0.201443\n  Epoch 193, Batch 140/324, Loss: 0.295232\n  Epoch 193, Batch 160/324, Loss: 0.144720\n  Epoch 193, Batch 180/324, Loss: 0.155398\n  Epoch 193, Batch 200/324, Loss: 0.342415\n  Epoch 193, Batch 220/324, Loss: 0.125021\n  Epoch 193, Batch 240/324, Loss: 0.295214\n  Epoch 193, Batch 260/324, Loss: 0.126762\n  Epoch 193, Batch 280/324, Loss: 0.065419\n  Epoch 193, Batch 300/324, Loss: 0.183458\n  Epoch 193, Batch 320/324, Loss: 0.330305\n  Epoch 193, Batch 324/324, Loss: 0.228930\n--- Finished Epoch 193/200, Avg Training Loss: 0.221326 ---\n    Saving checkpoint for epoch 192 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 192.\n\n--- Starting Epoch 194/200 (current loop index: 193) ---\n  Epoch 194, Batch 20/324, Loss: 0.421720\n  Epoch 194, Batch 40/324, Loss: 0.197357\n  Epoch 194, Batch 60/324, Loss: 0.395812\n  Epoch 194, Batch 80/324, Loss: 0.267789\n  Epoch 194, Batch 100/324, Loss: 0.271863\n  Epoch 194, Batch 120/324, Loss: 0.208254\n  Epoch 194, Batch 140/324, Loss: 0.058003\n  Epoch 194, Batch 160/324, Loss: 0.291062\n  Epoch 194, Batch 180/324, Loss: 0.274622\n  Epoch 194, Batch 200/324, Loss: 0.168173\n  Epoch 194, Batch 220/324, Loss: 0.136939\n  Epoch 194, Batch 240/324, Loss: 0.249755\n  Epoch 194, Batch 260/324, Loss: 0.251804\n  Epoch 194, Batch 280/324, Loss: 0.145809\n  Epoch 194, Batch 300/324, Loss: 0.190868\n  Epoch 194, Batch 320/324, Loss: 0.159994\n  Epoch 194, Batch 324/324, Loss: 0.240622\n--- Finished Epoch 194/200, Avg Training Loss: 0.217321 ---\n    Saving checkpoint for epoch 193 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 193.\n\n--- Starting Epoch 195/200 (current loop index: 194) ---\n  Epoch 195, Batch 20/324, Loss: 0.256854\n  Epoch 195, Batch 40/324, Loss: 0.242137\n  Epoch 195, Batch 60/324, Loss: 0.316882\n  Epoch 195, Batch 80/324, Loss: 0.310392\n  Epoch 195, Batch 100/324, Loss: 0.112209\n  Epoch 195, Batch 120/324, Loss: 0.066926\n  Epoch 195, Batch 140/324, Loss: 0.262520\n  Epoch 195, Batch 160/324, Loss: 0.118905\n  Epoch 195, Batch 180/324, Loss: 0.283023\n  Epoch 195, Batch 200/324, Loss: 0.194881\n  Epoch 195, Batch 220/324, Loss: 0.107591\n  Epoch 195, Batch 240/324, Loss: 0.136994\n  Epoch 195, Batch 260/324, Loss: 0.252544\n  Epoch 195, Batch 280/324, Loss: 0.129431\n  Epoch 195, Batch 300/324, Loss: 0.123538\n  Epoch 195, Batch 320/324, Loss: 0.282801\n  Epoch 195, Batch 324/324, Loss: 0.400817\n--- Finished Epoch 195/200, Avg Training Loss: 0.213680 ---\n    Saving checkpoint for epoch 194 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 194.\n\n--- Starting Epoch 196/200 (current loop index: 195) ---\n  Epoch 196, Batch 20/324, Loss: 0.257609\n  Epoch 196, Batch 40/324, Loss: 0.107706\n  Epoch 196, Batch 60/324, Loss: 0.259078\n  Epoch 196, Batch 80/324, Loss: 0.241037\n  Epoch 196, Batch 100/324, Loss: 0.228041\n  Epoch 196, Batch 120/324, Loss: 0.238876\n  Epoch 196, Batch 140/324, Loss: 0.186184\n  Epoch 196, Batch 160/324, Loss: 0.154410\n  Epoch 196, Batch 180/324, Loss: 0.336538\n  Epoch 196, Batch 200/324, Loss: 0.165712\n  Epoch 196, Batch 220/324, Loss: 0.229458\n  Epoch 196, Batch 240/324, Loss: 0.142718\n  Epoch 196, Batch 260/324, Loss: 0.203133\n  Epoch 196, Batch 280/324, Loss: 0.176358\n  Epoch 196, Batch 300/324, Loss: 0.261516\n  Epoch 196, Batch 320/324, Loss: 0.215751\n  Epoch 196, Batch 324/324, Loss: 0.059776\n--- Finished Epoch 196/200, Avg Training Loss: 0.207543 ---\n    Saving checkpoint for epoch 195 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 195.\n\n--- Starting Epoch 197/200 (current loop index: 196) ---\n  Epoch 197, Batch 20/324, Loss: 0.292191\n  Epoch 197, Batch 40/324, Loss: 0.040714\n  Epoch 197, Batch 60/324, Loss: 0.219188\n  Epoch 197, Batch 80/324, Loss: 0.113344\n  Epoch 197, Batch 100/324, Loss: 0.324899\n  Epoch 197, Batch 120/324, Loss: 0.120992\n  Epoch 197, Batch 140/324, Loss: 0.187103\n  Epoch 197, Batch 160/324, Loss: 0.239736\n  Epoch 197, Batch 180/324, Loss: 0.393631\n  Epoch 197, Batch 200/324, Loss: 0.308602\n  Epoch 197, Batch 220/324, Loss: 0.198968\n  Epoch 197, Batch 240/324, Loss: 0.172434\n  Epoch 197, Batch 260/324, Loss: 0.440589\n  Epoch 197, Batch 280/324, Loss: 0.153448\n  Epoch 197, Batch 300/324, Loss: 0.211118\n  Epoch 197, Batch 320/324, Loss: 0.094990\n  Epoch 197, Batch 324/324, Loss: 0.000000\n--- Finished Epoch 197/200, Avg Training Loss: 0.212937 ---\n    Saving checkpoint for epoch 196 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 196.\n\n--- Starting Epoch 198/200 (current loop index: 197) ---\n  Epoch 198, Batch 20/324, Loss: 0.198196\n  Epoch 198, Batch 40/324, Loss: 0.234545\n  Epoch 198, Batch 60/324, Loss: 0.172323\n  Epoch 198, Batch 80/324, Loss: 0.168817\n  Epoch 198, Batch 100/324, Loss: 0.211698\n  Epoch 198, Batch 120/324, Loss: 0.148593\n  Epoch 198, Batch 140/324, Loss: 0.065739\n  Epoch 198, Batch 160/324, Loss: 0.200151\n  Epoch 198, Batch 180/324, Loss: 0.125735\n  Epoch 198, Batch 200/324, Loss: 0.337585\n  Epoch 198, Batch 220/324, Loss: 0.163692\n  Epoch 198, Batch 240/324, Loss: 0.328685\n  Epoch 198, Batch 260/324, Loss: 0.145578\n  Epoch 198, Batch 280/324, Loss: 0.271882\n  Epoch 198, Batch 300/324, Loss: 0.092043\n  Epoch 198, Batch 320/324, Loss: 0.210772\n  Epoch 198, Batch 324/324, Loss: 0.496294\n--- Finished Epoch 198/200, Avg Training Loss: 0.207966 ---\n    Saving checkpoint for epoch 197 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 197.\n\n--- Starting Epoch 199/200 (current loop index: 198) ---\n  Epoch 199, Batch 20/324, Loss: 0.276577\n  Epoch 199, Batch 40/324, Loss: 0.228394\n  Epoch 199, Batch 60/324, Loss: 0.164460\n  Epoch 199, Batch 80/324, Loss: 0.232099\n  Epoch 199, Batch 100/324, Loss: 0.264800\n  Epoch 199, Batch 120/324, Loss: 0.186946\n  Epoch 199, Batch 140/324, Loss: 0.167269\n  Epoch 199, Batch 160/324, Loss: 0.138765\n  Epoch 199, Batch 180/324, Loss: 0.308708\n  Epoch 199, Batch 200/324, Loss: 0.127209\n  Epoch 199, Batch 220/324, Loss: 0.073789\n  Epoch 199, Batch 240/324, Loss: 0.178509\n  Epoch 199, Batch 260/324, Loss: 0.243320\n  Epoch 199, Batch 280/324, Loss: 0.171418\n  Epoch 199, Batch 300/324, Loss: 0.128739\n  Epoch 199, Batch 320/324, Loss: 0.128416\n  Epoch 199, Batch 324/324, Loss: 0.223738\n--- Finished Epoch 199/200, Avg Training Loss: 0.202410 ---\n    Saving checkpoint for epoch 198 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 198.\n\n--- Starting Epoch 200/200 (current loop index: 199) ---\n  Epoch 200, Batch 20/324, Loss: 0.184177\n  Epoch 200, Batch 40/324, Loss: 0.385878\n  Epoch 200, Batch 60/324, Loss: 0.185219\n  Epoch 200, Batch 80/324, Loss: 0.235500\n  Epoch 200, Batch 100/324, Loss: 0.087536\n  Epoch 200, Batch 120/324, Loss: 0.154150\n  Epoch 200, Batch 140/324, Loss: 0.368226\n  Epoch 200, Batch 160/324, Loss: 0.236462\n  Epoch 200, Batch 180/324, Loss: 0.128035\n  Epoch 200, Batch 200/324, Loss: 0.049946\n  Epoch 200, Batch 220/324, Loss: 0.203848\n  Epoch 200, Batch 240/324, Loss: 0.123012\n  Epoch 200, Batch 260/324, Loss: 0.229768\n  Epoch 200, Batch 280/324, Loss: 0.169301\n  Epoch 200, Batch 300/324, Loss: 0.462309\n  Epoch 200, Batch 320/324, Loss: 0.244670\n  Epoch 200, Batch 324/324, Loss: 0.072179\n--- Finished Epoch 200/200, Avg Training Loss: 0.196666 ---\n    Saving checkpoint for epoch 199 to /kaggle/working/vsr_checkpoint_16_05_09pm.pth...\n    Checkpoint saved successfully for epoch 199.\n\n--- Training Loop Finished ---\n--- Final trained model state_dict saved to /kaggle/working/model_fully_trained_16_05_09pm.pth ---\n--- End of Training Cell ---\n\n","output_type":"stream"}],"execution_count":12},{"id":"0f0b1365-085f-41c8-a35a-98c38ce55802","cell_type":"code","source":"# Cell 7: Evaluation / Decoding Test (Optional)\nprint(\"--- Starting Evaluation / Decoding Test ---\")\nimport torch # Ensure torch is in scope\nimport traceback\n\nif train_loader and model and char_map_instance:\n    print(\"  [Eval] Setting model to evaluation mode.\")\n    model.eval() # Set model to evaluation mode\n\n    print(\"  [Eval] Getting one batch from train_loader for testing...\")\n    try:\n        eval_data_batch = next(iter(train_loader)) # Get a batch\n\n        if eval_data_batch is None:\n            print(\"  [Eval] ERROR: The first batch from DataLoader was None.\")\n        else:\n            padded_sequences_batch_first, padded_targets_eval, _, target_lengths_eval = eval_data_batch\n            print(f\"  [Eval] Batch obtained. Sequence shape: {padded_sequences_batch_first.shape}\")\n\n            padded_sequences_batch_first = padded_sequences_batch_first.to(device)\n\n            print(\"  [Eval] Performing inference...\")\n            with torch.no_grad(): # Disable gradient calculations\n                output_log_probs_eval = model(padded_sequences_batch_first) # (B, T, NumClasses)\n            print(f\"  [Eval] Inference complete. Output log_probs shape: {output_log_probs_eval.shape}\")\n\n            # Greedy Decoding on the first sample\n            print(\"\\n  [Eval] --- Decoding First Sample in Batch ---\")\n            predicted_indices_batch = torch.argmax(output_log_probs_eval.cpu(), dim=2) # (B, T)\n            pred_indices_sample = predicted_indices_batch[0].tolist()\n            predicted_text_sample = char_map_instance.indices_to_text(pred_indices_sample)\n            print(f\"  [Eval] Predicted Text (Greedy): '{predicted_text_sample}'\")\n\n            first_target_len = target_lengths_eval[0].item()\n            if first_target_len > 0:\n                true_indices_sample = padded_targets_eval[0][:first_target_len].tolist()\n                true_text_sample = char_map_instance.indices_to_text(true_indices_sample, remove_blanks=False, remove_duplicates=False) # Show raw truth\n                print(f\"  [Eval] Ground Truth Text      : '{true_text_sample}'\")\n            else:\n                print(\"  [Eval] Ground Truth Text      : (Skipped - target length is 0)\")\n            print(\"  [Eval] --- End of First Sample Decode ---\")\n\n    except StopIteration:\n        print(\"  [Eval] ERREUR: DataLoader is empty.\")\n    except Exception as e:\n        print(f\"  [Eval] ERREUR during decoding test: {e}\"); traceback.print_exc()\nelse:\n    print(\"--- Evaluation Skipped: Model, DataLoader, or CharMap not available. ---\")\nprint(\"\\n--- End of Evaluation Cell ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T22:23:59.629721Z","iopub.execute_input":"2025-05-16T22:23:59.629979Z","iopub.status.idle":"2025-05-16T22:24:00.237094Z","shell.execute_reply.started":"2025-05-16T22:23:59.629959Z","shell.execute_reply":"2025-05-16T22:24:00.235586Z"}},"outputs":[{"name":"stdout","text":"--- Starting Evaluation / Decoding Test ---\n  [Eval] Setting model to evaluation mode.\n  [Eval] Getting one batch from train_loader for testing...\n  [Eval] Batch obtained. Sequence shape: torch.Size([4, 93, 1, 96, 96])\n  [Eval] Performing inference...\n  [Eval] Inference complete. Output log_probs shape: torch.Size([4, 93, 37])\n\n  [Eval] --- Decoding First Sample in Batch ---\n  [Eval] Predicted Text (Greedy): 'نبداو بخلاصه شو وقع اسرائيل ضربتله واحد القن صلديه ديال ايرن في سوريا وريا'\n  [Eval] Ground Truth Text      : 'نبداو بخلاصه شنو وقع اسرائيل ضربت لهم واحد القنصليه ديال ايران في سوريا سوريا'\n  [Eval] --- End of First Sample Decode ---\n\n--- End of Evaluation Cell ---\n","output_type":"stream"}],"execution_count":16},{"id":"b772a056-68bc-4fd4-854c-36a95ee87202","cell_type":"code","source":"# Cell 8: Load Saved Model and Make Predictions (Optional)\nprint(\"--- Loading Saved Model and Making Predictions ---\")\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport traceback\n\n# --- Configuration for Prediction ---\n# Choose which model weights to load for prediction\n# MODEL_TO_LOAD_PATH = FINAL_MODEL_SAVE_PATH # To load the model just trained\nMODEL_TO_LOAD_PATH = \"/kaggle/working/model_fully_trained_16_05_09pm.pth\" # Or specify a checkpoint path\n# MODEL_TO_LOAD_PATH = \"/kaggle/working/model_fully_trained.pth\" # Or your original final model\nprint(f\"Attempting to load model for prediction from: {MODEL_TO_LOAD_PATH}\")\n\n# Ensure necessary variables from Cell 4 (Configuration) are available:\n# NUM_CLASSES, GRU_HIDDEN_DIM, GRU_NUM_LAYERS, BOTTLENECK_DIM, device, char_map_instance\n# Ensure train_dataset is available if predicting on a sample from it (from Cell 5)\n\nprediction_model = None\ntry:\n    # 1. Instantiate the Model\n    if NUM_CLASSES == -1: raise ValueError(\"NUM_CLASSES not set, run configuration cell.\")\n    prediction_model = NewVSRModel(\n        num_classes=NUM_CLASSES, input_channel=1,\n        rnn_hidden_size=GRU_HIDDEN_DIM, rnn_num_layers=GRU_NUM_LAYERS,\n        bottleneck_dim=BOTTLENECK_DIM\n    ).to(device)\n    print(\"  [Prediction] Model architecture instantiated.\")\n\n    # 2. Load Saved Weights\n    if os.path.exists(MODEL_TO_LOAD_PATH):\n        print(f\"  [Prediction] Weights file found at {MODEL_TO_LOAD_PATH}.\")\n        \n        # If loading a checkpoint, it's a dictionary. If loading a final model, it might be just state_dict.\n        loaded_content = torch.load(MODEL_TO_LOAD_PATH, map_location=device)\n        \n        if isinstance(loaded_content, dict) and 'model_state_dict' in loaded_content:\n            prediction_model.load_state_dict(loaded_content['model_state_dict'])\n            print(\"  [Prediction] Loaded 'model_state_dict' from checkpoint dictionary.\")\n            if 'epoch' in loaded_content: print(f\"    Model from epoch: {loaded_content['epoch']}\")\n        elif isinstance(loaded_content, dict) and not 'model_state_dict' in loaded_content:\n             prediction_model.load_state_dict(loaded_content) # Assuming the dict itself is the state_dict\n             print(\"  [Prediction] Loaded weights (assumed dictionary is state_dict).\")\n        else: # Direct state_dict\n            prediction_model.load_state_dict(loaded_content)\n            print(\"  [Prediction] Loaded weights directly (assumed raw state_dict).\")\n        \n        prediction_model.eval() # Set to evaluation mode\n        print(\"  [Prediction] Model weights loaded and model set to eval mode.\")\n\n        # 3. Prepare Input Data (Example: from train_dataset)\n        if 'train_dataset' in globals() and train_dataset is not None and len(train_dataset) > 0:\n            sample_idx_to_predict = 401 # Change as needed\n            print(f\"  [Prediction] Fetching sample {sample_idx_to_predict} from train_dataset...\")\n            \n            # Ensure dataset is available from Cell 5 execution\n            sample_data = train_dataset[sample_idx_to_predict]\n            if sample_data:\n                sample_frames_tensor, sample_text_tensor_truth = sample_data\n                print(f\"    [Prediction] Sample frames tensor shape: {sample_frames_tensor.shape}\")\n\n                # Display a frame\n                if sample_frames_tensor.ndim == 4 and sample_frames_tensor.shape[0] > 0 and sample_frames_tensor.shape[1] == 1: # T, C, H, W\n                    frame_to_display = sample_frames_tensor[0, 0, :, :].cpu().numpy()\n                    plt.figure(figsize=(3,3)); plt.imshow(frame_to_display, cmap='gray')\n                    plt.title(f\"Sample Frame (Index {sample_idx_to_predict})\"); plt.axis('off'); plt.show()\n\n                input_tensor = sample_frames_tensor.unsqueeze(0).to(device) # Add batch dim: B, T, C, H, W\n\n                # 4. Make Prediction\n                print(\"    [Prediction] Performing inference...\")\n                with torch.no_grad():\n                    output_log_probs = prediction_model(input_tensor)\n                \n                predicted_indices = torch.argmax(output_log_probs.cpu(), dim=2)[0].tolist()\n                predicted_text = char_map_instance.indices_to_text(predicted_indices)\n                print(f\"    [Prediction] Predicted Text : '{predicted_text}'\")\n\n                if sample_text_tensor_truth is not None:\n                    true_indices = sample_text_tensor_truth.cpu().tolist()\n                    true_text = char_map_instance.indices_to_text(true_indices, remove_blanks=False, remove_duplicates=False)\n                    print(f\"    [Prediction] Ground Truth Text: '{true_text}'\")\n            else:\n                print(f\"  [Prediction] ERREUR: Sample {sample_idx_to_predict} from train_dataset is None.\")\n        else:\n            print(\"  [Prediction] train_dataset not available/empty. Cannot fetch sample for prediction.\")\n            print(\"             To predict on new data, load and preprocess it like VSRDataset.__getitem__.\")\n    else:\n        print(f\"  [Prediction] ERREUR: Weights file NOT FOUND at {MODEL_TO_LOAD_PATH}.\")\n\nexcept Exception as e_predict:\n    print(f\"  [Prediction] ERREUR during model loading or prediction: {e_predict}\")\n    traceback.print_exc()\n\nprint(\"\\n--- Prediction Cell Done ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T22:27:04.410476Z","iopub.execute_input":"2025-05-16T22:27:04.410759Z","iopub.status.idle":"2025-05-16T22:27:04.786657Z","shell.execute_reply.started":"2025-05-16T22:27:04.410737Z","shell.execute_reply":"2025-05-16T22:27:04.785824Z"}},"outputs":[{"name":"stdout","text":"--- Loading Saved Model and Making Predictions ---\nAttempting to load model for prediction from: /kaggle/working/model_fully_trained_16_05_09pm.pth\n  [Prediction] Model architecture instantiated.\n  [Prediction] Weights file found at /kaggle/working/model_fully_trained_16_05_09pm.pth.\n  [Prediction] Loaded weights (assumed dictionary is state_dict).\n  [Prediction] Model weights loaded and model set to eval mode.\n  [Prediction] Fetching sample 401 from train_dataset...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3554726621.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  loaded_content = torch.load(MODEL_TO_LOAD_PATH, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"    [Prediction] Sample frames tensor shape: torch.Size([77, 1, 96, 96])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 300x300 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAPoAAAERCAYAAABSGLrIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABzw0lEQVR4nO19edBtRXX9vt/8ngyKzI9JQUFQyooiYhJRUIgTIaLGShlRErQqimgcY2FBOfyiYgRKSpRYYomxTBRQTHAWknIsZyXGocAhDlHAiIHH+8bz+4Os+9Zd39p9+tzhPQjfrrp17j2nT/fu7j2svbvPub2maZrYoA3aoP/TNLWzGdigDdqgydOGom/QBt0DaEPRN2iD7gG0oegbtEH3ANpQ9A3aoHsAbSj6Bm3QPYA2FH2DNugeQBuKvkEbdA+gDUXfoA26B9CGokdEr9eL8847b2ezcbej2267Lfbee+/4h3/4h4m2c91110Wv14vrrrtuou3clWh5eTkOPPDAePvb3z6W+sam6N/5znfiaU97Whx88MGxsLAQW7Zsicc//vHxtre9bVxN3G3okEMOiV6vZz/btm3b2eyNjS666KLYdddd45nPfGb/3HnnnRe9Xi9uvvnmncjZZOjMM8+MXq8XT37yk+31q6++On7v934vFhYW4qCDDopzzz03VlZWBsr88pe/jFe96lXx2Mc+NnbdddfUgM3OzsZf//Vfxxve8IaxyMzMyDVExBe+8IV47GMfGwcddFCceeaZse+++8Z//ud/xpe+9KW46KKL4qyzzhpHM3creuhDHxovfelL152fm5vbCdyMn5aXl+Oiiy6Kl7zkJTE9Pb2z2Zk4ffWrX433vOc9sbCwYK9/7GMfi1NPPTUe85jHxNve9rb4zne+E69//evj17/+dVxyySX9ct///vfjTW96UzzgAQ+IhzzkIfHFL34xbfO5z31uvOpVr4r3v//9ccYZZ4zE/1gU/Q1veEPsvvvu8ZWvfCXufe97D1z79a9/PY4m7na0ZcuWeNaznlVdfuvWrbF58+YJcjRe+ud//ue46aab4hnPeMbOZmXi1DRNvOhFL4pnP/vZ8ZnPfMaWednLXhZHH310fPKTn4yZmTvVarfddov/9//+X5x99tlxxBFHRETEwx72sLjllltijz32iA996EPx9Kc/PW333ve+d5x00knxnve8Z2RFHwt0v+GGG+Koo45ap+QREXvvvffA78suuyxOOOGE2HvvvWN+fj6OPPLIAYsHOuSQQ+LJT35yXHfddfHwhz88Nm3aFA95yEP6MOfKK6+MhzzkIbGwsBAPe9jD4hvf+MbA/c95znNil112iRtvvDFOPvnkuNe97hX7779/vPa1r42aB/Z+/vOfxxlnnBH77LNPzM/Px1FHHRXvfve76welQI95zGPiwQ9+cHzta1+LRz/60bF58+Z49atfHRERH/nIR+JJT3pS7L///jE/Px+HHnpovO51r4vV1VVbx7e//e04/vjjY/PmzXHYYYfFhz70oYiI+Nd//dc49thjY9OmTXH44YfHpz/96bH28cMf/nAccsghceihh1b397vf/W489rGPjc2bN8eWLVvizW9+87qyP/vZz+LUU0+Ne93rXrH33nvHS17yklhcXLT1fvnLX44/+qM/it133z02b94cxx9/fHz+85/vX/+P//iP2LRpUzz72c8euO9zn/tcTE9Pxytf+cqqvl5++eVx/fXXxxve8AZ7/bvf/W5897vfjec973l9JY+I+Ku/+qtomqY/JxERu+66a+yxxx5V7UZEPP7xj4/Pfe5z8Zvf/Kb6HkvNGOikk05qdt111+Y73/lOa9ljjjmmec5zntNccMEFzdve9rbmpJNOaiKiufjiiwfKHXzwwc3hhx/e7Lfffs15553XXHDBBc2WLVuaXXbZpXnf+97XHHTQQc0b3/jG5o1vfGOz++67N4cddlizurrav//0009vFhYWmgc84AHNn//5nzcXX3xx8+QnP7mJiOY1r3nNQFsR0Zx77rn93//1X//VHHDAAc2BBx7YvPa1r20uueSS5pRTTmkiorngggta+3jwwQc3J510UnPTTTcNfG6//famaZrm+OOPb/bdd99mr732as4666zmne98Z/PhD3+4aZqmOfXUU5tnPOMZzfnnn99ccsklzdOf/vQmIpqXvexlA20cf/zxzf77798ceOCBzctf/vLmbW97W3PkkUc209PTzQc+8IFm3333bc4777zmwgsvbLZs2dLsvvvuze9+97ux9fGwww5rnvrUp647f+655zYR0dx0002W17PPPrt5+9vf3pxwwglNRDTXXHNNv9zWrVubBz7wgc3CwkLzile8ornwwgubhz3sYc3RRx/dRERz7bXX9st+5jOfaebm5prjjjuu+bu/+7vmggsuaI4++uhmbm6u+fKXv9wvd/755zcR0XzkIx9pmqZpbrvttubQQw9tjjzyyGbbtm2t/fzd737X7Lvvvs3f/u3fNk1z59w+6UlPGijzvve9r4mIgXZBBxxwgB2npmmaD37wg+v6pfS5z32uiYjmox/9aCuvJRqLon/yk59spqenm+np6ea4445rXvGKVzSf+MQnmqWlpXVlt27duu7cySef3Nz//vcfOHfwwQc3EdF84Qtf6J/7xCc+0UREs2nTpuYnP/lJ//w73/nOdQN2+umnNxHRnHXWWf1za2trzZOe9KRmbm5uQBBV0f/iL/6i2W+//Zqbb755gKdnPvOZze6772774HjXD9o4/vjjm4ho3vGOd1SNz/Of//xm8+bNA4KJOt7//vf3z33ve99rIqKZmppqvvSlL/XPY9wuu+yysfRxeXm56fV6zUtf+tJ11zJFj4jmve99b//c4uJis++++zannXZa/9yFF17YRETzT//0T/1zt99+e3PYYYcNzO/a2lrzgAc8oDn55JObtbW1ftmtW7c297vf/ZrHP/7x/XOrq6vNH/zBHzT77LNPc/PNNzcveMELmpmZmeYrX/lK2j+ml73sZc397ne//tg7RYcx+elPf7ru/mOOOaZ55CMfaeuuUfRf/OIXTUQ0b3rTm6r4zWgs0P3xj398fPGLX4xTTjklvvWtb8Wb3/zmOPnkk2PLli1x9dVXD5TdtGlT//utt94aN998cxx//PFx4403xq233jpQ9sgjj4zjjjuu//vYY4+NiIgTTjghDjrooHXnb7zxxnW8vfCFL+x/7/V68cIXvjCWlpYslI24Mx674oor4ilPeUo0TRM333xz/3PyySfHrbfeGl//+tdbx+TYY4+NT33qUwMfhpDz8/Px3Oc+d919PD7/8z//EzfffHP84R/+YWzdujW+973vDZTdZZddBjLehx9+eNz73veOBz3oQf0xAS8R28dn1D7+5je/iaZp4j73uU/rODCvnLOYm5uLRzziEQNzds0118R+++0XT3va0/rnNm/eHM973vMG6vrmN78ZP/zhD+PP/uzP4pZbbunzfvvtt8eJJ54Y//Zv/xZra2sRETE1NRXvec974rbbbosnPOEJ8fa3vz3+5m/+Jh7+8Ie38vyDH/wgLrroojj//PNjfn4+LXfHHXdERNgyCwsL/evDEMZ41FWMsSTjIiKOOeaYuPLKK2NpaSm+9a1vxVVXXRUXXHBBPO1pT4tvfvObceSRR0ZExOc///k499xz44tf/GJs3bp1oI5bb701dt999/5vVuaI6F878MAD7fn//u//Hjg/NTUV97///QfOPfCBD4yIiB//+Me2HzfddFP89re/jUsvvTQuvfRSW6YmwbjnnnvG4x73uPT6li1bbAb+3//93+Occ86Jz372s/G73/1u4JoawgMOOCB6vd7Aud133711fMbVx6bDy4kcr/e5z33i29/+dv/3T37ykzjssMPWlTv88MMHfv/whz+MiIjTTz89be/WW2/tK8mhhx4a5513Xrz85S+PBz/4wfGa17ymiuezzz47HvWoR8Vpp51WLAfj7HIJ27ZtGzDeXQljrGPSlcam6KC5ubk45phj4phjjokHPvCB8dznPjc++MEPxrnnnhs33HBDnHjiiXHEEUfEW9/61jjwwANjbm4urrnmmrjgggv6VhiULdtk57sIXkbg4VnPelYqSEcfffTI7bjJ/+1vfxvHH3987LbbbvHa1742Dj300FhYWIivf/3r8cpXvnJs4zNqH/fYY4/o9XrrDGuJxjln4P/888+Phz70obbMLrvsMvD7k5/8ZERE/OIXv4hbbrkl9t1332Ibn/3sZ+PjH/94XHnllQNOYWVlJe6444748Y9/HHvssUfstttusd9++0XEnWvkamR/+ctfxiMe8Ygu3RsgjPGee+45dB0RE1B0JsCjX/7ylxER8dGPfjQWFxfj6quvHvDW11577UTaX1tbixtvvLHvxSPuhGMRd2b1He21116x6667xurqatEjT4Kuu+66uOWWW+LKK6+MRz/60f3zP/rRj8bazqh9nJmZiUMPPXTsfB188MFx/fXXR9M0Ax7s+9///kA5ZPp32223Kv7f8Y53xKc+9al4wxveEH/7t38bz3/+8+MjH/lI8Z6f/vSnERHx1Kc+dd21n//853G/+90vLrjggnjxi1/cNzZf/epXB5T6F7/4RfzsZz9bF3p0IYzxgx70oKHriBjT8tq1115rLfM111wTEduhF6w6l7311lvjsssuGwcbli6++OL+96Zp4uKLL47Z2dk48cQTbfnp6ek47bTT4oorrojrr79+3fWbbrppYry68VlaWhrbNkhuZ9Q+HnfccfHVr351rHw98YlPjF/84hcDy1Fbt25dF1487GEPi0MPPTTe8pa3xG233bauHub/Rz/6Ubz85S+P0047LV796lfHW97ylrj66qvjve99b5GXE044Ia666qp1n7322ise/vCHx1VXXRVPecpTIiLiqKOOiiOOOCIuvfTSgWXQSy65JHq93kDOoSt97Wtfi16vN5CrGobG4tHPOuus2Lp1a/zJn/xJHHHEEbG0tBRf+MIX4h//8R/jkEMO6SedTjrppJibm4unPOUp8fznPz9uu+22+Pu///vYe++9+15/nLSwsBAf//jH4/TTT49jjz02Pvaxj8W//Mu/xKtf/erYa6+90vve+MY3xrXXXhvHHntsnHnmmXHkkUfGb37zm/j6178en/70p0df00zoUY96VNznPveJ008/PV70ohdFr9eLyy+/fCwhidKoffzjP/7juPzyy+MHP/jBAGIahc4888y4+OKL49nPfnZ87Wtfi/322y8uv/zydRuJpqam4l3velc84QlPiKOOOiqe+9znxpYtW+LnP/95XHvttbHbbrvFRz/60WiaJs4444zYtGlTf6/G85///Ljiiivi7LPPjsc97nGx//77W14OOuigdTmiiIgXv/jFsc8++8Spp546cP7888+PU045JU466aR45jOfGddff31cfPHF8Zd/+ZfrvPHrX//6iLgzHxNx5zr95z73uYiIOOeccwbKfupTn4rf//3fj/ve976Vo5jQSDn7/6WPfexjzRlnnNEcccQRzS677NLMzc01hx12WHPWWWc1v/rVrwbKXn311c3RRx/dLCwsNIccckjzpje9qXn3u9/dRETzox/9qF/OLWM0d0p884IXvGDg3I9+9KMmIprzzz+/f+70009v7nWvezU33HBDc9JJJzWbN29u9tlnn+bcc88dWG9Hnby81jRN86tf/ap5wQte0Bx44IHN7Oxss++++zYnnnhic+mll7aOR8Y76Pjjj2+OOuooe+3zn/9888hHPrLZtGlTs//++/eXKkOWYbI6uozbKH1cXFxs9txzz+Z1r3vdwPlsec3xevrppzcHH3zwwLmf/OQnzSmnnNJs3ry52XPPPZuzzz67+fjHP26Xob7xjW80T33qU5v73ve+zfz8fHPwwQc3z3jGM5rPfOYzTdM0zUUXXdRERHPFFVcM3PfTn/602W233ZonPvGJrf1UKs3tVVdd1Tz0oQ9t5ufnmwMOOKA555xz7BJzmKVXfJh++9vfNnNzc8273vWuznwq9f634f9z9JznPCc+9KEPWWi3QeOh173udXHZZZfFD3/4w3vEfvcdTRdeeGG8+c1vjhtuuGGkzH3ExmOqGzQCveQlL4nbbrstPvCBD+xsVv7P0fLycrz1rW+Nc845Z2Qlj5hw1n2D/m/TLrvsco99aGnSNDs728/8j4M2PPoGbdA9gP7PxugbtEEbtJ02PPoGbdA9gDYUfYM26B5AG4q+QRt0D6DqrDu2kq6trcXq6mqsra3FyspKrK2txfLycqyursby8nIsLS0NnGuaJtbW1qJpmlhZWYmmaWJxcbFfz/LycjRNE8vLy/1z2EbY3Pm8fL8etMv1r6ys9OvV9hyhTnyPqHsyiF/wyPeU7nX14/v09HT0er2YmZmJubm5mJqaipmZmZieno7Z2dmYnZ3tn0O5qamp/vWpqan+EZ+IGDjyh3nAOPOYYjwxJ1xmZWWlf23btm2xuroaS0tL/bleWloaqIvHmMdBj23jXRrTYVNL7n6MD48xvi8sLPTHnOdnamoq5ubmYmZmJmZmZmJ2drZ/jedwenq6X8fc3Fy/nvn5+ej1ejE/Px/T09P9dnu9Xp8fnbeIGHiwCdf+9E//tLXfY/foJSXL6J6SDxz2UcPsPjY8XE7Pa3lnrLJ6xkHjqueuQHdXWe28jg5rrR9WcPWauM7WnpEBX4cX0fvx23kM5W2S1Pzvk1V8jPDC7BQGVhoWm605vAi8BLx2m0d31p+9g3r7tbW1mJqaiqZpYmpqauAlDaurq/32MD/T09N9r85zFBEDc8hjVDMXbux2piKprGJ++VxE9BHn6upqf8ywM5DHGnOA+1AHPpiD2vEahYZSdHfOKT5fY4XWsplyR0R6nyq8g4rjJqfkJW+l11jJIQSqxLjG36H8UFo2CArN2YiwovN1FjY1VijL1/hetMnKPjMz0zcA09PTfUXgunkMeXzu6h6SlRxjgvO41uv1+uMBhcc4sKKrHrAhYcoQ2ShUrehuwpRxVT6nnOwV2Du4siC2pFyWDYS+lKGN/66UKXmm7CUvjuvsxTPvzh4d5/mai8H5nPP4UEQWYNzHngYKDMOjcT36pJ6q1+ut++OC0phm4+bKT4LakCgrJPoesT3XERH9MeMxAhJaWVnpIyQYSdSrnr3GgQxDnRXdQQ1Vdv4gUaZJPHycwjtvjQ8SgJyY4+RdifesTI0AlZScPaIqHI5QNOepkYybn5/vH5HIQdIGv5HUcdAdPGibDrqj3xh3JOM0UYcxx3UI7PT0dCwtLcXy8vIAAkCCFORgsAp12xzsCK+fORn0hZOoKysr/d9qKFEGNDMz009u4jrOR0Tf62sb+M0IYhQaWtEj1ntsXHeKqgag5PVL0J2FNBsAhYQullZB47IZdb2mMJo/6sX1A6XW3+z18d0l2bIwISIGPAjPD645I4bfvAoAwzAzMxMrKysxPT3dh6+s4EAK4/JSJY9XqxBdwgaHStFXPhcRA54aTg1j5mTZQfdJ0FCKzsrKyoslGIY4OAdLj87zeXgN1K/Lay5Rp2Ui/DKW4x/Xu0DHtmQbW2NNggFqt3l0tzTD13h5p9fr9a85ftibq6LzmGBMobjqySPu9Gqzs7OxsrLSh57oKzw6X8M53Bsx6Pmc5ywhrRIi64rinBPQOeT7MB58HXxDeeGBNcTEGM3MzPQNIOpYXl7uowPlpa0Pw9DI0F2V3Xlvjcsz715ScrZ+mtxQqvEcmVeoPafXnAd1yTX15lBojdHVo3OMruuuTAzTuW2+5vhnz8sZeTaMfH1ubq5/bXl5ecBru3lhuKuK0zaPiiqY2hTfIbeScXf1K58co+MeyCfPMWQZygzHxisdimjRhpurUWhoRWelVTjivLzG4/wdHS4psyIJ5glUC63d/V08OZPboIL4SzPlEWE9uiqxS7wpbGdFzjyUwnqN4/E9Q2uA4FNTU+vmT4Uc93CbLNQ4cr3ZXEQMbgzBdS7jDIOW0++uHR0z9eys5Gw0eBxQJ/edcxfw6CwPukTHjsHxqd69axg0cowOOA6o5+A6hIR3sLGhQIdxdJOjsF29gOt4m1XUJIqjNmiviS5VXMSz6tHdzjgH3XUpDdfQFgRPwxdOxqmyOxgK5VPPgvFGQglzhDZ4uS3izueol5aW+orN8F8Ntf6fHEjnXyG+U9asXvWWJeiuqyM61874cH0uHFQDjXJY0cAcwjDqfGZj05XGknXXDRSqjOqp1UOjXrXyzuq3eWONWWu8fFfrqJ5R4Th7b5dwg/VGGeZVedI2ulIWijih5CSdjrnyCwHmGNWFHagjYv1yniqdltUxgIw4eM1wGPyoIeE+c99LY8RtwEPz+KEdDW04GclHlnsux4YV38f5eq7Oiq4eW5fMFN7xed3PrvAfg6aWGYYD7WfEShOx3auNQm3eHm2x50WyjJfE1MMyhIdVZ4/Ocbl6ZVBN6MF8Og/PZRTCIyzQMAFzxMgEwo4kE4wAUBx7dDXwfHTeu0tZnFcZckquyl5yDKzgnIzUsedxw2/23iCUwVzzZiPOk3R1QhmNJRnH1ihLvmmZbBLxXSdRyyhl8SeuDTtgNTE6Ky57cIXdLEzuAQbuA/M8LPLo0i/18hyLKm+syJw4gvCqMjPkR7yvS3zs5SMGlZnL1np6DevalJyJx15DIr6P42vmkz08e3TsQ8jyWZk+jIM6Kboy53a56Xd4dPXyGBC17i7rHrHekyuUZWvM8BJWtA3GD0O6hIb2+OkmhrPMB/PIxoJDAEYBTCw47jrHeW0Cw8ZQPQgrD/o5Ozs7MGc4z8rMhgBPJwLVMSJwS6TuHMtHyQmwrHB+gPuZ/cY5HU/+jvFm5UdbnGBkapqmn4yDPvR6vYFMPDy61s9GdFSqVnQeRFVSheIK7bMknIvfnafXyQU5S4zzHBNrAmpcxIrOj5rqI4scr+I+heSs3G4jDMe7Kmja91Lcy4KE8u473wuYHjG4aoB2ee4grAxHAVvZ0OOczq/2U+F4DZRX59HWR56XNhlROcQ9ilbYyMB4YRmSr/P+A1b0ccL2iCH3urvBdxCeYyU3aW7ycJ3vcRZcadxKzMReixVKY21OxmVJOBdOKIx3MbWSCoPGdTovuMcliLoSeGNYz2MBJVa4y/eojODIgs7JNE5csbFSI8HyAZjMVNNfRTElAj8M35V0DjTExe45dpyY9yyk4XM1NLRH58kAc/oSCF0/53sZomXQXbOmwyo5oBVo2Ow1wgDcDw/e6/UGPLq+oIA9P3tGjs8jti+XtSER9WSs7EpcxnkJF6c6oWL+VcFnZ2f7dcMrQXB5N2Sm3Fk+x4VxmVfne9nYaEjAuYcSqYw42eNxd3kNEIda0Inl5eWIiH5owzKCF7OwMdd565qRH+q97s5TqMd3HtzV45IPWt8wNGx8U2sl1ZOxcrosu/PmpXNZooiVtc26s3fn8gyxS1SaA+ehFXazl4OgY7xY6XGe42pWHPZwimR4TT9icN55iym3uTPIISzNdTkjqMZ8WBpqeU1jc7e0xt4ck8UJCKfMmTEYlhiqIl4sDVjNNVVm3ewCTx4x6LX5Xobl2fZYteL8XY2iJiW5DC9Xol7cw54I5JQ/Uw42ckBNmGt4U5YBDcE4TuVlOJYj9fyZE4F8Yf895gSJL5ZbNlJt1BVF8hzy+HHijpeaFxcX+8gP47e4uDiAZjinw8a0Cw29vOYskVokdFIFE0edePbuGQroQgyj2LOVvH1bsgZHTZ7pRxNWLu7WhGHm1Uv94yOX5z6WFFU/tTGsog6OjzlmRcjDSgaCcnOWHuOG7DSUWJEMe3aWEzY4vG7NibBxeHUXBvH4YByY1ElGbN9sg2QlDFPE+u3Sbe2WaCRF59/84Q6hgw66aN3jIh0MCD3H6sNAe6egCtNLCbSSly7RMBBblZY9OCMdDR2UL1UkPt/Gl+ZeNK5UHtmDu9DAyZiLi9XouL61jf2woV9G6uDQD4RQUH5OOmpOQpONXfRmaOiuCq3xBjrhkiZ8dIYDVIpF2zqpk8/WX5NXNRPKiqzbPDPPzPep8qBNNQ4udm8jFiIn0KxQWh+Pp8sNuJBBy2X9A+xUxQQBUmuuA7G6zpMqCKA+ruNBEjYsDH11q2ppPJ2hzIjnu41Y3plXfDiXwfkLvgdj0cUQdUrGqVdnptlTq9dWKD5uD97G86iJDCb26vwb32vuH6ZNplKf1KCpkXSeAfe5djMPorCdiY0re2Xti0ugcSLNtalGQ+vg823hjwttsrEdh4d3eqEO0yFf/d7Gq6Oh97pz0s15dz6vyRj15m2knjg7V+J7WIVngcHyFz95pl488+Clc2w81JOXFDr7jfqyPmPd1tXvvDW+uzZcn53Xch4dhof5ZRgLOcPyHRQBm0xUnvAd/WbjgXlz96iHdwaN7+mq8OoMOazlWJ13ycGz8756Hu+uzrKzR89iDbVGWp7rUKpNANWcc3UPm6nkdly2PIPj46IuSIGJIXaNp44Y3O5aaxSzcIPrL3l0Flx+Ek69PuSI69XtrdmHQwhnaHgcWJFdqDcO4mSh+ziv7pBAV+q81529NM6zlcw8e1dmx5UdVdIkFCibTJd4U0HKElk1sZtTFF6eYeVjavvtQoo2GFu6R/lwys2emefbIQQon/LAvCjBwyti4XOcP+INM+pJI2Igs68IoUbZXfvaTzfOTLiXdYaXHfmR2yw5WUND7YxT6M2DqEtuKOeSbTXeflLEk5dNjApelmkf1vNmVKozGzM1igrv1NuqEuq9jifHIyslP57pYmuHgPgcy5MiJI33WTHVK+OjUN3VybLJ1FXZ3djxmJeMPjvPXm/wgRd+554euzjCoXbGgTlmMEu2ZUq+owkTr5TBbYXqqti10BbH2ntqSMc5E1QlhbHMI/MMgSsRL1OyN1XF5XqVGMGgfFv8mcF6Vmo20CiDrLvyzfUqisyUvSYxVzPXDq7rw2LOaXbx5KChPboywm921SwiGOR6MkjK18YV82JieMcWW/dMCFFGk3D6EArITUBpwtWDadt8v/PkDjGV2mWYCKpdPXD8cAzM41Lqs/N+DMW5PxznM2qEvKnR4rKrq6sxNzc34CGBOLgPaEOfq+AxKym7M5T8PZMx3M+yiSSpPtkWEX25ZaPYRdmH9ujcmCqzQqmS59nR1EURcZ2V2hmhYZAKC47W4ZSfzzul78oD6s4SUFpWBZ3n1BmjrG9ZvWpIHBpS+M6Cj5CKDZAiMT5yG7g/k09V9tJ4DkPOs7Oj5DyXxui17Q6djFOvzbCDy/D9Wecy6jp4JWHnQWmDXhyH61NmXWJyVV6+l3/r45hZeRUG/TCVBNL1oTQPTjFcHO3qz8aBz9caKtzHW2pVKWAQOObFfUB0qkRdQivwWfLqynOWWOWwB7zxkpouueFe1bsaGkrRmVm+BubZi2tZ93tHUpsVdNbfwdIaoVCDwm3jPMYr8yhcDnWosvN1vS/rv15n3hzMdKSxPgtuydBkstMmFw7248gxOhSG/z4KisL/KsP/LtOWl1Dko/y0kUNnOhYcWrHB0vnm3xP16E7Y2LrWQHX15gr7ulKXTpeIBdXF5BHdcgcZemClyJANx9M61hHrX2uMe1B/RrymXCu0TlAZtvNRkU9Wlyq88ux4U2VHW1AIVhQ8Sbi8vDzwfHzE9qUr3hZbQxmMVyPOS6TaH2eg+aEezYepjmkdNTS0ouNcBuNLVrrk7YdVVhdbsqDUDArDMJ4s/tR49BqlYzjH3iQbDx5PKLgiJy6fKQfzwzyUPLo755ABeylFLlxPaYkrC0Vc35xM8i5MKDWHQ1g756Rs19wRG2+XvVee+dza2uCGGa6Hd8ShL5z0xtiWQraMql2TsyR6VEvExBPJv3c0cbslSMpU6+2c4pX4UBTEQp9dc7C31GYbHC3F/DWC5GJFhaNOHrhcdm9XYY5YHxPDOOvbexil6XLcKKs9bY6N+8XXVL/cp2tczjTUFlgWRl5Oixh8qYTCMTYEXN84IHdXUkitSZXa5AwrOVtcV46vcYJIEYfbjANy9Tv47OLZLFTi1yxx+bYxwNxxCAHlQf1cl8tFqDfP8g+OD5Ux7j//Aw4nVuHVUY6foCs5qFpSlJvBe/zGkdf4wRP/LTaW2nCui1OJGOHlkGqVnFV3gqWe3Q1AaaljHNRmXJyS1NbL0MzVp5PjchqlxJbzgtyG9qvEf2aQOG7MxsrBdFevGjaMjytX+l3LvyYI8Ztft62v2OYXhGj+olYWOQxgw+141PxMFoq4vNfEPTp74wxi6nqfMubOTZq6ooVhYZszXM4jZkoTMfgSQX6W2lGNF3dlMsFhYdMlPqfMzlNhKYj7rn1TBODGQmUoC6XUmXDfm6axKyVzc3MDy8D8/+VsAPiZ8FrK4HipLL5jCa1pmn7yEAlDZ0DAf618DwXdVdn1BROq0OrF3QR1fatlDXVV8kxpaijzju63llVlQzaYIbDyl7XD8aYquONZkZkKkwtpsv6zYtQmLh0vaozaVm/4yG3yk3BQXl6XhvIgQYdXdbPzqpUDRg3MT8a7nufxQiIRRgg88UYZbaeNhoLu+O3O6e8aJcd57ugwNEysr96sSzslxXNxNmIxLs/vA3NCWoqX3TU1VnyPxr/sKVzf1DvXjq+7r3R/hvAy+VI04UJHR8yDwnStF/0voY+sL/zdjXF2nzrRrv0r0VB/ycRH96ZOt9wW0a7A2USCXCw4qnKD1CK3kQovlI7/kgkvpkB53msP4iyvCmGmbA7Ouzp0HDEfqB/zp+PCm1DwW190WfIqzlhkpKhP5YfrLwm6hiNcN3jhvuiSKfd11LBSk3Fq4J0c81Kgy2NlNHbo7iZWjyVFblNyzYJrm5Mijh2HbdvFxurVUY7fhYbymZLq0o8qugs12sIPhaRuPVgVVdtmRVejid81YVANIuRjraNoMwzKkwuRsnCnpn13Phsz1SsXTpX6UUsj/1NLySKzx3AMK1Ryys7kBmhYr17qZ5f6UJY9gv73mu7eYoFVpcU5VbJM+fXekkdHjIrtn7oUykra6w2+Oou9ILejQslzyEbJCS7GujTmLvzT71n9Lkx0Rowz8+xR8d1l3kuZc+aDE6tsZEvGiMdF9WtYGsvfJmdQI5tcUAbrShYsi+Wy+LWtT84b1ZIqFZQDf8mEdVwWppqYj4VQITO/N555aPPogOzT09OxtLTU/62JVO4TbwHGnyG4JJ8zFswDC23E+k1LmQzUyFN2XdvR8S0ZUmxH5WU6rpd/s4PCPZw0Y0OmqMmNGRuIkjfvSp2y7hp3u2UajdVLFkmtP86BHJSvUUSn5A6aM6TqquD46H54KPrMzEzMz88PbNbgf00peSdWJvcsvP4HHPNTigOXlpb6dULZ+T/R1KOD79nZ2QFFZ8NW43FYCbgt3mzFRpznrwu8dwre5pB03BypgvO9GXG/Itb/5TLzxsaB50FlZRSl7wzdwXQ2aTqBtbG5ixHdvQ6udrmubep9tcSKwIm3+fn5WFhYiNnZ2di8efOAh+c+6RKkCh978Ijo14/6WPk1Ycf18Bzx3wAtLi4OnFPBhNFCn+bn5wfgPMfqqlg4Mi/uzSn8shLdz63zlSkxj6EbWxxVHtEHlgWHMLvCZlVcJOH4CbqI9bsV0XdGAzvNo6MjOJYsTZcB4oEZZ7zdhXSCSqTxNI4QFCg2lBKxOtpxHobHU2EkBIUVna+5uF37xTBS8yhQPkUIalzwXSF8KTuOMmxQFMrzPax0zgHgXGmOnCd0BiIjNZYuRm9zDA6S41jKR7SFLMPSSH/JxN4b11xskTHIHXZK1hVOufgxa0/b5XqzkEOhMXt0KPPCwkJs2rQpZmdnY5dddunvudb/C9cwhyfZZe6haLOzs/1XJPF75XmLpxLmZGlpqf+KpW3btsXq6mosLS31FZ1fXcT1b9q0KTZt2jTQX/0/MO4XnwPxH3EuLy8PJAX5Pe0wAjyHaqh4rvhJPuYhM6Y81iobpdDHKTsTw22VTUYUWreGHNgco8bQIb4uNPRDLdq4Dl6b1XWkSjcqsbJnCR9NrNTWiyMrI3s/eHIoJv5hlb0gG0sWSlZ0jv2BEgCjUSeuuViTjXGv1xuAydyuCo96dOwNQKzOe8TRTknRNRRBwksVEjxoqIP5qY2nca8zQl0z2M5pZMQyrEYKv90OOmfIuM5RqXOM7uKHWg/elUpK6iizxo7chDiE4dpQT4sMO7z3wsJCzM3NxebNm2N2djbm5+f7CqJeinMbDPc0IYZ2YDjYy+O6s/QcEy8uLq77TzN4KkBrjAcMydTUVCwsLMTCwsKAonM4okjFOYDl5eW+J2f4zWGHxqsYK0aJ4J89Hj68lMs5AQfhM88+LGEM1FiyjGFcdZOS5gdY9sah5BEj/j+68+KToGG8bfZ72PrVk3BGenp6uq/MgLlQ9Lm5uf41TWDhqLsLWWg4NmdFh3JD+VySLCIGPDjetoL6AZn5XWoskKroaF/b5HHk72w4lpaWYnl5eaBNNaasqKzMDIlZUTjnwO3qO9ccb6XPqJRBazZgusyqiGbcNNKfLDroNcpAjdrJWmPAMIljQxamNj7V60Ip8VEIPzc3N1AHJ6ZUwNSbcw4A4QB7dM7Eq6Lj9+rqat8bLy8v97+zwKlSqJFh44I23birB+UYtWm2/2854Dve7cb3sDxhjphXTthlPHD+KONtGFI5VS+uxB7boVQNIXFUXRpFt4Z+r3tpwLIBLEHxWiV3lq/Wa+tRYyL2ICVSSA3IDu+9sLAQ8/Pz/aTcpk2bYn5+vt8uw1H+DzHmSxWLvTgn4+B5OTHHAsXQHY8+MgyGdwWs5jlCnc6jc4Ix8144MkTnJ8X0AR/w2DRNzM3N9WUNfIH/5eXlvvJrgksVRf+jDcQoyqFUNbxsOHHOJexK4WObcSmtKDgn24U6b5jRxl3s3pUyqzxMHTWkGX72rEwu76ATyRPOSTmNrbENktviJBgLLG855SMn3fTBDM2+a184EcbeWGNB7ie37drTdXweK32biy4Fuvo4RuUYHOMDI8By5xJ0XEZlVldYauU1M2i41kYcpjj0wX2fRFgx9F73bJeXo7brqniqdPjt4m8WNCdwGr9pP1APzgNG6gBzvU7Z1dIr9Fa+ED8uLy/3d6yhDq4LSsa70zTDrhCZBVINCPqIuuA5AePVo7MX5+23utLAfWdF4xCJM+444l4lTcatra3F1q1bY2lpKbZt29ZHB/xaKLSpG3EUNeCIz/Ly8rpwFOSW1TRXw3PCYR3Pt6JHJZ579d4OeeB6LQ0dozMj/D2LidooS0ZofOOgkbPqWka9M/OKweXElMs7aBv8m3lgReDvXB8gKdavI7bHxJyU05g/82AOHehYaMgRMWjYdPw4u88Gi4WZPTsrpxocjAFvBeUlNo7bud8M38Ev51V0xxmHKqzo2jcNCzLIzOOnpFCdcwa4znNUIpU1zX/x++kzB1SikV737JJx4yC1qCCGyPxbr7t6mF+3yQIeDL8dWlH4zmV1269u+WSlhheBR0cmnGNZzYWg3ZqPG7u1tbUBJVLFyqC+Cw8c9Oa5YYTmkJYSl9EEIMYWCUn0gw2Mm3ueL5VZPuc8ZZssq7F3kL5NyZ1jY8PD95YcbC0NDd0VAnGZUWIJF1NpHBwxqPTqQUtPiKlCQrkgZCwAEFY3sOBD4RWUF7u+cAR0w060xcXFvpIvLi4OQGX0lz2eooXstxszlOE3icLT4R1qKMf36FIaFJA9LsN55ocVlOvmMdXz3CY2GTnkgxUEzB1vQAGpA8o8pu611/KMklQG+VjKVzjDAT44tOBNQowy2CANG7OPtLw2ikKjnlrowdR2j4Ov/DtLIHZFJxoblzw5eGKh1TgSgqNCp2PtvDju5cdaeazYkGm4ge9OKdWo8Hl3DfVlslEK7dpQioPgfG9pnkpxr+5MdMRIhdvLEnRtfLGh4znRcdMQcli9q1Z0jXtUqNXalMhB4NryCk01FmbSmJW9Lqw5e3T3Er6sT6zg8MwREYuLi3HHHXfE6upqzM/Px9LSUn/DDMqura3FHXfc0d9AsrS01G+fYfva2lrMzc0NeB6U4VBDkYALadDPtbU7N87AO+qmFx5bzQ/Ao7vEHLfN+QI37jByOKKvqtCcmEI9LsRQY6Y79lhWOdTCkZccOVxSOeVQgeNxDV2YnOKqUWEDz3kJ1is915ZTUBppCyyf35HkIJIKq8KtiHwDB+4vxeVMXA5twGtjnbfX6/Wz6eCBhZt3ivFuNVZ4JMpczK5xbbbXXccn89YYMzfGWZyOtjMPnI2X5jTcuDO/bvzdvGTtR2w3zDBwJcXJ6gapYmeyl/Ht0Kbb2z+u+DxiDC+HBHNdLYyS88b63VlOJ3yIOTHgiJOzvsDbYbmmaZr+kgvXgzKoQxMq8OoR0d9QwnvdIWhra2uxbdu2fkIOdfL+cdD8/Hzfs7NnaZpmYH0ePGWkxk1RmLtfs/Eam/IYsoIx8sMTczBsWE5ELoO9KWfQ3WYelNeHcpw88NN1POdsZDQ2xzX0IcI7EA6T3NgokuxKLrTIvH0tKu7k0R2cHUa5M2udnddJVPikn4jBJIeLC1lQOHkEaA0ICn50DRgDz5AYE7GystLf8qqKjgnCAybgo9fr9Xd8MZ933HFHvz5WdN48grIZqeBkCs/jxP1nw8njxd957jj3wIq+uLjYP+KcGmAoPhttNoiq7OCD50ghP+rm/AivwbPxVqTKhpdDFfRV8xr4PorTy8JiPd9F/4Z+Hr2WunY6g0iZ5dRrGHC3xVL7or8xkBHbJx91cuyOPqlFZUFbW1vrb1ddXV3tb0xBOxByCCAr0dLSUt9bAyUsLS31DRmMCBukzEjyvLECug/PLZSMhUqVWs9xiIJ72Rujz6y4qugYf4wJhzusoKzoKjus6KgDpNCY+53BbSdDijjbnJfy52J017YqPPPeBTF08uiZpckabssogjSxUavMDNP4N9rq9Xr9JS6GuBq34/zKykp/1xU8Nh4AmZ6+86WK/MYY3ou+bdu2/i6yrVu3xszMTP+IpSLwzRCeeUJSDYoCD4/nzqEovd72/e0cEqjA8Hyw0Nxxxx0DL55gxeTx0Bid43Z+w63mB7g+IJfbbrutn4C84447+uELrzhwfZC15eXl2LZtW6ysrMTtt9/eRwRABfiAX95o4/IWjAzw3jxWdhgvhuVuLCI8dHcKz4qd5RH4N5wKhxjI4+DfW3CtloZ6ZxxblzYa1qPzgIJ0kHHOJZfYC9fyynAe3pb7oAk7B/0hLFA8ZPJZoFnpuG/ggTfRwKNjbR7QFf3jPEGGVDQ2ZeHJ1pEjtntVNa6s0FCqNo/OsFuz7sw7e3SgCvXkLpGHeXIOQcdEwxiO9THPkJ2M3LVSvKxyqE5Q63ThsebHuujVUNCdqUaJXKc43ozwb1VxWWEto7u9OG6CVVxdXR14LJOFCX3AALJnZCVAXVNTU31Py23yk2Q4D+Xk59HxUW8eEf0yGGegEXhteEeU5XtU8HieFKZu3bq1vwS4uLjY7zdDQ+ZLDWibR2fkh/pvv/32vkfnGB1jyg/OwDhiDLdu3Rqrq6uxbdu2gaSeKqkSK4oaDK7DPUGI5VBGBopqSorNBPl3yg5iA6ooWZOFCuVraOisO87VEntDFSCcUyVXCM+DzDu0UJYVXQcU0B3Cw3EleOOB5IlhZIB2VDjBN/4Jk2EWw38oKBP6pLEuCzvqi4iB8IH3obv5UAQC6A7lwdtgXXKL50XnjNvV59JZEKHoW7du7e8G3LZtWzTN9nfYcRjECowXZcAw4H5eHVGv5/rP11A3lF2TsooGa0g9sUOxmbLzmLKTcooNx8QGoNbYjPT/6O4aOuWoxJSzmBE+fte4nBXcxVKs6LgX9TH/nGBj5efJgxBgyY43rUxNTfXP8ZNimDB8d/EbKxaEMCIGtp9CQPTpMw4J3HwwLIZnhEfHllz26DpfzrOzR9dwipESoDvialYwHHmc9ZxbjuMkHqMxJoW5UAzdkYi5wTjp3NeQS9SWiJ0GxtfphpvPGuPmaGiP7hpiSFoTG6ty8yci1sFzVnLnVfWhFIbc+M7xMic1MEEspApXHT8s7DivL4vAO+RmZmb6715jC86hAr5jSQ0CsbS0FHNzc33vNjs7G0tLS+sQQim0gmDfcccd/YQYvwU2U3R8Z3712Xj2RjwH/C55JBO5zaZpBjy6zuny8nLcfvvtAx4dYQ3a0aQj9529Intyhf+QX/bo3I82paoNYUG6e4/bZ4OhesdoaeIevY0YpoAyuILymffAOVYmNghO0FhxnRFhoVWjxANX8oxsjbkcvDo8PseHavx4fPgaysJz8YafmZnt/7ICWMz8ZfCdhYSVjhVHx8AdMdYwQIyQdLMPIwX1xiy8rFwsvBg7XRXg8FEVUueN+65LaZrUUrSFPrvPuMnF/ZozUW8+MY+uWU6Q67zCYhYClFXPqDG3QvWpqe1vIeVtoupZ2KNDgNijcwIGAstLOm7C0Re2tiyoOIJPVlruO/OLOtA2G7eI7c8XYMkNCo/v27ZtG3hxQwm6s3Bv27Yttm3bNgCtNWzBPCm81PAK/eVzrKgYIyTR0KbzvPDubJARowN9uPVzVWZOuC0tLfU/fA4e3fWb5ZQRhoaVkPMa5JpRlthjQ8ayjNBwYh69NsPHXjUi76R6Y4WGDMU5s82xL+5XRcegRGyPFzkZx/Vy9tJlcDNPwePCKISJQweFW1xWEQKWzzgnwH3msVEvWvLo6B8evMF3VhT1XCyI7prmPHgs2CPBo7OX5z6jbTU6bglQvVkGcRmya3zOHp0RIMj1162ta1+7kCp5lvxTD15yuhl1/kumEoFpWGaFVxE+g67ewcXerPx81GU11MPxNiYXAsX8cXzGwqcW1cElVQ4WNr6uXkjrYUFHm1ia49CCvS/idvUwWbih0J29JI9DRAwYQowNC6EKp/PoauDdllOVBVxjBeL19myTSEmxEdfDo+uafAmhshw6j87IRR1ciZwX53odRNfYnOWshsai6DxRLLB65PLqhVmoVJnhhXnAeR2ZY0S2zLxJhb0mykLAEQ8DEvMganZWLT8mAsrIkFzrY/SgyglvhrwBhxtsBAE7MQaq6Fyn/maPjoQYFB286d4AVXIlvs4eT1c2OOnFCEWNM9AMPrqDLYOrCtuR9OM99gzb2fA4I6bIklGUjofG0iXi8XJGA5QpuDqOsUN3NN5mRZxlU2Z0YPk8D4LG5zoBmmDDkZWPhRBHDQkwQS62BGnSRydVY1v0IfNwOkkM03l82GihTg4JcB/zzkeuhw0WK46iEEUezGsWG2ssr8RLWNxXNowwckwZXzpXGh7pU24O8jO/mWzxPgnNM7m6SqShjZMXdgQlA+76UqKRsu7MiAotCyuYVyVm5WRrqX9QoHvL2aOzV0B9aI+P7L2R3GJIrBlhVgYkbTh+ZGjPyAT1q+BkQqAxIhQAY4FzqgRcPhMK1w4EhPuJ6yUjwQZUjV0pnuW+ufo13GEUw9eVJ0YHmEPMI3bgcRJOX3IBvlmZ+b/y+DFjIEreGKRGkNGBOkM1goqSXG7DGTweK5abGhprjA7KlN5BeBfv8fc27515bQyUXuOJRjKuTbCRFcf9HOs7T+biV67bGU1uE3W4nAHCAealpOhZ3Md1dvEMWT/Y6MAoaZ+ydlRoXb8zYjjL6+RZ0o1JjVPm0XmlhEO0LpTNUQbfawz3xD16lySAY147lw0sJ+PcOb4P9TK/Kiyc7GOFxT+A8LoyL20hRuT1XxYg1I3JZHTCyRzNkOvYOg/JkJYVgWN6jKEb95IwKAJyyCQLsxhyg9jAqvBmR1ZqGDFGNdwe7mFvzi8K4S22/Ns91sp9ZeSIJw3Zo8OrY/6gZJwcVITDY8Pjpoa/Bo1lNBFFL1Xs4EqJWOHxW5WdlZrP8Tq6evQsDlYor+dgXLDmzd5AlcslR9AX5pUFyHl1/a4KwmOl8JfvgaI4oXK/QZln4jFlA+IUW0mTiWyMmBcVbo3btU1GBXoPJ+EA13mrrWbseZxdHM6P/7Kis6HWcWAjzHxjnLk9RbBdFV1luZZG3hnXVclBquw4p5BcFd5Bd9TFExHh//yBJ4AFkoUaSg/PwQKo3pzX6ZkPVXSNy5hPB4F1jFko3Di5EELnSI2FKhZ411UP5ZdRBs+/8givyXzxOPPRwVf+nRmHLIbHPbx/giE87oEnx1bl2dnZ/tE9PMT9Z76YB0ZamfcuKTnPl7vuHEUbdfboXDE6qAKMa8pIKUZiq4pB1d1v/DbS0o4s5pWtOHjFpPNuOByxv5w3d7iYTxXdCavb7cdjoUt5qIfHRtdvdcMPyujGIzcenFeAt2UjCJ51z7lLlvFec+WZFVjnRudLEYTyjutsqLg/iqyYMM+sjHjkFwQvPj093f+ba/xZJpSfEaaOI2SEkZfKvoPuJU+uBjIj5xQyGtqjO6iYWWO+VxXCeWcnAG6AuIxrn89hUljZmBf+rvCIhZmNg9aviq5eN1M+HdcM6jp+3Thx/3QsGO6jPRYYNky8/Mheiz261u/4yzxbm8CXqNaToZ/KNxs29tqcZXdhpI4j6tBQxYVFNUruypf6PnaPrhWqEDmvHrHemul3VQI3uOrtI2Jgs4h6M12KAB/q5fkFiziH5I4+jMEbZ5xH5zZc/1VpI/xGC62D+8jLQKzg7m+THbFSwyuzIqMdfZ6AX3sFJOLqZH4zI+cUPDNUGJtM2Hmeefzw4cd79Zl5HlP3lCH/75yGR+rRWbkhO7xSw+3yd0V4Ws71WT8T8+hOkDVOY3Jwne9zlh71qADwbwibrqMzv+yJALvUe3Gf2AozAoDQ6IYTVXTU1SVnwYLqDAWPiwtZmD82eq4dFkYeB/7OL5RgLwYDyOOI+zR8cfwpGtFj5ihALGPg2Y0Tf+fdeZqL4ZAQSq2xeUSsM6qKZnhrNYxiljPQ/gxLXP9EYvRhyUEYnFcB5hidvbuz+urJefDU8mmMx8SKxmXY03Esz0hA91+zotdMiEI8HQuG0Yx0WKndCoT2zfVZE1mKeni5zBHDXx6/Loquc8dek9vg+vk8yvNz+Zs2bYrV1dV1Dy1xmwzNodzz8/MD+SDtj8oH+OAnytShOMNfQl2urxlpKFqikZ5eY0vp4kEuw+fwXWGUWloWavborAyZMKkgcyKFLb32UWEZJ2F03ZTLc7KK13mdQoGYLzVY/AeHrPSIH1XRWdlRN7ej7Wa/VUhZsHXOVPBrFFz7qQrbJje4xgaBDQ57WA45lEeWMf5jx7Y9DzxeaqRgXJgvHUMdR9RX0pOSMteixx3i0ZV04tBxhoqZsGT38neFeXxfmwBFxDqDoB6BlQGTyHCO68hgNLevwu5WFFysq+Oi9bXBOjU+ikba6nBzwbwwT1mczte0ztI8ahkOWfBiRyh85tFV0fn1WJk3ZcVFOTxxxzKh46F94DnKkGbJo9dCdtDYFb12wkAMQznzqZtlVNCdIWAr7GATBLtkIVGG3/XGCs/Kwdl3XONXFGHyuRzawJHHC/3gnVh4J5x6dghrxPYn+FiAXDzrQgouo8LnoGHJYPI8KNpS5ea5dx7Otanogg0jCEukjMDY6OIezqwjDucnITWf4SC4hoSI0cEfGxkXDmp/1QHgu5btquQRO8mjO3LC4DrsrKXzEJwczDyrtq/Wlb26g8Ls0dmwcHv4zqFDZunZcOn6OcNV9jiZstSQJtVcH51gsbFFH50hbvPg7hobHJSFwmn7zlNyeTa0rOg4sifHHHF2HoqbeVxuQ/vMCM/1Leuv9ofHwc1d7Zx3VvTMuvH1tsadQOmrnliBQCzcGrPz5ENBUZcmeEp8qMfQLKsaAVZovle3g2osz/1R5eDlMn3BBs45z+n6pvkBVRgNObSP7KW5nMsFsCFiQ4VzrOh8dHOsSgJeEHu7zUaQmSxHwnzwspkuJ3JdHHO7MIdlFUk5bU/3XDCxnJf0huVY+1RDnRVdO+yo1tJgsBTi8uBpW21KzsKFNkDKUybget4puvMmOEIxkAxy6+4lvrPHcvWc9l/5c/1zypF584hBgwYFzgSNFZk9J/cX3/noSMeT5QNJNx5PJigby6p6XQ4J3XIiGxgeEz5CVvFXXRGDG6oy5Jb1NaMMXU1E0V2lzquz4tbCCo2DsmWDNtieXdf7aweIvXpmcdWzQGBwTeEfjqroEeu3tGYxL593fGXGqK1/JVIFcCjBjbubI+fJSzyyIQJiQ7uK/Jgvp+hsIHV5khVd++0UvdfbvrSGhJzmjUqyljmWGpqYR1crX1J8XU905QBpoBS871qTXkxO+DNFV8XS5FGm+CpYOvkOfrNAAA7yJhP1QLxOyyhAj7yOjiPHlIx8XEjlDIHmMBx81/Kc+FK0lS29ssCrN9Xxc6SKAj5ZRlgWS0tNbAxU0fm3QvesrqZp+v/Kw39+iTmDPLOslULFGlJnWGOgQSPvjCud6+ppulopUFafg05tVlbvZWPR1h4IE62JGjUeCmvVW3MZ9YRq3JQUifB88L3ZvLJSOCVlZc/Ql/PsNePn7mdjrQYafOjad+YoImIdYtI5KfGp7cFIsCMpzU8WUjk+S32Z6IYZ3v7JDKg1ddZV4Rcnu3A+Ew69npV15fGdz7uBc+dZSTipl6EBEGfr+b/RGUIqz9nmIIaXvOVX+UQdClPRB1zD2rHyr4ZY+VHFK3lSNRRQKJxDmTaPrnwxGsE5pyjOgKnBUkV30J35QH2a1ee5wg45HS8dM+WTSZNujJQVybgchaOh/3utFjLxvWrxmFQhSx4gK+uEhs85+F4qD95Z2bMwwBF7oCz24z6wcOgrrFkg3Tiox2b+I8o7tbic1ss8MN81SVnuty4DtiES5gMC73hgOK99UFSZKboe3TjoOKnB5lCAz6m8uDGvQZjcFzYApfFjmshfMul9tfeWPLlaSKfkziA44S4JfMaTEgtbVo69WNZH5rsU17qcBPfF1c/eHdDSxfEZOWPE7aCubE98qV+1iq4og+WJecjmUseGlVxRU6bork+KSDPj4RxH6bfyquV4Diei6Aw9az145vFd/J4paSYcNYKjQoJ7XBa9pPCZwYjIFV6VUb1RpvDaD904kxk7JQ4dWBGzWNyRg/X8m+Frm8Dq9mY+ZveBSmFhts8B17geVcTSpiTlgw0Nfrt9Blj+1ByNI8dzyUCwJ+dnKmpo7Dvj2pRcyym5wY7wcB3Hrt5hHKSKn8Ez5jXjXwVfPWFGbajEeeASL1o3jiVPWUJHbl5qjJTjWY+la+DDKa0zls54to1RjcMZxaO7613QsdJQil4SajDE153lcnAvW4rhRAd7B/44yoRMvbqDfiUoiuuZkDvh0J1imaDVZn2ZB8cjj2PbuGi7OneAibim8WJJaTOEk/Hh7sV33bzilDzjg/vpPHpEDOyMczygbd2Qs7a2NrBhhjP6PA/ZzkwgA27bKTg+vGmolibm0fV3myXLFNIpgpYtTSyXc22WFLamvpp72mA3vpcejYzwy5PunP52ys7jyEdVYk14cTn9nlHm0Yedu5JHb6vHzYObH8d71gd1UKXcBihDvG5us3518e5jVXRNGOg5UMnLOCV22VGOt7J6GVJz3Xre9UOFXnlVD1LaU58JGI4qGG3UloRh/tz4ZLEo7uNPr9db5zlq43yn1G4sHGXoysHXbK5KzkMTm6qkWd8UTSHJCYSKOnQfRNY3d821yfE5ZJf/kLKNxqbozAh+R+Qxulue0UF3x6wM7md+nGBp4kN5bFNy5tXxjsl2fXIC7jwrUw1CUoFW3jUH4NaL2QjqXJb4yfjm85kBr6G2XEHJo2eGtU2uSn3jdnhfQJbBzwxFW+irxOGlrqfX0EQfUy0l4tjCRtRBJDdx7p7M43QRsFpygpa1ocKqSEAVVD1ZqV79zjzp8welpI56dP24frgxcd9ryLXhkKLLAbXxkzmKLoqu9XG92fdR5Y7HgT36RJbXaplom4gMTuG3G5hsMjIvkU0OBL1tSaiNXLttgqfjwn3hc22wroYn/GYF56epOBGqhkU9Ou5VD1ID32uFnOvi+jN+MljPfOgxC/96vfXPGGR1cXsYW0B3hevZvoeM2jwzjz/21XehiXj0YRVIqeQZhrGQ6nXVAE2C74xq48xa0pBDvXcGIfW3U6qsrGs/U7QuY+IUXM/XjAOozWmUnEQJObZ57KzOWrhd6ldXWR27ovPf8NYw1AZ12rx9LdUqMuoueQ3lh+/Lyut4KMKIGPx76Qy1ZPW79rJ4nv8PDaSIhJU9iwlLiKpGuRy/uoznjE5JUbLlS5cZr0nGKaEs86VtlDy5WyKsJbTZ6/U6e/WJxug6KaVBLG2S0XNdOpgJY1ZWFbHWcrp6a2F4zaQPY9yccQFla7pO2Xgd3SGiTKkzvmugvBqaiLDv3dM6kQHnc6rMbV4dlMkrr+JonW48lGeVqxpYjzYyA95GY1F0jc8R19VSyaOzxxsHn+OC545qvTp/Rwyd1eUEqNSHLLmZ9d3B5SxO1vscPGfh12uOL5xzfDi+M5SShT1OmZ0nx/fs2QRuH8kwXbp1Sq/34sN/zul459+6jFyDbJSGemec7nt33qDkTRy5wVHLOIrCj1PBnTUuCZsKarZJgmGgM3iZsnAcnrWvyqRHhcqObz5qfx08zvgthQ06Tg5htPHB4+qguftAyduQJY8172hz+zr0Hg2DImLdq8Xa+qTjU0tjeXqNJ4KPuAdwimEVKDMAGfwZhtq86yQo876lyXF9rlF0Vx5tO6OQHUveu8SzM0iZ0ZkklTy6lnHjWRpX/q2GvqavJSPKRqOkD6OM51Avh4zIl9P0Ooi9inqfUif4vFte4fpRvhTHlOD7qMpfmoi2unGvwkr3soZh6teyGWRv8xQZTHceU/nVeXfwO+OzjZxCZgk4PR8xuPxWqhvQXY+ZgVNPHhED39E276XgNksoYSLr6KUBL0H4rlSCQOPgc5hro5ATYL3uhDT74DpTyXMrZcrkYvSIcgI14zXrl/LuvOOopLLjDJPyV4qrazx6Vj/IhW6s0FBY3jzl+MgUvoaGSsbBImm8kUG/cVAGMd0AZWUzYe7Kc6acqCu7FrH+yT/1OPqdM8ht2yr5exb3Yrz0Pke6tMl9YV75rbc4tiEQbp/HRBNcTG6MnWI5z+qUPCunbTpU4vhyfcN31Q2NzflPId3bhdoMSBsN/eIJPbq1VjcoNVTjnXjQVNnd0lGbktfwmglXyUu567oXPtvH7yBxRP7iQD5yNl+Vf2pqqrgqkq1F8/fS/u4Muiu/fJ2Vm5XdEY+xGlGnxKUlNa4zU/JMsXXFJOurC490TvDoq5OVGuTRRkNl3fXIylPjLUaBaM6jd+E3u5bF+KOQE8jMM+KcxpClN8xoO3x0Hh3K5/5coE2gHd8uBi55oIj1KwDMG5SbY/kaOXIyUON1S3V2odp7SrKI813an6hHz7w4l1FFcdAo805KzgrqdfYKek2PbbCe4aHjvyvpvdpfQF9Wan55QelNrK6f3A+FizxPJYPBxin7uDDDeXTmMTNGJU9e8urcB7e0lxmz7L5a4+Dqc3klRY98Th0Azqtxy+rv6ow6eXS3bMZHvjZJYsGt9ejZ/ZMgnRAWbj5X+qCekrcchhjWO96YGH6rYVa0gWsZj26sFfG08VNDJS/f5f5RyfWXFZzLad9r+z8xj84w3cUariO1xALjyHkrVZyae7NdXxmkrPEubf0q3ct/g8xPPPGfKzovxXU6BMXe0m3QYWjc1nfnBTNPrmVRJxsXN0Y8hw4FtS0lOUOZletCGerpQq7fmBP9l10nn85oduVhJOiux3FQjYfmTpa8gIOtfN5B+hoeRiFNUmVJt0yZHLl+OyPoxogVvlS3KrGDyl2SRS48yAwO2s92FGpf9Lvrk7tWA99HRYHqDN38uI1lrvxEdsaxpeG/pcU5PmakyzVt8TlTVytWWzYzGkyZUGTXMnLLVTWesSS06u0ydKMGkcdevaXOi4PumjB09+FetF9zXsvUzqMqecaH9rMGBQxLjHaHQQFYSeLvWnctdd4w4/57mr9rkkHJwcFM4RVa15KiDxcXqUd3m0Rq+lBLLsaFkmgyTr15Sclx1ARbCYa7etqUJOOf19GdIDIfXXI5XRRQlyu1H1rXJJS6ht8sbudlYRg33jYe4RHNxPe6jxKHR9Q/kjoKDePRQWxJx8mXM3L6exjhbmszi4v1d6YIjreapbRRqc3bZ+dq+eg65sOQ68MoaJN/TywZxw1ojD6M8rP36hrjjYMYWtWUGwdfqty6vObGYFwGh5VUEYDz6u7ooK8+/w2e3bEWxpbCwQx+D0POyA4jy8MgTxDH5AiNOUkXEescj+6Xb6ORHlN11zJSKOXiwNLvrjSKR9fr4zY8TrgyQ1dKNo7Kgx41ZMk8pMshuHKl9mqoi9IM45kzhDVpcu1wIs6tUGj+aGIe3THWNdPOwqyZZVzXQcjiOrf04LyI+2hd7rcuf0xC2bht9SRoJ4u1swcgmE9FIvpbjYxm9t28ZNBdx0U9uEMFNXE694nPgYcscdk2T7WhTy2ay1CP1tPWlnpsHm/Nd0w0Ru8K1dVjZMtHrj0W2NL6t+PT1ae8t4UeqjSZwLm2tP9tNMyYZhNdAz8VRWThU+a12x5cceOsCaUS3FUYXYMquip7l7JqrBxPbcru6gQpfEffMXYuEXeXTcaB1IOwgHEZx4MuJfExG1xnDJzxUB4jBreLTsKjsxK0GZuu5AyVXucP/783rrtyEbEOiWmfuH2uozQOfHT9yNbSuyh5F8Pbpbyrf1g9aTOQuDaR/15z219Lk+MICu3+JLEGBkIhkawo7ZhSb+EQAf921hH8liZ9XMrPO9nceNSiBKek2f5xKHc2DyjjPlgW1LFQIeXvEFp9yWM2J9ovKHvE+r3tmtR149LWJ0dufktIhMc360sbenPKrfO6srKy85JxbdQ20LWQx/FUsqTOIOn3UVDKqMqO+9WzM3IZ1rO4+B/XM6PiknKuTNsGDu1Lm0d3GXl3Twn5tY3TuBHZKJQZEZYFnHPjO3HoXnvdwTyXWXa/+TgMlWJ49RzqfZhv0LgU0PEJj8tW3KGVGoSh/SopTo3h1etZLN82rsMa8xrUOEy9WT/beBk1bO1CmsiMGAwpu/DSyaNn3ryk5PxdIZVLxo2q3I4fBwndbj6nHDywbGHHRcqTQm19dLFNebsoBt/Lx8yj83f16MqjM1j86TKGbX1x37vSzvL0JaPNYwjiXXQTidFr4DB+YyJLg1eCjMNQKZZqI93Oq8qt8HfcQpHFo3ydeciQRUnJ2yCw/lblzpZAs7FwdQ/rzcdB48ql7AjS+WVUhJeGdA2fOyfjmJGS8mcTzcm4DCoOS87rlRInHBe6x2zhjdQAjVPhS3yxN8zOsbK7+vS6nldy88F/PqgKr/A9YjCZlJWvCTuy8elK44bb4wgtSwR+2WO7FY6JQXeufFKD15WP0vWsjOsDC2nE9pjcrS6MGqPXeOXSvZMSMFdv2wMjGY1LProigDYDsjNonPPFfZhYMq7Nkw9TV9d71Iu5ujQZpUuDWhbeXOMeLB/hTwn1CTeOp5kvtboo58IALcNl3Z4BF693tfKjoCftowsxGB3xd/3wfSWhLfGahY7Mj64OlIxBVqf2uVYHakKmrA6WudrcWEbVQfGwnjzb+daFSh44K18KK5ziu4d0VldX150r8eVCh7b+uBCj5t5S+5OmLBeSKX1Wpq2NLgbJOSLmYVjnMq7ywxpXl7eqlRWmzsm4zCuC2jrTBW5k1tcph15vs7r4rt4HHh0TA2+OYykR1ZaEdB5a78/6iOvZDjrXx3FShla0badYmUd3czRsCJcZmjbjXOoLjqU5G4W6rj6AhzYE5GjkdXQ3IKU6xhmLlybHeRRXBt8ZvkdsfyUyKzkrO3/PlF8hH59za+SON864cn1N0wyEAjXUdfzdvndu34VQ+nFIaVLGqGTwM4+PY5cxzNpTKtXJY9hlPFimd8qGGS43KlRvi8l0kEoT6I5Kbs87x8lqNDg+r3m6bRxGUBVrFEEpGW2ur7TioL9Lyu3GsOTVlbSvk1jtyH678e0y3sorx91atxvvjLp69bH9PzqOmeDBM4yyVs7tlTrKHlqFb3V11SaKcF4nIVtC0g0juEeXlNr6gaN6dN0Oq/ueWeCzMS0lfdxGHByVD7SRKRrzyN6maZpYWVnpjy9/b1NwRRIqV21LdNyXGs+ehU3KU5vTUJ557EoEHkoyrUazC93lPHrpwQSuH97U8ZbBqzbYlQmBogdWElXSkvCpIeDzrizXn10vGdY2Ycx41DFwfPM9OKq3znYfumOJnJJnVCt/LD+ODzcGXXjOaFjd0DYnugV2ZxB7UrVm7PmUStlete5IwrFHitj+H2U8yVNTd75zHfUwj/DAbUKpXr9WqHCet5Q2TdOKkrIsOfPi2mNDg7xETXJUk5s48hg75c94d8rrkoOOr1qPznU5j+m8vsoQKMvdsBMrPXnp+hEx+OemXWksHr00+cNSBs10cF0WugTdXF08gDyo4ANQE/wgM89KoDvWcG/m6XVdXfnSUAG/1bBl3q7rfLl7dI+AE3S+TwWyaZp1ip55etd+hlpccnAUyuRiHMSwvTaUc6QhUVt5pc6KPo5EyCSptvMlSw9iZXSW1GXkeXmOLbbzTJk3LSWAOI5DItA9LlqaJzZijk82QIwcNFfhDCp+Z9l2ZyDaPFTmXdvmuotHz/IVINdetqLjymTXa4xK1s5Oy7qP0wjUQjt33W2Iye7T5Bzf1+v1+gkkfsyW93HjGmI+Vjp+T7sqE/PKkB9tc93MDysb7+JzY5/NB9fN5VTgNSGmZXF0H16m1HF25WqUUn+r8SzxmCUfQW3oIvvt7nOhCZdhp8Fj4Cgznl1pqL3ud3WvHlFnmGoMASfFWGBYcKCsHM+rUCnszZTdKZ3zJlqfUsnjOQHkfqjgZnyoZ1F42aa8ztvVzFsNb67O0jELSyLqHiSp8czu/lKWnY+1ziujoaB7qTFWDDA4jiW1jLrWzwqr/XDJN7SB8pyB5nPsISMGhYMNQ/aaI+5HlpnPPlmSTImfp2cUwfyzwjs+dSxZUJ2iZxBeldwpX8lI8L0ZfxpmYB7VCDuvXJJ3lu02o8akSq0GslSW2xxG2cfu0e8qHn9YC5vFalAofb0PlNfF5rr27Hji61Awl3xywta2M84lACPWv72GY/WIGPgroJrxK31U6ZkPrQPfXRul9mt5dIqfjSGPlZIzUplyZ/xm/c0UfFhPDhpqea1kRcdFWbyl35l4kDLrruWdUDK5iVCI6xJUGk87mKiKBmPCCu/K4Bwvs5XGTs8zD/yseVuIoHVwn1XQnZdUyoS+tk6MQUZuo4q7H0nNEq+OSgaONwXpag5WISIG8yyufb6P2+xKQz+PHlFePoIAjgLb3SRlHsxNuCZrXJ2OnIdRBVVvgOPs7GxfeFgpeNusexkD88aogPnVl3W4Okr9LN0DHjkc0XtKY+U8mjuXxeVuvGuyzZng6xw5g6Fz58jJFfORoRi+7u7j64wUa9reIck4NOYGqDRgo9Ko0KW2rhroiH4yvGMIjyOUnZezFBaz8pbG07WH8zWKzkaZhQoGRZ/ac98zKnmbbKxLELZN8bVc1kYJDZb4rFGkzEhl9brz6ulBpR2Iw9JIf8nkYkn+PqrSO2+q7Ud0f/RVj24SnIV2pH95PDs7GxHr/9JY/4Ujwq+1457Mo2YeXam035736fNfHoOXlZUV+6ovV59TSr2u1xTSqhfE0cX0GVRW0rxDZjxQf9eELvOoS7TKF3tu/o7yLAOuv9qmK9dGQ0H3GsgzKmXKx9eGbb9WMFXJtTw8ISs1PHnE4Ot64fEZkus4KlJAnWgLxxroXiMEbGjQV+YT7bd59prxdOSUtQamZ4qOo+Yiajx6Vzhc4zBK/eB7NJxTvjM004WG/qeWWiUbJk53sMb9L9UoSq6/1SKrJ8gECsrO/IFfLHsxjMc++batkU7BcXTeOvPg7jd7bPDNm3vYcHE/mYcS9C3NC49xaUsse3SdIy2HMjov7glDrk8NQ9YfJfXQ/HGyogrLMsZjACq9XKTNsGQ09H+vjeLJS7Bek0HcZubR2QOVeHcCo9d5H7v7uHbcZhfeH8/ekj169vhiG2zPvDyfc/XxbygBvz6Ycw0K27X+kpBlKIPHUIVe/6aJDT3f55QfR54D94CR8lI6565jLpkfVfauzoKv14aiWlcNjZR1x+9apedyNR6gtiOlgelikBxUcpOGI+rmJTGQQl5FIrykU+JRlVrhf1a+1H/eGMOrA84IOWVvowytqKK3efSSgW4zwM6rZsqmMuyUUZW8VL/jQ/upfXEhnFLWfg2N/G+qXT27wj6+n5VCBxKKpArVFV24SWRyVrlmgMHDzMzMQCab4/ipqcE/mGTlzUgVxsHRbEde9ps9N0P2iMG/Q86ge0ZsHNw+ehx137vz1Kq8bBict2TDwCjPKR4f9Xzbb2dg2mREDVi2/9/F50o6TrU0skevpWHW00tK3FXBS23odxUezf6WxoANEU+ua5O3vLaR25EXEQMGpXSfO6/IAHWp0DlFd+2hH1hCdN68ZER5bJxHd0rtYnT8zq5xGRxrZUkNkZtjVWztD9fD7dd4dHesobH8P3qbh1cBKcWTXGYcisw8upiJ+XdC6GKvGnIT1+tt31DDWe1SHXp0Y5Ml7rQeEL/4Uj06ow2HOkrKrlt++TwLPOJxNaQRZUV3u810XsGXUwbnlfk3k8p05qHVEbj2nBFwKKDGozvEUEMT9+gsfCWICeq6ntnVGNRAOZ0AFsw2YiXSTGrpCTdHmSKpQpdidtynfdYHWPDhc2ygWIlKbaAs/2YFdUgJPOnRISk2vnyNk3EZEuCxUmXN8kc6bs5AZOfc/Vl9XWP0Ep+OxrJhJmOs5LEzqo1BhlFyV4ceM2/OCt9GWL7SbbBs8TUOjmjftgoj4Ty68/4gNVDqtXl5sNajKw8csmgOAv3PDKcT3rZkXBYXs2EqeWmlkiGrUaiS7LhyWncJQei9XZ1txIjvjCspW5sSOg+kFjarH9nikpGpJRY2hZX4vbKyMnCuhpy3hLJGtCunO9+mdG31gWCAGGW5dXTcrw++uDZdvTUxOl/Xowq2em+tk8e1JBNO+TIFz+pxRqYU9rn+dvHMDp10oYm8HLJrYkO/Z2Xwu2QExkGZNXVeBgThYoGDt2RyoUlN1pyP2veaLbNM/OcTXJd6QvaOrr2sDX7XPY8fjrpXQa87xXDXXBk4gXFRDQTPeMuMVclZTEquh36VVOZBGKq2WVUMCoRpdXW1vzyF+vQPElwdWd1ZGQefsjiQP3itlAqY7ibjcVCPrtd4jDIl5996P5/ne9y5jF8+uj5kiIEhvv52yuY8MyhLlLr7dL6U52y8lZcSXC/x7jw5ywg+QIPuDcMlGO5WLNz3LjSyR+8CkUrnM09Z49HHCd9xZGHUydXYi5NhCiExyQyNtayjbOMLC6/rd7YGznWgT5xw0/Hk8twfrpP7rXVxzN421iWYrmV1frRMycANSzUQ2yGN0vJhbZvjopEVvbYT6r2dRYTwcBzL1wA5nbeuQRDKo35nXiJi3aTxRoc2UsVgQayJI7PVB/agXM4ZgpJHV160jqw8t6FJOz5my3E6D6r4Lhlb8u5cN/Pn2ueyTDw3zqjoUb04XmWtr7Z277R3Ri3Tm3FC+LHH6LWWiGE6lBiKDmXiB0XYW/BvXC/Bsy5WlCdSJ0ihfMR2JUCmXcfB8Vij6BnpzjgVaOfR9cjPnpc21GT3cx9clt4petZXZ/Szcu43oxsu40KiUpiU1e+MjHpuHAHTsz+tUC/v2p0Ujby8puf0uhtAwLoMMuNehpeKBjSOaaO2bLlaarXg7pljhepuGatpmn5SjuG84712DwHGRZU2YnuijccnGyddBuNyOr5aB7bKMkSfmZkZmDfuk6srg+KZ0rVRhjiGNaqOB1VSlRlW8sw5dFm5GZdXH3rDDMd16l1x1KWFDCpHDG6fxCDhN8eDEGz19uCpjfea8AJJFP5eWmabmpoaePQT5DbauOSXoxJ0LykuyvCx7XrXpKAmu1TRdf1dYXXJu/OxjVy9/CwB72VQXochVWzIRJaA00Sc6kRNP8fl8ceyYaarZ3UK7z58vRQ/jUpscDgGdBPi2mSE4pbKeIxY6UvjVpOo0/sdouBy7jqjEr2PSaE/JxhxfmVlZeDfY7Tu2pCli+FWHrMchf4ehZysOqfmIHutNx83TSxGZ4VxGV72noA8OOobWpzQZFBK47XMcmZKzBOVxejcPggePTN8qnS172J3lCn6MJ6+BK3b6lheXh7w7tPT07GyslL06uxVx9F/jDfnBhRtOAPQFc47xeWkG3t3RoRqAFBXrUcfF+2wf1PluFoHjKG5xjIcb2Ji9bq247638Yb6sknQdp2nyjw67uHyXfjT+mq8AiuDMyrKQ0nos5if8yeoC8rFc6YxO1/LjFUN2uG1elVgtxzYpuC1UNp5aZd000w77t+RCg4a6Q0zLjbXQXRQXM+zF+cNMzyoTqFrLHLmzSP8fupRCZC2rcywxMuLIDcOrg1W/tJ1rTczFDznyFNErH+mnWWF5y1LBOJ3mxFwc5Z59nHAdfXQei6D6u6+cVCXeoZOxjmYnA0mL51FbBdCTqDgHCs9r5vzQLUJRWZcmByM0vhKz7eNS0kAS+PTZcJ0Db/NE7t2MmheGxvzPTz2mjhlz4rEGPPVxo8qNh9LeYRxKLlzTJpJV+jelojjJPOo1LWeiUF3Z8EZnuuaMw8ow0Q+V/LOo/J6d6WSsrQl57Rsl4y0ziPmWuvi727Dk6u3jR8XwmiOoWQktHwNOSTqHAT6yWUz2d2RNNEYnRXYeVCOdXk3HL+ZFPdq/O48tpu82glFHQ4aj4scn1lbJRSi1KagbYrRVn9GDr4zSsM5HluXUHXKx4aDz7Nh0fKQHU4OsvfPDF0bknEyW0rG6bWI0cK1cdAOScapt+bv7L3brKaW4eMovLk6sMllEjQMzyVBqfXGbUYvi/0zzwue1Hs7787zrPe5ukttZglP/Gbl1nzAsJ7cndeY3K3OlOrYkTTSXzKVrqvSYhA4C8yxGnt2hnq4B8LiFHOSA8mCwwKbJa74e43nrqEab9DF2zvD4OaTeW6D2RGDL8Zkpcc5zC0jNebRhXvcNm+Scn0sLaUNQ+xkON4ubXOFZ3eJN+e4dhQN/U8t+N6m8EzqvR10Zy/PwstWc5TdTV0o61uXBNioVFJyHt+Mpy5K3SVZ6NrGvKEuTsqxh1cFdEuP3AbLTMajQnoNJWr452tOKTPIzktpvB9EX6GVUYZgxkkT2TDDk+2sGshBd7ag8BAly8iC3GZ4amlYL9AGKWtpmEnv2ncHn0vtusSezrXCdAfhNRyo4Tnz9G4jTm3SrUvupu3jVmp43GoQ2TDK3qX82B9TdUqug8FQD0rNcA6/IRy8PMcIQIWprfM1k1sSDA4htNywMWCJh2Emvmv7NQjJCSru07nnvAbXy6+A5vAMVBu/q4FASDU9Pb3uDy9Z2R2Uz8ZZldbBdk3Auf3unG1nvoc15KXfbTQSzixBuhrlU1iUWUwu6+p1bWZUqwjjUNiI0aD8uHgYhdqSgJqI4nvcstIoEFXvdYpb2u6r93Zp03lxNQYuVHX97eoQavSnjcYG3dGo8+j4zp6ak3IR6zfR8DmGhHyvlmcIrzGYLu04Yq+N+hwc17rUm6ty8+/aZRaMGWe0dyR1aY+he8RgUk49Ps+lW4Pnuc/CClVs/vDymtsG26VPDonyZpgsTlejlyljG3IbZ9w+FkVnyOjiqcwzoxwrsFrNiLK31wlBGyCFSqrMSplA8Dl3v17PxqnGw7PRG+dkj5t0TnUVhRW6lEvJQo4sSYffrOiA7mp025JypX7xb4XzLgPfFqszjWNuu9y/Y9LXRKXB0EF0a5MKmZwRcaSwrsbSO6PQJiwlRe4KxdsgW+nTpV7OfdQmj0ohmoPwpf60UZZRV2V3y2t8H7638Y/v6kg0VleZdMo+bqqdY6WJb5hRJQR8Y2+O6zxYDAUB8fld7vrqqdqOq7BwhphpmA0zmedwsFOpNpbNlAntO0SlCS693oWcYnD9DMEdssvQUhsvmUKzJ9eP1lmaFz7HY6wKnL0bTve6T1LZh6Ed6tGdJ8mulTw2G4caT1aC1SUBUy/Qtvd6FFKlLJFTcnc9G+9snLrG5V14GMWb6/yxoa7x3sNQm3eH0mehY4YUdhbtsOfRQeyRsbyWQUfefIA4DF6W98ar4kcMKil7EvUM7NEzaK/tdBUgV94ZtmwZS8dOz3NZjoNx5L7ULKfVkvKg41Oj5Jk3L8F1jcs1AYf7u8bnDi3ho8tny8vL/fNu0wwbA+1v25hMgnaqR2+DSRrL604j9fBKbXF4reVXI5FdnyTx2HVZ8uJ7hxGuYQVS4/NRKYvPs5h9VHLeWWWSjwzXtY4u7U2KdppH1xhdl8awFIe4jwc1YvCJNjYSJWiuiRrnxXU/u/PybqkOG35crO+EvZR1zRRzXMKwtrb9v+vGYaQURWmeoBRS4R59WYcaV8Tj8OQzMzPrsu0aqzN/pb6WPDl/3DKaxu9tYaRre0d49x2u6BHrn0tn78NJFgwo/lOcoRDXwQrOu+xQHx+5DY3vsnK6TOQQQgl+Zuvobcqu3qHkHVXZ2kjbHZfCcz1ZmKBzojvZuJx67JmZmb5Cz87OrvtzSIXqbNTa+pqhS1ZuPLDC53V5bUcpbxfqrOjDCoTzTmr1uW71/ArjIUQK41l5anjlMhCUpmls/M7ns/uzutEnd3+bUIx6fRQaZ1jiFLkEt11szo4A1zhG5/uGicudnLGnjogBxdfwcVyhSsanfq+loT167UAy6WD2etv3PfMOMLw2GOd4eQ3vJeNkHHhhr+ve21YSGn5NMfhw9eOcW7Lio+s7lDqD86Uxq73W1bNPgjLUw+SWw1wZhe6A7fzRJbaS8Y3IQ6JSPL6yshLLy8vrkm+8nKZPYA6r/KWQbdh8y1BZi3EJUVuCyVlXnFeI1BbPZjw7L6BKmyV3apXcXeuSMLqrwcBhSXMiOPI4lza/sNJn4ZerV5EiU0l+WP74fCkmn+RcjdLGTonRleCJ9W2jsOKwnvwSSUYEWKZzSRdNEKnQqPdhiI4jx9pavovRU4Hj+tyY3F2ptFkFc6oKzX8GwfcoCtMlNZzL8iyO2pSdIbtbWuPlNd1Ao8ZBEYJzUjuC7hKKHjGYXIsYVFBN2nHcjjJtg5bFwqr4ek7LlZQz89IuJq+Jze/upN4bxEaVFVlJFb2UeGNv79rMKFPILPPOSTf9HnHXNdBjXUfvKrhs+XTA3ICrNeRzXdvP4J+D7U6gmDKoyde1bRxrIHxbCLEjqRR71xg/p5Ru7Pici+ezLLujLJfhPipnpWU2t9QWUf4jkBJN0suP5NFLMVAXSAsPx5l0heu8n1jLOqUv8eyUmoUKsJrXmiFUXZbXuH812fZSkg7luczORAVt6EbLunMM2TMjphAdybeaeD0i/0MN5yBYUbP1cveW13HsbZ/0PO506M4wliE5Q/cs4eGsZlev7pZlNNurHrhGuFGeSTP6rKg1SntXhPvOOLWNjRtzVwbXGdo7j+/Ina8ZO5WpDKZn6+ZqNPhcTRK5xNcoVK3o6rVrPbazskqq5Bgk/StiJEMiBl9lpIk6F7tn8Tk8xezs7IAianKQUQfKMDGfKiR6jvMNqvzMF+cj7mqEOQXK0GuqiFkSjb/jXq5/aurOjTG9Xq+/QUaX0lwIACqFOyVPzh8sqy0vL8fy8nIsLS0N/OYknc55l5Ui5cv9HlYWhvLoXWB5G7HCQpH0CGXm7LqLz3WAS5Ps4B4nfPjhGezS4/p4DFjQ1Nur9+ZHbR2cz3jd2cquY9kWPjgDX4LZ7sjeXL9naClDCiXl4XOl2N292pk9u9atjoe/1yp52/laGmpnXO3E1pIqO5RLBwwbFMADBhuvLoJy8vu/NQxwPKuHwH28TspLbFyXg/W4zn+lzG0rKkD9MCYwFi4e53h2R3n8tiRgLVRvO3JbbHh5L3sWn+OeNjnMILZCco7J214CyX+RrIk5bbfEU03ZYakzdK+B7W3XVUki1r8zjmFzr9cbePUzlBpwDoqEXXOagFFF47bR3szMnUPBe+yhaDMzMwMhg/aBBU/DDwf1p6a2//Moj6eOm/vNYY6eHydlKwV63eUrXEZds+SlrDuXc4o+MzOzrg6F68qT86RtKzyQM1Vw3SGX3etySkqT8uBKOz0ZV0s8ePrAvxtgTCIrIOoBQZE52cOoImL9iw6dwWMB5QmGoKpxZITQlmWvPX9XJE2g4TufKyXZnHHIvpc8uXNO6slVlrKXPuI3l2d5Q50l2hnzN5bltYj6LaddiL0XIDoUEB4Rk8sxL/9JoxM2/Q0PgTbZoKAc6nWKyQLLAsCKn3l41MmIRuE7l+exVWi/MyjLumdxtSbhOC8yNTXVnwv25ngklZ9c43N8dAqN8Am/IwbnGU+kYbfb0tJSLC0t9RNvfMxeOKGJtox2lpG+23h0EA8oJ+gw6FBKVrSIGEiCgSBcMBIwLPDoOM/Gwq3JRmxHDpy0Y6XldlT47u6UwfzM+LhYWr2yvjWGr/NjrXrNtekSs5rUdTE2w3dO/rpXSDljru3tTBpJ0V2sPSpxnQy3oUTs0RFP4zssM5JgqKckcKygzAPD64j1zzUrHGR0AEFho8MfnHfCrjEvDIImJ51X39mkfPN558k1LldPznE5b5bhJ9b0pRMZuXic97Lzk2jOk3OM7jLwLsPO7blrO5Luch5dE2lKsKiA1VDyiO1wng0CW3+X3GLYz2iBwwbw5DLo6r0gBCjLiUPwH7F+84wqbAmW67WdoezOeLLxwVGXw9x39tz6nDkn31ix+Rx7dp4blxtxnlwVHhDdveWV4bpbN8/obu3Ru1IXz49JYC+nCshLa6yUrFxQOhYo5keTaurNa2MvENoAD6iD69J4WxUEwqMKrB6Tr+lbVJjaoPS4KVNqDYFKSba21zhr3K9zCNLvTsHd8+W6Uca9Sca1cVelakUftjMK70tQ2hHHs6wkvKGFvwO6sxKrIGRJOcc7x/FYgtO+cf/QNgsEhwaa5YfQ64Yc1J8lu5jnnZ2QUwPk4mj10hn01pdLaMKt19u+Q04hO4ydwmjN60BGAMORYFtcXIzV1dV+Mm5lZWUdhNf97doGUxcHMWm6y0H3CL//3S1lsPKzN1b4rF7ZKYTG45r5zuAZQ28c+TvH0yVjlym0yxVk12vheymWHYVc1jtivTd3n2ypzCXp9N5S2zr/KgtOthgV6vIaFBz3Rkzm0VRFJqPSRP4fnY8am9Ym8FjJmbBrjhNwKKfnIgY3bHD8r0KiySO+VzfLlPrNsboKxsrKSj/U4CQb+C0JKyhLyoFflNcQQcvoOA9DWc4ji7mhqPwGV142g7fu9XrWoysS4Pbc2PE48JIp4nGOyRmqq0fXRJxm5nmO1Anxta4efpzKvkM9eq1QsSJoHI16cGSPni2vsZJnnj3zsKw8ugTn+FBY7mJJ5800uZbxyWOhSqsQ3iX8HI2i7DxeGXJxORKXfYfyMhx38XtpTDXcUS+unjpLsLl97erxeWz1O48tH2to3N48YoKKzh0cV+yoFjpi+4sj2fPqWjZvtMG5Nt5YAR3U5lABxkU31JT67YyAtufud9AdxDGq8/RdqM04uLicv2eJNi3DyTc+8netx30cqXJnHl23tnLW3e1vd95cldx5dIxbzW/n4LjurjQRRdeM5DisE1tl9d6AvpgEfocYoLHGwArXS1QSdu2r82pc3gkoT6qiEG4bUJ9/Oy+C5F7mGXQcRqFS9tsZMqe8paScKn/Wno4xj4+LtwHLORkHBdd1c6fkmnnnOSkdwaP+1vv1vF7rShN9N1EbY6XrGSTKLGmWUMliKffdGSWnCArPcWQB7EpO+bWeDPYrP3xvm9cbB7mwxxk6d4/2xZ13yhwR65TNzSF7dZUNZwgyCI/29N5xObJJ01DLazVCw/Aji1Pa6oFHQz34DW+lG2WWl5f73pwhLMe9aJdjuTbldErN9ehyjvbNZdRRJjMkLuMPKiXjNLZXrz+qUJYgu44TJ+CcJ+brXEYfG0adbTBdr7FCQnHVe6+ursbi4mKsrd25vLa8vByLi4uxbdu2aJpmANZr3J7RKGM8LqiuNNGXQ07C0qm1LiVdStczy59ZaKfkznuOSq6OLC514Yeigkl6crSRGasu96oR0JBHSROh/F0Nm8qF+zA8R1nOsiO+jyj//fRd1bt39ugcX3AsxNdKVtdRjTDCe7E3w738HZ4awoKJ42UpVhKOjTNiD6X8Zn3i85n1VwTAcbkbW/7N3px517HQl1mgfMkjZZR5c76WLXOVfjvK8jG4xkjPtZcpM7yyPl+OcxqjI45Xped+qOKXHEppTHGvi+H5/DDGu3Myrk2Rh2GmS1jAiSq9B4LN6+jYzQZrzF6QJygTfCfQbd6mtMyStcFQnZUyi09V8R1hrDRMqVE0Nh5tyKDWUHclVRQ2YsoXvqvn1u+8pOZeKMFK7mJ2hxrAKx/1ey05/RlWuZmG9uhOYGriJ723RrF5EmHZNTsdsX0zDSaUPTovF6mity1pod02YW+b5FJYwLkH5Q9865jgXq7fxffjhu8ZvHbeXO9x5LwaKw4bLO17pugcmzsP75bZ1Ai4OiLaDXmbF98Z1EnR2yCE88xc3t1bq/ARgxYdS2bYbabXUS8v1fCuuLW1tYEEEM7r20iVajw6j4cKrBunzHvzUprCZhUm9BlKwXvucV3RQg1pf7OEmyub1ed+417Mi4Yo3A/nKLhteF4oKyswL6nx21xxTT06P5aqWX6QzvMkaVij3SkZp50ZxnK13VtzzsVGDM/YomOi3bKIQjo3YV0gZ82E14xXpgx6ToXcKWSX/QJ6r7Y7zqRjidx887zpHLo55/njjzunXt6Vr5H1HaXsw9DQMXoGQbkcjs46s/XlezMhd4rHll4hO67D88NqKy/wmnhTDSsNoLAbA+6bMzJ8XYUkExoOQ0peFHW4BBu/Ehv3MIzXjThtyqye2/0ueXnXRg3x+HDY5VAj8wIZiNj+PwDqtXk/O+9112U0hfBZaObm0znFnUkjZd2Z2iazzUBksSQGWV9GoXG6UzB+gk03PeB7xPb3til/amSyfnGbej4rX0NtCqNZeobmwyhapuQZX13aGBYFqMK7MeDchnr9bCNMdo6fSWc5cjzVotNRlTzTmy401i2wagxKsXnb/dk1d529qiZt+FrEdqWOWP8PHW5TTZuSo349p79rwxSHIEqKV8oplJbR2pRZ68m8d1bHOOB9jXArOlRYj/MarvGHwzuHxvg381XjAO4qNBR0V3KKp8peKt/WBiA6hAz1qhfmiez1tr8bnj02rHbE4GOjrLT6imZnBGrGycE5hYFtdWbxdRbiKDTXUGcc1BZW6LVa4rCK7y05Cl5ZYOjulDp7Sk2X0tQoMG9tEN1dGwfVoMsS7fz/4e1ItR0dB3QeB93VLDvTpJNqO4JGkYdSorUtCXtXnldHvebuxvEGbdAGdaa7nUffoA3aoO60oegbtEH3ANpQ9A3aoHsAbSj6Bm3QPYA2FH2DNugeQBuKvkEbdA+gDUXfoA26B9CGom/QBt0DaEPRN2iD7gH0/wFdLb0BsUmqtQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"name":"stdout","text":"    [Prediction] Performing inference...\n    [Prediction] Predicted Text : 'اللي كاينه في اوروبا م الره اللي كاينه في المغرب معارفينش معلومات عليها'\n    [Prediction] Ground Truth Text: 'اللي كاينه في اوروبا اما الشركه اللي كاينه في المغرب معارفينش معلومات عليها'\n\n--- Prediction Cell Done ---\n","output_type":"stream"}],"execution_count":20}]}